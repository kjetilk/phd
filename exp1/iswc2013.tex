%\documentclass{llncs}
\documentclass{article}
\usepackage{cite}

\title{Introducing Statistical Design of Experiments to SPARQL
  Endpoint Evaluation}
\author{Kjetil Kjernsmo\inst{1}}
%\titlerunning{Introducing Design of Experiments to SPARQL Evaluation}
%\institute{Department of Informatics,
%Postboks 1080 Blindern,
%0316 Oslo, Norway
%\email{kjekje@ifi.uio.no}}

%\maketitle

\begin{document}



\begin{abstract}
This paper argues that the common practice of benchmarking is
inadequate as a scientific evaluation methodology. It further attempts
to introduce the empirical tradition of the physical sciences by using
techniques from Statistical Design of Experiments applied to the
example of SPARQL endpoint performance evaluation. It does so by
studying full as well as fractional factorial experiments designed to
evaluate an assertion that some change introduced in a system has
improved performance. This paper does not present a finished
experimental design, rather its main focus is didactical, to shift the
focus of the community away from benchmarking towards higher
scientific rigor.
\end{abstract}

\section{Introduction}

\section{Related work}

A literature survey as not revealed any direct prior art, however, the
general approach has been well established, not only in statistics. A
relatively well cited textbook is \cite{citeulike:5190414} but we have
not found the parts discussed in the present paper to be widely
adopted. Recently, an comprehensive text on experimental methods have
been published in
\cite{Springer-2010-Experimental-Methods-for-the-Analysis-of-Optimization-Algorithms}. 
We have chosen to turn to statistical standard texts
\cite{wu2009experiments} for this study.

The problematic sides of the benchmarking have been noted in several
papers, notably \cite{Duan:2011:AOC:1989323.1989340} and
\cite{MontoyaVCRA12}. We also acknowledge that great
progress has been made to improve benchmarks, in particular, we are
indebted to \cite{mxro:Morsey2011DBpedia}. 

We believe that automatic creation of benchmark queries, as
pioneered by \cite{goerlitz2012splodge} is a critical ingredient for
the application of DoE to be successful as a methodology.

Finally, we acknowledge the efforts of the Linked Data Benchmark
Council and the SEALS project. However, they appear to focus on
industrial benchmarking rather than scientific evaluations. 


\section{Conclusion}

\section*{Acknowledgements}



\bibliographystyle{plain}
%\bibliographystyle{abbrv}
%\bibliographystyle{jbact}
%\bibliographystyle{splncs03}
\bibliography{selectivity,federation,benchmarks,optimization,experimentsperformance}

\end{document}
