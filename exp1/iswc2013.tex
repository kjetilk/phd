%\documentclass{llncs}
\documentclass{article}
\usepackage{cite}
\usepackage{verbatim}
\newcommand{\todo}[1]{\ensuremath{^{\textrm{\tiny{TODO}\normalsize}}}\footnote{\textbf{TODO:}~#1}}

\title{Introducing Statistical Design of Experiments to SPARQL
  Endpoint Evaluation}
\author{Kjetil Kjernsmo\inst{1}}
%\titlerunning{Introducing Design of Experiments to SPARQL Evaluation}
%\institute{Department of Informatics,
%Postboks 1080 Blindern,
%0316 Oslo, Norway
%\email{kjekje@ifi.uio.no}}

%\maketitle

\begin{document}



\begin{abstract}
This paper argues that the common practice of benchmarking is
inadequate as a scientific evaluation methodology. It further attempts
to introduce the empirical tradition of the physical sciences by using
techniques from Statistical Design of Experiments applied to the
example of SPARQL endpoint performance evaluation. It does so by
studying full as well as fractional factorial experiments designed to
evaluate an assertion that some change introduced in a system has
improved performance. This paper does not present a finished
experimental design, rather its main focus is didactical, to shift the
focus of the community away from benchmarking towards higher
scientific rigor.
\end{abstract}

\section{Introduction}

\section{Related work}

A literature survey as not revealed any direct prior art, however, the
general approach has been well established, not only in statistics. A
relatively well cited textbook is \cite{citeulike:5190414} but we have
not found the parts discussed in the present paper to be widely
adopted. Recently, an comprehensive text on experimental methods have
been published in
\cite{Springer-2010-Experimental-Methods-for-the-Analysis-of-Optimization-Algorithms}. 
We have chosen to turn to statistical standard texts
\cite{wu2009experiments} for this study.

The problematic sides of the benchmarking have been noted in several
papers, notably \cite{Duan:2011:AOC:1989323.1989340} and
\cite{MontoyaVCRA12}. We also acknowledge that great
progress has been made to improve benchmarks, in particular, we are
indebted to \cite{mxro:Morsey2011DBpedia}. 

We believe that automatic creation of benchmark queries, as
pioneered by \cite{goerlitz2012splodge} is a critical ingredient for
the application of DoE to be successful as a methodology.

Finally, we acknowledge the efforts of the Linked Data Benchmark
Council and the SEALS project. However, they appear to focus on
industrial benchmarking rather than scientific evaluations. 

\section{Experimens}

\subsection{Setup}

As the focus of this paper is didactical, the actual measurements are
much simpler than what has been used in benchmarking. We have used the
dataset of DBPedia SPARQL Benchmark\cite{mxro:Morsey2011DBpedia}, but only consider the
smallest of their datasets (that we found to be more than 15
MTriples). Moreover, we have taken subsets of that dataset by using
the last 1 or 2 MTriples from their file.

To only deal with the relatively straightforward aspects of the DoE
formalism, we have chosen to constrain the experiment strictly to 2
levels. We have chosen 8 factors each having 2 levels, ``TripleC'' the number of triples in the
dataset, 1 or 2 MTriples, ``Machine'', which is the software and hardware platform,
where one machine runs GNU/Linux Debian Squeeze, has 16 GB RAM, an
Intel Core2 Duo E8500 CPU and two Parallell-ATA disks in a RAID-1
configuration. The other machine runs Debian Wheezy, has 8 GB RAM, an
Intel Core i7-3520M CPU and a single SSD on SATA-III. 

Then, we test some language features: 
``BGPComp'', which is a Basic Graph Pattern of varying complexity. The
first level is:
\verbatiminput{experiment/BGPComp-1}
and the second level is 
\verbatiminput{experiment/BGPComp-1}

The following factors tests the absence (at level 1) or presence (at level 2) of the following clauses:
The ``Lang'' factor is this FILTER:
\verbatiminput{experiment/Lang-2}
and similarly,  ``Range'' is this FILTER:
\verbatiminput{experiment/Range-2}
``Union'' tests the following:
\verbatiminput{experiment/Union-2}
and ``Optional''
\verbatiminput{experiment/Optional-2}

These fragments has been carefully designed for illustrative utility,
as well as suitable selectivity. When the experiment is run, they are
combined to yield a query, which is then sent to a SPARQL
endpoint. The SPARQL endpoint is itself set up using 4store\todo{ref}
version 1.1.5. The ``Machine'' factor specifies a hostname, whereas
``TripleC'' and ``Implement'' specifies a port number. On each host,
four 4store instances runs independently on different
ports. \todo{possible problem?}

Finally, ``Implement'' is the implementation undergoing
evaluation. Level 1 is running the new implementation, whereas Level 2
is the old implementation. The null hypothesis $H_0$ is that the old
implementation is as good as the new, and the alternative hypothesis
$H_1$ is that the new implementation has overall improved the
performance of the system.

However, as a real optimization is beyond the scope of this paper, we
have instead created a simulation of an optimization. This has also
the advantage that we can \textit{a priori} understand the effect of
the changes. In a real-world case, there is often the case that an
optimization has negative side-effects, and we would need to simulate
both the optimization and the side-effects. To do that, we degraded
the performance of a the 4store SPARQL implementation by inserting
\texttt{sleep} statements. Specifically we inserted the C statement
\texttt{sleep(2)} on line~920 in \texttt{src/frontend/query.c} on
level~1, to simulate the optimization. This has the effect of delaying
execution for 2~seconds for every block for all kinds of joins. On
level 2, we inserted \texttt{usleep(2000)} on line 987 in
\texttt{src/frontend/filter.c} to simulate the negative
side-effect. This delays execution for 2 milliseconds every time the
\texttt{langMatches} SPARQL function is called.

The experimentation is implemented in R\todo{ref}, which is a free
software environment for statistical computing and graphics. Necessary
tools for DoE has been implemented by the R community in packages
called DoE.base, BsMD and FrF2. The experiments are run on a third
computer in a 1 gigabit LAN with the two experiment hosts.  As
response variable, we have chosen to use the time from the namespace
server lookup to the data has been transferred. This choice is
somewhat arbitrary, many other response variables could be chosen, and
indeed, future work should explore multi-variable responses, but for
simplicity, we think this reflects the total amount of work done by
the SPARQL endpoint well. For the measurements, we have chosen to use
the RCurl package, which is a thin wrapper around the curl
library. Curl has well developed facilities for timing requests and
responses, and so we rely on its measurements.

\subsection{Full factorial experiment}

\subsection{Fractional factorial experiment}

\subsection{Fractional factorial experiment with more data}


\section{Discussion}

\paragraph{Summary and hypothesis test}

\paragraph{Strategy for evaluating experiment}\todo{Say something
  about examining variance} \todo{Say something about drilling down in
  factors} \todo{Say something about spreading levels}

\paragraph{Fold-over}

\section{Future Work}


\section{Conclusions}



\section*{Acknowledgements}



\bibliographystyle{plain}
%\bibliographystyle{abbrv}
%\bibliographystyle{jbact}
%\bibliographystyle{splncs03}
\bibliography{selectivity,federation,benchmarks,optimization,experimentsperformance}

\end{document}
