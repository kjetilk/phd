\documentclass{llncs}
%\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{verbatim}
\usepackage{graphicx}
\newcommand{\todo}[1]{\ensuremath{^{\textrm{\tiny{TODO}\normalsize}}}\footnote{\textbf{TODO:}~#1}}

\title{Introducing Statistical Design of Experiments to SPARQL
  Endpoint Evaluation}
\author{Kjetil Kjernsmo\inst{1} \and John S. Tyssedal\inst{2}}
\titlerunning{Introducing Design of Experiments to SPARQL Evaluation}
\institute{Department of Informatics,
Postboks 1080 Blindern,
N-0316 Oslo, Norway \email{kjekje@ifi.uio.no} \and Department of Mathematical Sciences,
The Norwegian Institute of Technology,
N-7491 Trondheim,
Norway
\email{john.tyssedal@math.ntnu.no}}



\begin{document}

\maketitle

\begin{abstract}
This paper argues that the common practice of benchmarking is
inadequate as a scientific evaluation methodology. It further attempts
to introduce the empirical tradition of the physical sciences by using
techniques from Statistical Design of Experiments applied to the
example of SPARQL endpoint performance evaluation. It does so by
studying full as well as fractional factorial experiments designed to
evaluate an assertion that some change introduced in a system has
improved performance. This paper does not present a finished
experimental design, rather its main focus is didactical, to shift the
focus of the community away from benchmarking towards higher
scientific rigor.
\end{abstract}

\section{Introduction}

The practice of benchmarking is widespread, both in industry to select
the most appropriate implementation amongst several choices, and in
academia as means to verify or refute an assertion on the performance
of a certain practice or system. While we could contribute to the
industrial cases, our focus is on the latter. A benchmark typically
consists of several microbenchmarks, which are procedures for placing
loads on the system to measure its performance as a function of the
load. For SPARQL benchmarks, usually the performance is measured as 
the number of queries the SPARQL implementation is able to answer in a
certain time (the throughput) or the time required for it to respond
to a query.

However, there are many problems with this approach when used to
evaluate assertions about the overall performance of a system or
several systems: 
\begin{itemize}
\item As the complexity increases, the number of parameters that
one may wish to test increases dramatically.
\item Nothing compels anyone to find microbenchmarks that
  can refute an assertion, and there is no structured approach to
  investigate whether a benchmark is flawed.
\item There is no meaningful summary of the performance as a whole, so
  that an assertion about performance cannot be refuted as an
  optimization may have detrimental side effects that cannot be
  identified, or cannot be compared to possibly positive effects in a
  different microbenchmark.
\end{itemize}

To address some of these problems, one may try to standardize
benchmarks, to eliminate some parameters and identify others as
believed to be important. Also, social pressure should then prompt
developers to use a standard benchmark so that assertions may be
refuted.

While this is an improvement which is possibly sufficient for
industrial uses, it is still flawed as empirical
research, as it cannot capture the full diversity of the problem: For
example, if a vendor claims that their implementation is better if
used on a highly capable hardware platform, then the standardization
of a hardware platform cannot test this assertion. Moreover, to test
this assertion, one would need a summary technique that can identify
the interaction between implementation and hardware platform and
possibly other parameters.

One important aspect of science is the structured approach to falsify
hypotheses, and thus is important that assertions about performance is
formulated as hypotheses that can be falsified.

Thus, attacking the complexity problem by eliminating parameters we
think influence the outcome for simplicity is untenable as
scientific practice. We must instead tackle this problem by enhancing
our capability of testing complex systems and to identify situations
where our assumptions are flawed. Such constraints can only be made
when we have very good reason to suspect that a certain parameter does
not have any influence.

In this paper, we employ techniques from the well-established field in
statistics known as Design of Experiments (DoE) to point out a
direction that promises to address these problems. First, we design a
simple 8-parameter experiment, and then we demonstrate how a full
factorial experiment can meaningfully provide a summary of the
statistics.

Then, we demonstrate how a fractional factorial experiment can be used
to maintain a structured approach to the problem while reducing the
number of runs the experiment needs, and we discuss the trade-offs
involved. We then continue to discuss techniques that shows our
simplistic experiment to be flawed, yet scientifically better than the
practice of benchmarking. Finally, we outline a road-map to establish
the use of these techniques in SPARQL endpoint evaluations both as a
practical tool and as a scientifically more rigorous methodology.

\section{Related work}

A literature survey as not revealed any direct prior art, however, the
general approach has been well established, not only in statistics. A
relatively well cited textbook is \cite{citeulike:5190414} but we have
not found the parts discussed in the present paper to be widely
adopted. Recently, a comprehensive text on experimental methods have
been published in
\cite{Springer-2010-Experimental-Methods-for-the-Analysis-of-Optimization-Algorithms}
demonstrating the broad applicability of the methodology we employ.
We have chosen SPARQL Endpoint evaluation as an example in this study
since SPARQL Federation is our main interest, and to turn to
statistical standard texts \cite{wu2009experiments} for this study.

The problematic sides of the benchmarking have been noted in several
papers, notably \cite{Duan:2011:AOC:1989323.1989340} and
\cite{MontoyaVCRA12}. We also acknowledge that great
progress has been made to improve benchmarks, in particular, we are
indebted to \cite{mxro:Morsey2011DBpedia}. 

We believe that automatic creation of benchmark queries, as
pioneered by \cite{goerlitz2012splodge} is a critical ingredient for
the application of DoE to be successful as a methodology.

Finally, we acknowledge the efforts of the Linked Data Benchmark
Council and the SEALS project. However, they appear to focus on
industrial benchmarking rather than scientific evaluations. 

\section{Experiments}

Common experiments include comparing several different
implementations. However, we focus on a slightly different problem: We
would like to compare the same implementation before and after some
change has been made, typically with the intention to enhance the
performance of the system. There are two reasons for this: One is that
there are currently many SPARQL implementations available, also Free
Software ones that can be modified by anyone, and the other is to
constrain the scope of the study to comparing two different things.

\subsection{Setup}

As the focus of this paper is didactical, the actual measurements are
much simpler than what has been used in benchmarking. We have used the
data-set of DBPedia SPARQL Benchmark\cite{mxro:Morsey2011DBpedia}, but only consider the
smallest of their data-sets (that we found to be more than 15
MTriples). Moreover, we have taken subsets of that data-set by using
the last 1 or 2 MTriples from their file.

First, note that the concept of parameters as considered in the
introduction is known as \emph{factors} in DoE. For each factor, we
have chosen in this paper to only deal with the relatively
straightforward aspects of the DoE formalism, and therefore 
constrain the experiment strictly to 2 levels.

We have chosen 8 factors each having 2 levels, ``TripleC'' the number of triples in the
data-set, 1 or 2 MTriples, ``Machine'', which is the software and hardware platform,
where one machine runs GNU/Linux Debian Squeeze, has 16 GB RAM, an
Intel Core2 Duo E8500 CPU and two Parallel-ATA disks in a RAID-1
configuration. The other machine runs Debian Wheezy, has 8 GB RAM, an
Intel Core i7-3520M CPU and a single SSD on SATA-III. 

Then, we test some language features: 
``BGPComp'', which is a Basic Graph Pattern of varying complexity. The
first level is:
\verbatiminput{experiment/BGPComp-1}
and the second level is 
\verbatiminput{experiment/BGPComp-2}

The following factors tests the absence (at level 1) or presence (at level 2) of the following clauses:
The ``Lang'' factor is this FILTER:
\verbatiminput{experiment/Lang-2}
and similarly,  ``Range'' is this FILTER:
\verbatiminput{experiment/Range-2}
``Union'' tests the following:
\verbatiminput{experiment/Union-2}
and ``Optional''
\verbatiminput{experiment/Optional-2}

These fragments has been carefully designed for illustrative utility,
as well as suitable selectivity. They are not themselves important,
they only serve to illustrate certain examples of possible
factors. When the experiment is run, they are combined to yield a
query, which is then sent to a SPARQL endpoint. The SPARQL endpoint is
itself set up using 4store (see \cite{harris20094store}) version
1.1.5. The ``Machine'' factor specifies a hostname, whereas
``TripleC'' and ``Implement'' specifies a port number. On each host,
four 4store instances runs independently on different
ports. \todo{possible problem?}

Finally, ``Implement'' is the implementation undergoing
evaluation. Level 1 is running the new implementation, whereas Level 2
is the old implementation. The null hypothesis $H_0$ is that the old
implementation is as good as the new, and the alternative hypothesis
$H_1$ is that the new implementation has overall improved the
performance of the system.

A real optimization is beyond the scope of this paper. Instead we have
performed a simulation that enable us to understand the effect of the
changes.  
In a real-world case, there is often the case that an
optimization has negative side-effects, and we would need to simulate
both the optimization and the side-effects. To do that, we degraded
the performance of a the 4store SPARQL implementation by inserting
\texttt{sleep} statements. Specifically we inserted the C statement
\texttt{sleep(2)} on line~920 in \texttt{src/frontend/query.c} on
level~1, to simulate the optimization. This has the effect of delaying
execution for 2~seconds for every block for all kinds of joins. On
level 2, we inserted \texttt{usleep(2000)} on line 987 in
\texttt{src/frontend/filter.c} to simulate the negative
side-effect. This delays execution for 2 milliseconds every time the
\texttt{langMatches} SPARQL function is called.

The experimentation is implemented in R\cite{rmanual}, which is a free
software environment for statistical computing and graphics. Necessary
tools for DoE has been implemented by the R community in packages
called DoE.base\cite{doebase} and FrF2\cite{frf2}. The experiments are run on a third
computer in a 1 gigabit Ethernet LAN with the two experiment hosts.  As
response variable, we have chosen to use the time from the namespace
server lookup finishes to the data has been transferred. This choice is
somewhat arbitrary, many other response variables could be chosen, and
indeed, future work should explore multi-variable responses, but for
simplicity, we think this reflects the total amount of work done by
the SPARQL endpoint well. For the measurements, we have chosen to use
the RCurl package\cite{lang2007r}, which is a thin wrapper around the curl
library. Curl has well developed facilities for timing requests and
responses, and so we rely on its measurements.

Finally note that there are two common issues in benchmarking we do
not consider: we do not allow a warm-up run, nor do we take into
account that the server may cache all or parts of the result set. The
reasons for this choice will be discussed later.

All the code to reproduce this study as well as detailed instructions
is published on Github: \url{https://github.com/kjetilk/doe-sparql}.

\subsection{Full factorial experiment}\label{sec:full}

In the full factorial experiment, all combinations of the 8 factors
are executed, in total 256 runs.\todo{concrete example?} This is
called a $2^8$ factorial experiment, and we can see that the number of
runs grows exponentially with the number of factors, thus making
large experiments prohibitively expensive. We shall nevertheless find
the following small example instructive. Note that each combination is
executed only once (i.e. it is \emph{unreplicated}), however, so even a
full factorial experiment compares well to a typical benchmark in
which each microbenchmark must be executed several times to obtain
enough data to compute a mean with reasonable certainty.

The above experimental setup is executed across a design matrix
generated with the DoE.base package, which returns a data frame
consisting of the factors with a column containing the corresponding
curl measurements. To analyze the experiment, we first generate a
normal plot, see figure~\ref{fig:fullnormal}\todo{explain axes}. In this plot, if the
null hypothesis $H_0$ holds, all estimated effects will fall on a
straight line, as their means would be zero. Any significant departure
from a straight line can be interpreted as a significant effect (see
Section~4.8 of \cite{wu2009experiments} for a detailed
explanation). In our case, it is most important to note that any
effect that is negative means the effect
enhances the performance of the SPARQL endpoint, the runtime
decreases. If ``Implement'' and its interactions are negative conditional on that
the factors involved with its interactions are on their high level,
then it supports the alternative hypothesis, i.e. we have successfully
improved the performance.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{fullnormal.pdf}
  \caption{A normal plot of the Full Factorial Experiment. The
    labelled points are considered significant by using the Lenth
    criterion at a level $\alpha=0.05$.}\label{fig:fullnormal}
\end{figure}


To proceed beyond a visual judgment of significance, we may use
Lenth's method\cite{lenthmethod}, which is a simple method for
estimating significance in an experiment with no replicates. In
table~\ref{tab:effects}, the most important effects are listed.  In
figure~\ref{fig:fullnormal}, Lenth's method is used to label the
significant effects. We see that we have many very significant effects
(unfortunately so many some labels are illegible). We also note that
``Implement'' itself is significantly negative. We also note that the
``Implement:Lang'' interaction is significantly positive, as expected
from our setup, where ``Lang'' was the detrimental side-effect we
simulated.

% latex table generated in R 2.15.1 by xtable 1.5-6 package
% Fri May 10 16:31:49 2013
% Modified by hand to remove 1's
\begin{table}[hb]
\begin{center}
\caption{The magnitude of effects for some important main effects and interactions.}\label{tab:effects}
\begin{tabular}{rr}
  \hline
Factors & Effect  \\ 
  \hline
Implement & -21.27 \\ 
  Implement:Optional & -11.49 \\ 
  Implement:Union & -6.21 \\ 
  Implement:TripleC:Lang1 & 4.12 \\ 
  TripleC:Lang & 4.20 \\ 
  Implement:TripleC & 4.32 \\ 
  Implement:BGPComp & 4.89 \\ 
  Lang:Union & 5.16 \\ 
  Implement:BGPComp:Lang & 5.16 \\ 
  BGPComp:Lang & 5.25 \\ 
  Implement:Lang:Union & 5.75 \\ 
  TripleC & 7.06 \\ 
  Implement:Lang & 7.73 \\ 
  Lang & 8.62 \\ 
  Optional & 11.30 \\ 
  Union & 17.18 \\ 
   \hline
\end{tabular}
\end{center}
\end{table}


To investigate whether the detrimental side-effects cancels the
positive effect of our simulated optimization, we have to formulate a
hypothesis test. By inspecting the normal plot in
figure~\ref{fig:fullnormal} and table~\ref{tab:effects}, we see that
six factors are highly significant by either being a significant main
effect or participate in a highly significant interaction. We say that
the two factors ``Range'' and ``Machine'' are \emph{inactive} since
they do not contribute significantly to the observed variation in
performance. ``Machine'' is marginally significant, but the effect is
so little, it will be of greater use for us in the hypothesis test. In
practical situations, the five factors ``BGPComp'', ``Lang'',
``Optional'', ``Union'' and ``TripleC'' can be regarded as given,
i.e. we cannot change the size of the data-set without loss of
information, or change the query, since it gives a different
answer. In practice, we can only control ``Implement'', i.e. we can
only change the implementation. In this context, we call ``Implement''
a \emph{control} factor and the other five active factors
\emph{environmental}.

We would like an overall test to see whether the new implementation is
better than the old, and the presence of inactive factors makes it
possible to average the performance of the new and the old
implementations into two distinct vectors. We do this by creating an
average over all the 32 level-combinations of the five active
environmental factors. This leaves us with 4 values for each of the
two levels of ``Implement'' and a two-sample, t-test can be
performed. Effectively, we treat our experiment as a $2^6$ experiment
replicated 4 times.

We perform the test as a one-sided hypothesis test where $H_0$ is that
the vectors are equal and $H_1$ is that the vectors representing the
new implementation is smaller. In this example, we find that the
available data supports the assertion that the new implementation is
better with a high probability, $p=1.16 \cdot 10^{-07}$.\todo{figure?}

While it is interesting to test the overall hypothesis that the new
implementation is an improvement, it is also interesting to know if
there are cases that it fails. Again, we treat the experiment as a
$2^6$ experiment replicated 4 times. Now, for each of the 32 resulting
combinations of the environmental factors for each level of
``Implement'', we extract 4 values. We may now run a t-test for each
of the replications, as above. The results are tabulated in
table~\ref{tab:pvaluesfull}. We see that there are significant
improvements in most cases, one may want to further investigate the
cases where they do not.

\begin{table}[ht]
\begin{center}
\caption{p-values for different parts of the experiment}\label{tab:pvaluesfull}
\begin{tabular}{cccccl}
  \hline
``TripleC'' & ``BGPComp'' & ``Lang'' & ``Union'' & ``Optional'' & $p$ \\ 
  \hline
  1 & 1 & 1 & 1 & 1 & 0.012 \\ 
  1 & 1 & 1 & 1 & 2 & $1.4 \cdot 10^{-09}$ \\ 
  1 & 1 & 1 & 2 & 1 & $3.1 \cdot 10^{-09}$ \\ 
  1 & 1 & 1 & 2 & 2 & $6.1 \cdot 10^{-11}$ \\ 
  1 & 1 & 2 & 1 & 1 & $3.3 \cdot 10^{-06}$ \\ 
  1 & 1 & 2 & 1 & 2 & $2.7 \cdot 10^{-09}$ \\ 
  1 & 1 & 2 & 2 & 1 & $2 \cdot 10^{-06}$ \\ 
  1 & 1 & 2 & 2 & 2 & $3.6 \cdot 10^{-10}$ \\ 
  1 & 2 & 1 & 1 & 1 & 0.014 \\ 
  1 & 2 & 1 & 1 & 2 & $1.2 \cdot 10^{-10}$ \\ 
  1 & 2 & 1 & 2 & 1 & $2.8 \cdot 10^{-14}$ \\ 
  1 & 2 & 1 & 2 & 2 & $4.1 \cdot 10^{-15}$ \\ 
  1 & 2 & 2 & 1 & 1 & $2.1 \cdot 10^{-05}$ \\ 
  1 & 2 & 2 & 1 & 2 & $2.7 \cdot 10^{-07}$ \\ 
  1 & 2 & 2 & 2 & 1 & 0.0072 \\ 
  1 & 2 & 2 & 2 & 2 & $1.6 \cdot 10^{-05}$ \\ 
  2 & 1 & 1 & 1 & 1 & 0.28 \\ 
  2 & 1 & 1 & 1 & 2 & $3 \cdot 10^{-07}$ \\ 
  2 & 1 & 1 & 2 & 1 & $3.3 \cdot 10^{-07}$ \\ 
  2 & 1 & 1 & 2 & 2 & $1.7 \cdot 10^{-08}$ \\ 
  2 & 1 & 2 & 1 & 1 & 0.0023 \\ 
  2 & 1 & 2 & 1 & 2 & $6.5 \cdot 10^{-07}$ \\ 
  2 & 1 & 2 & 2 & 1 & 0.00032 \\ 
  2 & 1 & 2 & 2 & 2 & $1.3 \cdot 10^{-06}$ \\ 
  2 & 2 & 1 & 1 & 1 & 0.013 \\ 
  2 & 2 & 1 & 1 & 2 & $1 \cdot 10^{-11}$ \\ 
  2 & 2 & 1 & 2 & 1 & $3.8 \cdot 10^{-11}$ \\ 
  2 & 2 & 1 & 2 & 2 & $4.1 \cdot 10^{-15}$ \\ 
  2 & 2 & 2 & 1 & 1 &   1 \\ 
  2 & 2 & 2 & 1 & 2 & $2.3 \cdot 10^{-05}$ \\ 
  2 & 2 & 2 & 2 & 1 &   1 \\ 
  2 & 2 & 2 & 2 & 2 & 0.99 \\ 
   \hline
\end{tabular}
\end{center}
\end{table}


For future larger experiments, one must investigate whether it is
necessary to adjust the p-value due to the larger numbers of
hypotheses tested.

The main objective of this paper is not to establish that the
optimization is significant, it is merely a simulation. It is to
establish a critical practice of evaluations. It is of critical
importance that we understand why some effects are
significant.\todo{add table of effects} That ``Implement'' is
significant is hardly a surprise, that is what we worked for. That the
main effects ``Union'', ``Optional'' and ``Lang'' are strong is also
due to that these are fairly demanding things to evaluate. Since the
delay we inserted affects all kinds of joins, it is also intuitive
that interactions between ``Implement'' and those that require joins
are strong. The strong ``BGPComp:Lang'' and ``Lang:Union''
interactions might be due to that the many more triples needs to be
searched for a language tag in one case. However, the positive
``Implement:TripleC'' interaction evades such explanations; in fact,
it hints that our optimization may not work as well for larger
databases. This is a clear indication that the separation between
levels for the sizes of the data-set is too small and should be
investigated further. Such reasoning should be applied to all
significant effects.



\subsection{Fractional factorial experiment}\label{sec:frac}

As mentioned, the full factorial experiment goes as $2^n$ and becomes
prohibitively expensive when $n$ is a large number of
factors. As the evaluation of SPARQL endpoints is an inherently complex
problem, a large number of factors are needed, and so full experiments
cannot scale. However, we may reduce the size of the experiment
significantly by sacrificing some explanatory power, in the form of
fractional factorial experiments. 

We loose explanatory power due to \emph{aliasing}, for example, the
``TripleC:BGPComp'' interaction may be aliased with the
``Machine:Range'' interaction. That is to say, a detected increase in
run time from larger Basic Graph Patterns for large databases, can
also be explained by a less powerful machine evaluation FILTER clauses
with ranges. They are indistinguishable. This may or may not cause a
problem. In many cases, we may not be interested in these effects, we
may only be interested in ``Implement'' and its
interactions. Moreover, it is possible (even easy using the FrF2
package in R) to declare which effects must not be aliased, and
determine the size of the experiment based on that. Such a main effect
or two-factor interaction is called \emph{clear} if none of its
aliases are other main effects or two-factor interactions. Another
possibility to shrink the size of the experiment is if we \textit{a
  priori} can say that some effects are negligible. We have not found
such assumptions to be tenable in our case.

We have made two fractional factorial designs: One with 32 runs and
one with 64 runs. Again, the number of runs is in powers of 2, but
much smaller. In both cases, we have specified that all the factors
must be clear and also all two-factor interactions where ``Implement''
is involved must be clear. The resulting experiments is returned to us
as design matrices, but they can also be described sufficiently for
reproducability in terms of \emph{design generators}. These also
declare aliasing relations for some of the main effects. The design
generators are\\
\begin{tabular}{ccl}
``Range'' &=& ``TripleC'' ``Machine'' ``BGPComp'' \\  
``Union'' &=& ``TripleC'' ``Machine'' ``Lang'' \\  
``Optional'' &=& ``TripleC'' ``BGPComp'' ``Lang'' ``Implement'' 
\end{tabular}
\\and\\
\begin{tabular}{ccl}
``Union'' &=& ``Implement''  ``TripleC''  ``Machine'' ``BGPComp'' \\
``Optional'' &=& ``Implement''  ``TripleC'' ``Lang''  ``Range'' 
\end{tabular}
\\for the 32 and 64 run experiments respectively.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{frac32normal.pdf}
  \caption{A normal plot of the Fractional Factorial Experiment with
    32 runs. The labelled points are considered significant by using
    the Lenth criterion at a level
    $\alpha=0.05$.}\label{fig:frac32normal}
\end{figure}


The resulting normal plot is in figure~\ref{fig:frac32normal}. We see
that only two main effects are significant on a $\alpha = 0.05$ level
according to the Lenth criterion, ``Implement'' is again deemed a
significant improvement, and ``Union'' is deemed significantly the
hardest operation for the query engine. We see that some other points
also deviate from the straight line, but with as few runs as this, the
total variance is great, so no other conclusions can be drawn. We may
use the t-test we used in the previous section, as we have only two
active factors and so we may treat it as a $2^2$ experiment with 8
replications, but the resulting $p = 0.00098$ must be expected with
such a small number of runs and thus higher variance.


For any other purpose than a rough idea of some key influences on the
overall performance of the endpoint for the given levels, this little
experiment is insufficient.

We turn to the 64-run experiment and its normal plot in
figure~\ref{fig:frac64normal}. We see that the we get more significant
effects, and that they correspond well to those we saw in the full
factorial experiment. They also all have an intuitive
explanation as above. However, we do not see the effects that we found
worrisome in the full factorial design. The ``Implement:TripleC''
interaction emerges only for a level $\alpha=0.15$. To ensure that we discover such
cases will be a key challenge in further studies.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{frac64normal.pdf}
  \caption{A normal plot of the Fractional Factorial Experiment with
    64 runs. The labelled points are considered significant by using
    the Lenth criterion. Triangles correspond to a level $\alpha=0.15$
    and crosses at $\alpha=0.05$. The use of different symbols have
    been added to FrF2 by the authors.}\label{fig:frac64normal}
\end{figure}

Since the 64-run experiment has 3 inactive factors, ``Machine'',
``TripleC'' and ``Range'', we can again treat it as a $2^5$ experiment
replicated twice. Using the same procedure as above, we find that
again, the hypothesis test indicates that the new implementation is an
improvement at $p = 5.9 \cdot 10^{-6}$. Again, the comparatively higher p-value
is due to the lower number of runs. It is also possible to perform the
per-environmental test.


\subsection{Fractional factorial experiment with more data}\label{sec:hugefrac}

The importance of finding flaws in the levels is highlighted by the
normal plot in figure~\ref{fig:frac64hugenormal}. The smaller
experiment made it feasible to run with a larger data-set, this
experiment uses the original 15~MTriples data-set of
\cite{mxro:Morsey2011DBpedia}. The picture is now as hinted in the
previous discussion, for larger ``TripleC'', our simulated performance
is actually worse, as evident from the positive ``Implement''. We also
see that the interactions ``Implement:Lang'' and ``Implement:TripleC''
is highly significant. It is clear what happened: For larger
``TripleC'', the delay we introduced in the langMatches function has a
devastating effect, since it delays the execution for every triple
that the query engine applies the filter to, and thereby completely
dominates the execution time.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{frac64hugenormal.pdf}
  \caption{A normal plot of the Fractional Factorial Experiment with
    64 runs where the level 2 of ``TripleC'' is 15 MTriples. The
    labelled points are considered significant by using the Lenth
    criterion at a level $\alpha=0.05$.}\label{fig:frac64hugenormal}
\end{figure}


Moreover, we note that the ``Machine'' factor has become highly
significant in the other end along with its ``Machine:TripleC''
interaction. One should take careful notice that the ``Machine''
factor encompasses many different things, that should be different
factors; CPU, disks, available RAM, installed software, tuning
parameters and so on. It is advantageous to have such broad factors in
the design in early experiments, because they may point out unexpected
effects that should be further scrutinized. The ``Machine'' factor is
a good example of usage that also helps us manage complexity: If
possible, one may group many factors into one if one is in doubt of
its relevance and break it up in subsequent experiments if deeper
understanding is needed. In this case, the cause of the performance
problem was highly audible: It was the old P-ATA disks of the larger
machine, but in general, such clues are not available to the
experimenter, therefore more factors are needed.

We are forced to conclude that the three preceding experiments are
severely flawed, as they failed to take into account the effects from
larger data sizes on slow hardware.


Note that we do not take advantage of the usual performance of 4store
in this simulation, to the contrary, we chose 4store because of its
relative simplicity to modify it to suite our needs for illustrative
purposes, which consists of degrading it in several ways. The small
data-sets used here does not imply that the approach will not work for
large data-sets; properly configured, actual optimization will give
very different runtimes, and the small number of runs will make it
possible to evaluate a very large number of factors.



\section{Discussion}

It is a fundamental property of two-level experiments that they can
only model linear effects, however, they are usually quite sufficient
for identifying significant effects even though the underlying
response may be non-linear.  As we saw in section~\ref{sec:full}, the
choice of levels may be critical, and if non-linearity is expected,
more than two levels must be considered, or at the very least,
attention must be paid to the choice of levels.\todo{this is a long
  discussion, how much should be done?}

We have argued that fixing certain parameters is untenable, but we
have allowed ourselves to disregard caching and warm-up runs. We are
aware that caching may be important in 4store, as a preliminary
experiment gave the results in \ref{tab:cacheexp}. We see that the
response time was significantly lower when an OPTIONAL clause was
added to the query. As this is not more restrictive query, the most
likely explanation is that the result of the evaluation of the Basic
Graph Pattern was cached, so only the OPTIONAL left join was necessary
to add in the second query.

\begin{table}[ht]
\begin{center}
\caption{Two experiments with and without OPTIONAL clause.}\label{tab:cacheexp}
\begin{tabular}{ccccccccr}
  \hline
``Implement'' & ``TripleC'' & ``Machine'' & ``BGPComp'' & ``Lang'' & ``Range'' & ``Union'' & ``Optional'' & Time \\
 \hline
1   &    1  &     1  &     2 &   1 &    1   &  1  & 1 &  53.918049 \\
1   &    1  &     1  &     2 &   1 &    1   &  1  & 2 &   1.252312 \\
   \hline
\end{tabular}
\end{center}
\end{table}

It is \emph{randomization} that makes this permissible, i.e. the order
of execution is random, so the benefit of caching is likely to apply
randomly to different runs, for example, had the run order been
reversed in the previous example, it could have been the query without
OPTIONAL that would have benefitted. Unless the effect of caching is
cumulative throughout the experiment (which is possible), the
randomization will have the effect of reversing the runs randomly. The
end result is that the effect of so-called ``lurking variables'' such
as caching, warm-up effects, etc, contributes to total unexplained
variance of the experiment.

As the unexplained variance of the experiment may cloud important
effects, notably effects that indicate flaws, it should be kept to a
minimum. Thus, lurking variables should be turned into factors
whenever possible, is they degrade the quality of the experiment, but
in many cases, we can live with them if their contribution to the
variance is small.

Finally, we note that the most problematic cases are those that are
covered neither by the broad factors, nor the lurking variables, or
any of the specific factors. We cannot in the general case be sure
that they are accounted for, so reviewers must remain vigilant that
some factors are simply ignored. What finally invalidates the present
experiment is the complete absence of certain obvious language
features, such as solution modifiers.

\section{Future Work}

The present experiment is very primitive in its choice of factors, and
we believe that the strategy employed to construct suitable
experiments will be the main determinant for the long-term feasibility
of this direction. One strategy will be to start with broad factors
such as ``Machine'' and refine them with experience. Also, the data
must be parameterized with many more factors than just the number of
triples; data heterogenity \cite{Duan:2011:AOC:1989323.1989340} is one
example, but also queries over highly skewed data is important. A more
difficult issue is how to address the problem of testing the whole
SPARQL language\cite{sparql11query}. We believe that the
parameterization work initiated by \cite{goerlitz2012splodge} with
their SPLODGE system is a key ingredient. They parameterized SPARQL
queries in such a way they could be autogenerated. One suggestion is
to parameterize queries based on the grammar, so that one factor
becomes the number of e.g. UNIONs, and the levels are chosen. One
problem one will soon run into is that parts of the language is
recursive, e.g. a GroupGraphPattern can consist of several
GroupGraphPatterns. However, we believe that pragmatic limits can be
set, it is for example not of practical interest to nest OPTIONALs to
any great depth even if it is allowed by the language.

We saw in section~\ref{sec:frac} that with a smaller experiment, fewer
interactions are significant, and so provides us with fewer clues to
assess the soundness of the experiment. This should be a focus of
further research. 


Finally, we have only employed the simplest parts of the DoE
theory, what has been presented here is part of a much more general
formalism known as Orthogonal Arrays. The use of orthogonal arrays
allow for different numbers of levels, for much greater flexibility in
total run size, and for non-regular designs that can provide a good fit
for the complex problem of SPARQL endpoint evaluations. 

\section{Conclusions}

As evaluating SPARQL endpoints is inherently difficult a simplistic
experiment such as the one we designed should not hold up to
scrutiny. We set out to demonstrate how an experiment could be set up
using DoE, and show how it can be analyzed. In
sections~\ref{sec:full}~and~\ref{sec:frac}, we first saw how the
analysis correctly pointed out the most important effects, under
assumptions dictated by the factors and levels that were given. We saw
how the formalism provided a comprehensive view of the
experiment. Then, we saw that the formalism pointed out weaknesses
that could invalidate the experiment, and in
section~\ref{sec:hugefrac} we saw that the experiment did indeed not
hold up to scrutiny.

We saw that while 2 level experiments can perform well in determining
significant effects, they do not necessarily work well to find a
model, as exemplified by failure to identify the possibly non-linear
effect of the P-ATA disks. This issue would be fixed by using an
orthogonal array design.

Also, experiments as small as the 32-run is not useful in estimating
any detailed characteristics and is thus of little use for science,
but could be useful in engineering: If the experiment has been
validated by a larger experiment, it could serve well in a Continuous
Integration system.

We have seen that we can perform a proper hypothesis test based on
summary statistics, and by using our expertise, we showed how to show
that the experiment was flawed, and we have cautioned that failure to
control unexplained variance may compromize our ability to do so. We
have also pointed out how this direction can provide more rigorous
evaluation practices.

To this end, the following questions must be asked:
\begin{enumerate}
\item Are there factors that cover all realistic features?
\item If not, are they adequately covered by randomization?
\item If so, would the variance resulting from randomization obscure
  factors that could provide clues that the levels are wrongly set?
\item By carefully examining interactions with ``Implement'', are
  there any that are unaccounted for, and that could point out wrongly
  set levels?
\end{enumarate}

Even if the complexity is great, with properly tuned endpoints, it is
feasible to do millions of runs, and with orthogonoal arrays, this
formalism can be extended to many different evaluation problems.



\section*{Acknowledgments}

Kjetil Kjernsmo would like to thank his main supervisor Martin Giese
for kind assistance. He would also like to
thank Steve Harris for his generous support in degrading his excellent
work on 4store.


%\bibliographystyle{plain}
%\bibliographystyle{abbrv}
%\bibliographystyle{jbact}
\bibliographystyle{splncs03}
\bibliography{practical,selectivity,federation,benchmarks,optimization,experimentsperformance,specs}

\end{document}
