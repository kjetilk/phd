%\documentclass{llncs}
\documentclass{article}
\usepackage{cite}
\usepackage{verbatim}
\newcommand{\todo}[1]{\ensuremath{^{\textrm{\tiny{TODO}\normalsize}}}\footnote{\textbf{TODO:}~#1}}

\title{Introducing Statistical Design of Experiments to SPARQL
  Endpoint Evaluation}
%\author{Kjetil Kjernsmo\inst{1}}
%\titlerunning{Introducing Design of Experiments to SPARQL Evaluation}
%\institute{Department of Informatics,
%Postboks 1080 Blindern,
%0316 Oslo, Norway
%\email{kjekje@ifi.uio.no}}



\begin{document}

\maketitle

\begin{abstract}
This paper argues that the common practice of benchmarking is
inadequate as a scientific evaluation methodology. It further attempts
to introduce the empirical tradition of the physical sciences by using
techniques from Statistical Design of Experiments applied to the
example of SPARQL endpoint performance evaluation. It does so by
studying full as well as fractional factorial experiments designed to
evaluate an assertion that some change introduced in a system has
improved performance. This paper does not present a finished
experimental design, rather its main focus is didactical, to shift the
focus of the community away from benchmarking towards higher
scientific rigor.
\end{abstract}

\section{Introduction}

The practice of benchmarking is widespread, both in industry to select
the most appropriate implementation amongst several choices, and in
academia as means to verify or refute an assertion on the performance
of a certain practice or system. While we could contribute to the
industrial cases, our focus is on the latter. A benchmark typically
consists of several microbenchmarks, which are procedures for placing
loads on the system to measure its performance as a function of the
load. For SPARQL benchmarks, usually the performance is measured as a
the number of queries the SPARQL implementation is able to answer in a
certain time (the throughput) or the time required for it to respond
to a query.

However, there are many of problems with this approach when used to
evaluate assertions about the overall performance of a system or
several systems: 
\begin{itemize}
\item As the complexity increases, the number of parameters that
one may wish to test increases dramatically.
\item Nothing compels anyone to find microbenchmarks that
  can refute an assertion, and there is no structured approach to
  investigate whether a benchmark is flawed.
\item There is no meaningful summary of the performance as a whole, so
  that an assertion about performance cannot be refuted as an
  optimization may have dentrimental side effects that cannot be
  identified, or cannot be compared to possibly positive effects in a
  different microbenchmark.
\end{itemize}

To address some of these problems, one may try to standardize
benchmarks, to eliminate some parameters and identify others as
believed to be important. Also, social pressure should then prompt
developers to use a standard benchmark so that assertions may be
refuted.

While this is an improvement which is possibly sufficient for
industrial uses, is it still flawed as empirical
research, as it cannot capture the full diversity of the problem: For
example, if a vendor claims that their implementation is better if
used on a highly capable hardware platform, then the standardization
of a hardware platform cannot test this assertion. Moreover, to test
this assertion, one would need a summary technique that can identify
the interaction between implementation and hardware platform and
possibly other parameters.

One important aspect of science is the structured approach to falsify
hypotheses, and thus is important that assertions about performance is
formulated as hypotheses that can be falsified.

Thus, attacking the complexity problem by eliminating parameters we
think influence the outcome for simplicity is indefensible as
scientific practice. We must instead tackle this problem by enhancing
our capability of testing complex systems and to identify situations
where our assumptions are flawed. Such constraints can only be made
when we have very good reason to suspect that a certain parameter does
not have any influence.

In this paper, we employ techniques from the well-established field in
statistics of Design of Experiments to point out a direction that
promises to address these problems. First, we design a simple
8-parameter experiment, and then we demonstrate how a full
factorial experiment can meaningfully provide a summary of the
statistics. 

Then, we demonstrate how a fractional factorial experiment can be used
to maintain a structured approach to the problem while reducing the
number of runs the experiment needs, and we discuss the tradeoffs
involved. We then continue to discuss techniques that shows our
simplistic experiment to be flawed, yet scientifically better than the
practice of benchmarking. Finally, we outline a roadmap to establish
the use of these techniques in SPARQL endpoint evaluations both as a
practical tool and as a scientific more rigorous methodology.

\section{Related work}

A literature survey as not revealed any direct prior art, however, the
general approach has been well established, not only in statistics. A
relatively well cited textbook is \cite{citeulike:5190414} but we have
not found the parts discussed in the present paper to be widely
adopted. Recently, an comprehensive text on experimental methods have
been published in
\cite{Springer-2010-Experimental-Methods-for-the-Analysis-of-Optimization-Algorithms}. 
We have chosen to turn to statistical standard texts
\cite{wu2009experiments} for this study.

The problematic sides of the benchmarking have been noted in several
papers, notably \cite{Duan:2011:AOC:1989323.1989340} and
\cite{MontoyaVCRA12}. We also acknowledge that great
progress has been made to improve benchmarks, in particular, we are
indebted to \cite{mxro:Morsey2011DBpedia}. 

We believe that automatic creation of benchmark queries, as
pioneered by \cite{goerlitz2012splodge} is a critical ingredient for
the application of DoE to be successful as a methodology.

Finally, we acknowledge the efforts of the Linked Data Benchmark
Council and the SEALS project. However, they appear to focus on
industrial benchmarking rather than scientific evaluations. 

\section{Experiments}

Common experiments include comparing several different
implementations. However, we focus on a slightly different problem: We
would like to compare the same implementation before and after some
change has been made, typically with the intention to enhance the
performance of the system. There are two reasons for this: One is that
there are currently many SPARQL implementations available, also Free
Software ones that can be modified by anyone, and the other is to
constrain the scope of the study to comparing two different things.

\subsection{Setup}

As the focus of this paper is didactical, the actual measurements are
much simpler than what has been used in benchmarking. We have used the
dataset of DBPedia SPARQL Benchmark\cite{mxro:Morsey2011DBpedia}, but only consider the
smallest of their datasets (that we found to be more than 15
MTriples). Moreover, we have taken subsets of that dataset by using
the last 1 or 2 MTriples from their file.

First, note that the concept of parameters as considered in the
introduction is known as \emph{factors} in DoE. For each factor, we
have chosen in this paper to only deal with the relatively
straightforward aspects of the DoE formalism, and therefore 
constrain the experiment strictly to 2 levels.

We have chosen 8 factors each having 2 levels, ``TripleC'' the number of triples in the
dataset, 1 or 2 MTriples, ``Machine'', which is the software and hardware platform,
where one machine runs GNU/Linux Debian Squeeze, has 16 GB RAM, an
Intel Core2 Duo E8500 CPU and two Parallell-ATA disks in a RAID-1
configuration. The other machine runs Debian Wheezy, has 8 GB RAM, an
Intel Core i7-3520M CPU and a single SSD on SATA-III. 

Then, we test some language features: 
``BGPComp'', which is a Basic Graph Pattern of varying complexity. The
first level is:
\verbatiminput{experiment/BGPComp-1}
and the second level is 
\verbatiminput{experiment/BGPComp-2}

The following factors tests the absence (at level 1) or presence (at level 2) of the following clauses:
The ``Lang'' factor is this FILTER:
\verbatiminput{experiment/Lang-2}
and similarly,  ``Range'' is this FILTER:
\verbatiminput{experiment/Range-2}
``Union'' tests the following:
\verbatiminput{experiment/Union-2}
and ``Optional''
\verbatiminput{experiment/Optional-2}

These fragments has been carefully designed for illustrative utility,
as well as suitable selectivity. They are not themselves important,
they only serve to illustrate certain examples of possible
factors. When the experiment is run, they are combined to yield a
query, which is then sent to a SPARQL endpoint. The SPARQL endpoint is
itself set up using 4store\todo{ref} version 1.1.5. The ``Machine''
factor specifies a hostname, whereas ``TripleC'' and ``Implement''
specifies a port number. On each host, four 4store instances runs
independently on different ports. \todo{possible problem?}

Finally, ``Implement'' is the implementation undergoing
evaluation. Level 1 is running the new implementation, whereas Level 2
is the old implementation. The null hypothesis $H_0$ is that the old
implementation is as good as the new, and the alternative hypothesis
$H_1$ is that the new implementation has overall improved the
performance of the system.

However, as a real optimization is beyond the scope of this paper, we
have instead created a simulation of an optimization. This has also
the advantage that we can \textit{a priori} understand the effect of
the changes. In a real-world case, there is often the case that an
optimization has negative side-effects, and we would need to simulate
both the optimization and the side-effects. To do that, we degraded
the performance of a the 4store SPARQL implementation by inserting
\texttt{sleep} statements. Specifically we inserted the C statement
\texttt{sleep(2)} on line~920 in \texttt{src/frontend/query.c} on
level~1, to simulate the optimization. This has the effect of delaying
execution for 2~seconds for every block for all kinds of joins. On
level 2, we inserted \texttt{usleep(2000)} on line 987 in
\texttt{src/frontend/filter.c} to simulate the negative
side-effect. This delays execution for 2 milliseconds every time the
\texttt{langMatches} SPARQL function is called.

The experimentation is implemented in R\todo{ref}, which is a free
software environment for statistical computing and graphics. Necessary
tools for DoE has been implemented by the R community in packages
called DoE.base, BsMD and FrF2. The experiments are run on a third
computer in a 1 gigabit Ethernet LAN with the two experiment hosts.  As
response variable, we have chosen to use the time from the namespace
server lookup finishes to the data has been transferred. This choice is
somewhat arbitrary, many other response variables could be chosen, and
indeed, future work should explore multi-variable responses, but for
simplicity, we think this reflects the total amount of work done by
the SPARQL endpoint well. For the measurements, we have chosen to use
the RCurl package, which is a thin wrapper around the curl
library. Curl has well developed facilities for timing requests and
responses, and so we rely on its measurements.

Finally note that there are two common issues in benchmarking we do
not consider: we do not allow a warm-up run, nor do we take into
account that the server may cache all or parts of the result set. The
reasons for this choice will be discussed later.\todo{lurking
  variables}

The code will be published\todo{github URL}.

\subsection{Full factorial experiment}

In the full factorial experiment, all combinations of the 8 factors
are executed, in total 256 runs.\todo{concrete example?} This is
called a $2^8$ factorial experiment, and we can see that the number of
runs is grows exponentially with the number of factors, thus making
large experiments prohibitively expensive. We shall nevertheless find
the following small example instructive. Note that each combination is
executed only once (i.e. it is \emph{unreplicated}, however, so even a
full factorial experiment compares well to a typical benchmark in
which each microbenchmark must be executed several times to obtain
enough data to compute a mean with reasonable accuracy.

The above experimental setup is executed across a design matrix
generated with the DoE.base package, which returns a data frame
consisting of the factors with a column containing the corresponding
curl measurements. To analyze the experiment, we first generate a
normal plot, see figure~\ref{fig:fullnormal}. In this plot, if the
null hypothesis $H_0$ holds, all estimated effects will fall on a
straight line, as their means would be zero. Any significant departure
from a straight line can be interpreted as a significant effect (see
Section~4.8 of \cite{wu2009experiments} for a detailed
explanation). In our case, it is most important to note that any
effect that is negative (i.e. has a negative slope) means the effect
enhances the performance of the SPARQL endpoint, the runtime
decreases. If ``Implement'' and the sum of its interactions are
negative, then it supports the alternative hypothesis, i.e. we have
successfully improved the performance.

To proceed beyond a visual judgement of significance, we may use
Lenth's method\todo{ref Lenth}, which is a simple method for
estimating significance in an experiment with no replicates. We plot
the results in figure~\ref{fig:fulllenth}. Visually, this is not as
useful, since the large number\todo{really include?} We see that we
have very significant effects. We also note that ``Implement'' itself
is significantly negative. We also note that the ``Implement:Lang''
interaction is significantly positive, as expected from our setup,
where ``Lang'' was the detrimental side-effect we simulated.

To investigate whether the detrimental side-effects cancels the
positive effect of our simulated optimization, we have to formulate a
hypothesis test. By inspecting the normal plot in figure~\ref{fig:fullnormal}

\subsection{Fractional factorial experiment}

\subsection{Fractional factorial experiment with more data}


\section{Discussion}

\paragraph{Summary and hypothesis test}

\paragraph{Strategy for evaluating experiment}\todo{Say something
  about examining variance} \todo{Say something about drilling down in
  factors} \todo{Say something about spreading levels}

\paragraph{Fold-over}

\section{Future Work}


\section{Conclusions}



\section*{Acknowledgements}



\bibliographystyle{plain}
%\bibliographystyle{abbrv}
%\bibliographystyle{jbact}
%\bibliographystyle{splncs03}
\bibliography{selectivity,federation,benchmarks,optimization,experimentsperformance}

\end{document}
