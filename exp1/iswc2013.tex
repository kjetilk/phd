%\documentclass{llncs}
\documentclass{article}
\usepackage{cite}
\usepackage{verbatim}

\title{Introducing Statistical Design of Experiments to SPARQL
  Endpoint Evaluation}
\author{Kjetil Kjernsmo\inst{1}}
%\titlerunning{Introducing Design of Experiments to SPARQL Evaluation}
%\institute{Department of Informatics,
%Postboks 1080 Blindern,
%0316 Oslo, Norway
%\email{kjekje@ifi.uio.no}}

%\maketitle

\begin{document}



\begin{abstract}
This paper argues that the common practice of benchmarking is
inadequate as a scientific evaluation methodology. It further attempts
to introduce the empirical tradition of the physical sciences by using
techniques from Statistical Design of Experiments applied to the
example of SPARQL endpoint performance evaluation. It does so by
studying full as well as fractional factorial experiments designed to
evaluate an assertion that some change introduced in a system has
improved performance. This paper does not present a finished
experimental design, rather its main focus is didactical, to shift the
focus of the community away from benchmarking towards higher
scientific rigor.
\end{abstract}

\section{Introduction}

\section{Related work}

A literature survey as not revealed any direct prior art, however, the
general approach has been well established, not only in statistics. A
relatively well cited textbook is \cite{citeulike:5190414} but we have
not found the parts discussed in the present paper to be widely
adopted. Recently, an comprehensive text on experimental methods have
been published in
\cite{Springer-2010-Experimental-Methods-for-the-Analysis-of-Optimization-Algorithms}. 
We have chosen to turn to statistical standard texts
\cite{wu2009experiments} for this study.

The problematic sides of the benchmarking have been noted in several
papers, notably \cite{Duan:2011:AOC:1989323.1989340} and
\cite{MontoyaVCRA12}. We also acknowledge that great
progress has been made to improve benchmarks, in particular, we are
indebted to \cite{mxro:Morsey2011DBpedia}. 

We believe that automatic creation of benchmark queries, as
pioneered by \cite{goerlitz2012splodge} is a critical ingredient for
the application of DoE to be successful as a methodology.

Finally, we acknowledge the efforts of the Linked Data Benchmark
Council and the SEALS project. However, they appear to focus on
industrial benchmarking rather than scientific evaluations. 

\section{Experimental setup}

As the focus of this paper is didactical, the actual measurements are
much simpler than what has been used in benchmarking. 

To only deal with the relatively straightforward aspects of the DoE
formalism, we have chosen to constrain the experiment strictly to 2
levels. We have chosen 8 factors each having 2 levels, ``TripleC'' the number of triples in the
dataset, 1 or 2 MTriples, ``Machine'', which is the software and hardware platform,
where one machine runs GNU/Linux Debian Squeeze, has 16 GB RAM, an
Intel Core2 Duo E8500 CPU and two Parallell-ATA disks in a RAID-1
configuration. The other machine runs Debian Wheezy, has 8 GB RAM, an
Intel Core i7-3520M CPU and a single SSD on SATA-III. 

Then, we test some language features: 
``BGPComp'', which is a Basic Graph Pattern of varying complexity. The
first level is:
\verbatiminput{experiment/BGPComp-1}
and the second level is 
\verbatiminput{experiment/BGPComp-1}

The following factors tests the absence (at level 1) or presence (at level 2) of the following clauses:
The ``Lang'' factor is this FILTER:
\verbatiminput{experiment/Lang-2}
and similarly,  ``Range'' is this FILTER:
\verbatiminput{experiment/Range-2}
``Union'' tests the following:
\verbatiminput{experiment/Union-2}
and ``Optional''
\verbatiminput{experiment/Optional-2}

Finally, ``Implement'' is the implementation undergoing
evaluation. Level 1 is running the new implementation, whereas Level 2
is the old implementation. The null hypothesis $H_0$ is that the old
implementation is as good as the new, and the alternative hypothesis
$H_1$ is that the new implementation has overall improved the
performance of the system.

We have used the dataset of \cite{mxro:Morsey2011DBpedia}, where 

\section{Conclusion}

\section*{Acknowledgements}



\bibliographystyle{plain}
%\bibliographystyle{abbrv}
%\bibliographystyle{jbact}
%\bibliographystyle{splncs03}
\bibliography{selectivity,federation,benchmarks,optimization,experimentsperformance}

\end{document}
