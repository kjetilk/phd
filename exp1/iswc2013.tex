%\documentclass{llncs}
\documentclass{article}
\usepackage{cite}
\usepackage{verbatim}
\newcommand{\todo}[1]{\ensuremath{^{\textrm{\tiny{TODO}\normalsize}}}\footnote{\textbf{TODO:}~#1}}

\title{Introducing Statistical Design of Experiments to SPARQL
  Endpoint Evaluation}
%\author{Kjetil Kjernsmo\inst{1}}
%\titlerunning{Introducing Design of Experiments to SPARQL Evaluation}
%\institute{Department of Informatics,
%Postboks 1080 Blindern,
%0316 Oslo, Norway
%\email{kjekje@ifi.uio.no}}



\begin{document}

\maketitle

\begin{abstract}
This paper argues that the common practice of benchmarking is
inadequate as a scientific evaluation methodology. It further attempts
to introduce the empirical tradition of the physical sciences by using
techniques from Statistical Design of Experiments applied to the
example of SPARQL endpoint performance evaluation. It does so by
studying full as well as fractional factorial experiments designed to
evaluate an assertion that some change introduced in a system has
improved performance. This paper does not present a finished
experimental design, rather its main focus is didactical, to shift the
focus of the community away from benchmarking towards higher
scientific rigor.
\end{abstract}

\section{Introduction}

The practice of benchmarking is widespread, both in industry to select
the most appropriate implementation amongst several choices, and in
academia as means to verify or refute an assertion on the performance
of a certain practice or system. While we could contribute to the
industrial cases, our focus is on the latter. A benchmark typically
consists of several microbenchmarks, which are procedures for placing
loads on the system to measure its performance as a function of the
load. For SPARQL benchmarks, usually the performance is measured as a
the number of queries the SPARQL implementation is able to answer in a
certain time (the throughput) or the time required for it to respond
to a query.

However, there are many of problems with this approach when used to
evaluate assertions about the overall performance of a system or
several systems: 
\begin{itemize}
\item As the complexity increases, the number of parameters that
one may wish to test increases dramatically.
\item Nothing compels anyone to find microbenchmarks that
  can refute an assertion, and there is no structured approach to
  investigate whether a benchmark is flawed.
\item There is no meaningful summary of the performance as a whole, so
  that an assertion about performance cannot be refuted as an
  optimization may have detrimental side effects that cannot be
  identified, or cannot be compared to possibly positive effects in a
  different microbenchmark.
\end{itemize}

To address some of these problems, one may try to standardize
benchmarks, to eliminate some parameters and identify others as
believed to be important. Also, social pressure should then prompt
developers to use a standard benchmark so that assertions may be
refuted.

While this is an improvement which is possibly sufficient for
industrial uses, is it still flawed as empirical
research, as it cannot capture the full diversity of the problem: For
example, if a vendor claims that their implementation is better if
used on a highly capable hardware platform, then the standardization
of a hardware platform cannot test this assertion. Moreover, to test
this assertion, one would need a summary technique that can identify
the interaction between implementation and hardware platform and
possibly other parameters.

One important aspect of science is the structured approach to falsify
hypotheses, and thus is important that assertions about performance is
formulated as hypotheses that can be falsified.

Thus, attacking the complexity problem by eliminating parameters we
think influence the outcome for simplicity is untenable as
scientific practice. We must instead tackle this problem by enhancing
our capability of testing complex systems and to identify situations
where our assumptions are flawed. Such constraints can only be made
when we have very good reason to suspect that a certain parameter does
not have any influence.

In this paper, we employ techniques from the well-established field in
statistics known as Design of Experiments (DoE) to point out a
direction that promises to address these problems. First, we design a
simple 8-parameter experiment, and then we demonstrate how a full
factorial experiment can meaningfully provide a summary of the
statistics.

Then, we demonstrate how a fractional factorial experiment can be used
to maintain a structured approach to the problem while reducing the
number of runs the experiment needs, and we discuss the trade-offs
involved. We then continue to discuss techniques that shows our
simplistic experiment to be flawed, yet scientifically better than the
practice of benchmarking. Finally, we outline a road-map to establish
the use of these techniques in SPARQL endpoint evaluations both as a
practical tool and as a scientifically more rigorous methodology.

\section{Related work}

A literature survey as not revealed any direct prior art, however, the
general approach has been well established, not only in statistics. A
relatively well cited textbook is \cite{citeulike:5190414} but we have
not found the parts discussed in the present paper to be widely
adopted. Recently, an comprehensive text on experimental methods have
been published in
\cite{Springer-2010-Experimental-Methods-for-the-Analysis-of-Optimization-Algorithms}
demonstrating the broad applicability of the methodology we employ.
We have chosen SPARQL Endpoint evaluation as an example in this study
since SPARQL Federation is our main interest, and to turn to
statistical standard texts \cite{wu2009experiments} for this study.

The problematic sides of the benchmarking have been noted in several
papers, notably \cite{Duan:2011:AOC:1989323.1989340} and
\cite{MontoyaVCRA12}. We also acknowledge that great
progress has been made to improve benchmarks, in particular, we are
indebted to \cite{mxro:Morsey2011DBpedia}. 

We believe that automatic creation of benchmark queries, as
pioneered by \cite{goerlitz2012splodge} is a critical ingredient for
the application of DoE to be successful as a methodology.

Finally, we acknowledge the efforts of the Linked Data Benchmark
Council and the SEALS project. However, they appear to focus on
industrial benchmarking rather than scientific evaluations. 

\section{Experiments}

Common experiments include comparing several different
implementations. However, we focus on a slightly different problem: We
would like to compare the same implementation before and after some
change has been made, typically with the intention to enhance the
performance of the system. There are two reasons for this: One is that
there are currently many SPARQL implementations available, also Free
Software ones that can be modified by anyone, and the other is to
constrain the scope of the study to comparing two different things.

\subsection{Setup}

As the focus of this paper is didactical, the actual measurements are
much simpler than what has been used in benchmarking. We have used the
data-set of DBPedia SPARQL Benchmark\cite{mxro:Morsey2011DBpedia}, but only consider the
smallest of their data-sets (that we found to be more than 15
MTriples). Moreover, we have taken subsets of that data-set by using
the last 1 or 2 MTriples from their file.

First, note that the concept of parameters as considered in the
introduction is known as \emph{factors} in DoE. For each factor, we
have chosen in this paper to only deal with the relatively
straightforward aspects of the DoE formalism, and therefore 
constrain the experiment strictly to 2 levels.

We have chosen 8 factors each having 2 levels, ``TripleC'' the number of triples in the
data-set, 1 or 2 MTriples, ``Machine'', which is the software and hardware platform,
where one machine runs GNU/Linux Debian Squeeze, has 16 GB RAM, an
Intel Core2 Duo E8500 CPU and two Parallel-ATA disks in a RAID-1
configuration. The other machine runs Debian Wheezy, has 8 GB RAM, an
Intel Core i7-3520M CPU and a single SSD on SATA-III. 

Then, we test some language features: 
``BGPComp'', which is a Basic Graph Pattern of varying complexity. The
first level is:
\verbatiminput{experiment/BGPComp-1}
and the second level is 
\verbatiminput{experiment/BGPComp-2}

The following factors tests the absence (at level 1) or presence (at level 2) of the following clauses:
The ``Lang'' factor is this FILTER:
\verbatiminput{experiment/Lang-2}
and similarly,  ``Range'' is this FILTER:
\verbatiminput{experiment/Range-2}
``Union'' tests the following:
\verbatiminput{experiment/Union-2}
and ``Optional''
\verbatiminput{experiment/Optional-2}

These fragments has been carefully designed for illustrative utility,
as well as suitable selectivity. They are not themselves important,
they only serve to illustrate certain examples of possible
factors. When the experiment is run, they are combined to yield a
query, which is then sent to a SPARQL endpoint. The SPARQL endpoint is
itself set up using 4store\todo{ref} version 1.1.5. The ``Machine''
factor specifies a hostname, whereas ``TripleC'' and ``Implement''
specifies a port number. On each host, four 4store instances runs
independently on different ports. \todo{possible problem?}

Finally, ``Implement'' is the implementation undergoing
evaluation. Level 1 is running the new implementation, whereas Level 2
is the old implementation. The null hypothesis $H_0$ is that the old
implementation is as good as the new, and the alternative hypothesis
$H_1$ is that the new implementation has overall improved the
performance of the system.

However, as a real optimization is beyond the scope of this paper, we
have instead created a simulation of an optimization. This has also
the advantage that we can \textit{a priori} understand the effect of
the changes. In a real-world case, there is often the case that an
optimization has negative side-effects, and we would need to simulate
both the optimization and the side-effects. To do that, we degraded
the performance of a the 4store SPARQL implementation by inserting
\texttt{sleep} statements. Specifically we inserted the C statement
\texttt{sleep(2)} on line~920 in \texttt{src/frontend/query.c} on
level~1, to simulate the optimization. This has the effect of delaying
execution for 2~seconds for every block for all kinds of joins. On
level 2, we inserted \texttt{usleep(2000)} on line 987 in
\texttt{src/frontend/filter.c} to simulate the negative
side-effect. This delays execution for 2 milliseconds every time the
\texttt{langMatches} SPARQL function is called.

The experimentation is implemented in R\todo{ref}, which is a free
software environment for statistical computing and graphics. Necessary
tools for DoE has been implemented by the R community in packages
called DoE.base, BsMD and FrF2. The experiments are run on a third
computer in a 1 gigabit Ethernet LAN with the two experiment hosts.  As
response variable, we have chosen to use the time from the namespace
server lookup finishes to the data has been transferred. This choice is
somewhat arbitrary, many other response variables could be chosen, and
indeed, future work should explore multi-variable responses, but for
simplicity, we think this reflects the total amount of work done by
the SPARQL endpoint well. For the measurements, we have chosen to use
the RCurl package, which is a thin wrapper around the curl
library. Curl has well developed facilities for timing requests and
responses, and so we rely on its measurements.

Finally note that there are two common issues in benchmarking we do
not consider: we do not allow a warm-up run, nor do we take into
account that the server may cache all or parts of the result set. The
reasons for this choice will be discussed later.\todo{lurking
  variables}

All the code to reproduce this study as well as detailed instructions
is published on Github: \texttt{https://github.com/kjetilk/doe-sparql}.

\subsection{Full factorial experiment}

In the full factorial experiment, all combinations of the 8 factors
are executed, in total 256 runs.\todo{concrete example?} This is
called a $2^8$ factorial experiment, and we can see that the number of
runs is grows exponentially with the number of factors, thus making
large experiments prohibitively expensive. We shall nevertheless find
the following small example instructive. Note that each combination is
executed only once (i.e. it is \emph{unreplicated}, however, so even a
full factorial experiment compares well to a typical benchmark in
which each microbenchmark must be executed several times to obtain
enough data to compute a mean with reasonable certainty.

The above experimental setup is executed across a design matrix
generated with the DoE.base package, which returns a data frame
consisting of the factors with a column containing the corresponding
curl measurements. To analyze the experiment, we first generate a
normal plot, see figure~\ref{fig:fullnormal}. In this plot, if the
null hypothesis $H_0$ holds, all estimated effects will fall on a
straight line, as their means would be zero. Any significant departure
from a straight line can be interpreted as a significant effect (see
Section~4.8 of \cite{wu2009experiments} for a detailed
explanation). In our case, it is most important to note that any
effect that is negative (i.e. has a negative slope) means the effect
enhances the performance of the SPARQL endpoint, the runtime
decreases. If ``Implement'' and the sum of its interactions are
negative, then it supports the alternative hypothesis, i.e. we have
successfully improved the performance.

To proceed beyond a visual judgment of significance, we may use
Lenth's method\todo{ref Lenth}, which is a simple method for
estimating significance in an experiment with no replicates. We plot
the results in figure~\ref{fig:fulllenth}. In
figure~\ref{fig:fullnormal}, Lenth's method is used to label the
significant effects. Visually, this is not as useful, since the large
number\todo{really include?} We see that we have very significant
effects. We also note that ``Implement'' itself is significantly
negative. We also note that the ``Implement:Lang'' interaction is
significantly positive, as expected from our setup, where ``Lang'' was
the detrimental side-effect we simulated.

To investigate whether the detrimental side-effects cancels the
positive effect of our simulated optimization, we have to formulate a
hypothesis test. By inspecting the normal plot in
figure~\ref{fig:fullnormal}, we see that only five factors are highly
significant by either being a significant main effect or participate
in a highly significant interaction. We say that the three factors
``BGPComp'', ``Range'' and ``Machine'' are \emph{inactive} since they
do not contribute significantly to the observed variation in
performance. In practical situations, the four factors ``Lang'',
``Optional'', ``Union'' and ``TripleC'' can be regarded as given,
i.e. we cannot change the size of the data-set without loss of
information, or change the query, since it gives a different
answer. In practice, we can only control ``Implement'', i.e. we can
only change the implementation. In this context, we call ``Implement''
a \emph{control} factor and the other four active factors
\emph{environmental}.

The presence of inactive factors makes it possible to use a standard
t-test by averaging the inactive factors for each combination of
control and environmental factors. This enables us to treat our
experiment as a $2^5$ experiment replicated 8 times.

We then extract two vectors, one for the each of the implementations,
and then perform a one-sided hypothesis test where $H_0$ is that they
are equal and $H_1$ is that the vector representing the new
implementation is smaller. In this example, we find that the available
data supports the assertion that the new implementation is better with a
high probability, $p=2.9 \cdot 10^{-10}$.\todo{rationale could be
  stronger}\todo{figure?}

While it is interesting to test the overall hypothesis that the new
implementation is an improvement, it is also interesting to know if
there are cases that it fails. Again, we treat the experiment as a
$2^5$ experiment replicated 8 times. Now, for each of the 16 resulting
combinations of the environmental factors for each level of
``Implement'', we extract 8 values. We may now run a t-test for each
of the replications, as above. The results are tabulated in
table~\ref{tab:pvaluesfull}. We see that there are significant
improvements in most cases, one may want to further investigate the
cases where they do not.

\begin{table}[ht]
\begin{center}
\caption{p-values for different parts of the experiment}\label{tab:pvaluesfull}
\begin{tabular}{ccccl}
  \hline
``TripleC'' & ``Lang'' & ``Union'' & ``Optional'' & $p$ \\ 
  \hline
  1 & 1 & 1 & 1 & 0.00032 \\ 
  1 & 1 & 1 & 2 & $2 \cdot 10^{-14}$ \\ 
  1 & 1 & 2 & 1 & $4.7 \cdot 10^{-14}$ \\ 
  1 & 1 & 2 & 2 & $2 \cdot 10^{-18}$ \\ 
  1 & 2 & 1 & 1 & $1.4 \cdot 10^{-06}$ \\ 
  1 & 2 & 1 & 2 & $8.3 \cdot 10^{-11}$ \\ 
  1 & 2 & 2 & 1 & 0.0017 \\ 
  1 & 2 & 2 & 2 & $2.1 \cdot 10^{-06}$ \\ 
  2 & 1 & 1 & 1 & 0.09 \\ 
  2 & 1 & 1 & 2 & $1.6 \cdot 10^{-09}$ \\ 
  2 & 1 & 2 & 1 & $8.4 \cdot 10^{-10}$ \\ 
  2 & 1 & 2 & 2 & $2.5 \cdot 10^{-13}$ \\ 
  2 & 2 & 1 & 1 & 0.29 \\ 
  2 & 2 & 1 & 2 & $9.1 \cdot 10^{-07}$ \\ 
  2 & 2 & 2 & 1 & 0.92 \\ 
  2 & 2 & 2 & 2 & 0.071 \\ 
   \hline
\end{tabular}
\end{center}
\end{table}


For future larger experiments, one must investigate whether it is
necessary to adjust the p-value due to the larger numbers of
hypotheses tested.

The main objective of this paper is not to establish that the
optimization is significant, it is merely a simulation. It is to
establish a critical practice of evaluations. It is of critical
importance that we understand why some effects are critical.\todo{add
  table of effects} That
``Implement'' is significant is hardly a surprise, that is what we
worked for. That the main effects ``Union'', ``Optional'' and ``Lang''
are strong is also due to that these are fairly demanding things to
evaluate. Since the delay we inserted affects all kinds of joins, it
is also intuitive that interactions between ``Implement'' and those
that require joins are strong. The strong ``BGPComp:Lang'' and
``Lang:Union'' interactions might be due to that the many more triples
needs to be searched for a language tag in one case. However, the
positive ``Implement:TripleC'' interaction evades such explanations;
in fact, it hints that our optimization may not work as well for
larger databases. This should cause some alarm. It is a fundamental
property of two-level experiments that it can only model linear
effects. If the effect is truly linear, and the variance is small, the
two levels can be quite close together and still yield a reasonable
estimate for the effect.\todo{ikke helt god diskusjon, øke avstand vil
gi klarere effekter, i lineær sammenheng} However, if the effect is non-linear, the
choice of levels becomes critical. In this case, the experimenter
should be concerned that they may have a non-linear effect that is not
well represented by the sizes of data-sets. Such reasoning should be
applied to all significant effects.



\subsection{Fractional factorial experiment}

As mentioned, the full factorial experiment goes as $2^n$ and becomes
prohibitively expensive when $n$ is a large number of
factors. As the evaluation of SPARQL endpoints is an inherently complex
problem, a large number of factors are needed, and so full experiments
cannot scale. However, we may reduce the size of the experiment
significantly by sacrificing some explanatory power, in the form of
fractional factorial experiments. 

We loose explanatory power due to \emph{aliasing}, for example, the
``TripleC:BGPComp'' interaction may be aliased with the
``Machine:Range'' interaction. That is to say, a detected increase in
run time from larger Basic Graph Patterns for large databases, can
also be explained by a less powerful machine evaluation FILTER clauses
with ranges. They are indistinguishable. This may or may not cause a
problem. In many cases, we may not be interested in these effects, we
may only be interested in ``Implement'' and its
interactions. Moreover, it is possible (even easy using the FrF2
package in R) to declare which effects must not be aliased, and
determine the size of the experiment based on that. Such a main effect
or two-factor interaction is called \emph{clear} if none of its
aliases are other main effects or two-factor interactions. Another
possibility to shrink the size of the experiment is if we \textit{a
  priori} can say that some effects are negligible. We have not found
such assumptions to be tenable in our case.

We have made two fractional factorial designs: One with 32 runs and
one with 64 runs. Again, the number of runs is in powers of 2, but
much smaller. In both cases, we have specified that all the factors
must be clear and also all two-factor interactions where ``Implement''
is involved must be clear. \todo{declare generators?}

The resulting normal plot is in figure~\ref{fig:frac32normal}. We see that
only two main effects are significant on a $\alpha = 0.05$ level
according to the Lenth criterion, ``Implement'' is again deemed a
significant improvement, and ``Union'' is deemed significantly the
hardest operation for the query engine. We see that some other points
also deviate from the straight line, but with as few runs as this, the
total variance is great, so no other conclusions can be drawn. Also,
the Student's t-test we used in the previous section cannot be
employed, as we have no additional data to use for the test.

For any other purpose than a rough idea of some key influences on the
overall performance of the endpoint for the given levels, this little
experiment is insufficient.

We turn to the 64-run experiment and its normal plot in
figure~\ref{fig:frac64normalalpha005}. We see that the we get more significant
effects, and that they correspond well to those we saw in the full
factorial experiment. They also all have an intuitive
explanation as above. However, we do not see the effects that we found
worrisome in the full factorial design. The ``Implement:TripleC''
interaction emerges only for a level $\alpha=0.15$, see
figure~\ref{fig:frac64normalalpha015}. To ensure that we discover such
cases will be a key challenge in further studies.

Since the 64-run experiment also has 3 inactive factors, ``Machine'',
``TripleC'' and ``Range'', we can again treat it as a $2^5$ experiment
replicated twice. Using the same procedure as above, we find that
again, the hypothesis test indicates that the new implementation is an
improvement at $p = 5.9 \cdot 10^{-6}$. The comparatively higher p-value
is due to the lower number of runs. It is also possible to perform the
per-environmental test, but we refrain from that in the interest of
brevity.\todo{good way to say it?}


\subsection{Fractional factorial experiment with more data}

The importance of finding flaws in the levels is highlighted by the
normal plot in figure~\ref{fig:frac64hugenormal}. The smaller
experiment made it feasible to run with a larger data-set, this
experiment uses the original 15~MTriples data-set of
\cite{mxro:Morsey2011DBpedia}. The picture is now as hinted in the
previous discussion, for larger ``TripleC'', our simulated performance
is actually worse, as evident from the positive
``Implement''. We also see that the interactions ``Implement:Lang''
and ``Implement:TripleC'' is highly significant. It is clear what
happened: For larger ``TripleC'', the delay we introduced in the
langMatches function has a devastating effect, since it delays the
execution for every returned triple, and thereby completely dominates
the execution time. 

Moreover, we note that the ``Machine'' factor has become highly
significant in the other end along with its ``Machine:TripleC''
interaction. One should take careful notice that the ``Machine''
factor encompasses many different things, that should be different
factors; CPU, disks, available RAM, installed software, tuning
parameters and so on. It is advantageous to have such broad factors in
the design in early experiments, because they may point out unexpected
effects that should be further scrutinized. The ``Machine'' factor is
a good example of usage that also helps us manage
complexity: If possible, one may group many factors into one if one is
in doubt of its relevance and break it up in subsequent
experiments. In this case, the cause of the performance problem was
highly audible: It was the old P-ATA disks of the larger machine, but
in general, such clues are not available to the experimenter.

We are forced to conclude that the three preceding experiments are
severely flawed, as they failed to take into account the non-linear
effects arising from larger data sizes on slow hardware. 


Note that we do not take advantage of the usual performance of 4store
in this simulation, to the contrary, we chose 4store because of its
relative simplicity to modify it to suite our needs for illustrative
purposes, which consists of degrading it in many ways. The small
data-sets used here does not imply that the approach will not work for
large data-sets; properly configured, actual optimization will give
very different runtimes, and the small number of runs will make it
possible to evaluate a very large number of factors.



\section{Discussion}

\paragraph{Summary and hypothesis test}

\paragraph{Strategy for evaluating experiment}\todo{Say something
  about examining variance} \todo{Say something about drilling down in
  factors} \todo{Say something about spreading levels}

\paragraph{Fold-over}

\section{Future Work}


\section{Conclusions}



\section*{Acknowledgments}



\bibliographystyle{plain}
%\bibliographystyle{abbrv}
%\bibliographystyle{jbact}
%\bibliographystyle{splncs03}
\bibliography{selectivity,federation,benchmarks,optimization,experimentsperformance}

\end{document}
