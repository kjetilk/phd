%\documentclass{llncs}
\documentclass{article}

\begin{document}
\title{Sharing statistics for SPARQL Federation optimization, with
  emphasis on benchmark quality}
\author{Kjetil Kjernsmo}
\institute{Department of Informatics,
Postboks 1080 Blindern,
0316 Oslo, Norway}



\maketitle

\email{kjekje@ifi.uio.no}

\begin{abstract}


\end{abstract}

\section{Introduction}

Query federation has been an active field for some time, but has until
the advent of the Semantic Web not been used in a highly diverse set
of endpoints, commonly they have been under the control of a single
institution. RDF has a triple-based data model for the Semantic Web,
and SPARQL is a standardized query language to query such data.

Query federation with SPARQL has attracted much attention
from industry and academia alike, and four implementations of basic
query federation were submitted to the SPARQL 1.1 Working Group as
input for the forthcoming
work\footnote{http://www.w3.org/2009/sparql/wiki/Feature:BasicFederatedQuery}. 
The basic query federation feature was
supported by a large number of group members, and the Last Call working
draft of the proposed standard was published on 17 November 2011.

While the basic feature set of the proposed standard can enable users
to create federated queries, it is not of great use as it requires
extensive prior knowledge of both the data to be queried and
performance characteristics of the involved query engines. Without
this knowledge, the overall performance is insufficient for many
practical applications.

I intend to investigate possible remedies to this problem by using
statistical techniques. 

Since the main objective of the proposed work is to create systems
that have sufficient performance for practical applications, it is of
paramount importance to have methodology that can falsify a
hypothesis about an implementation's performance.

In a federated regime, any query is likely to execute queries on
several different SPARQL implementations. Many of those are free
software so the algorithms they use could in principle be analyzed,
but in practice, this would be very time-consuming. Some
implementations may be undocumented or even trade secrets, thus
precluding any such analysis. It is therefore my intention to treat
the individual SPARQL implementations as ``black boxes'' and
exclusively evaluate the performance characteristics by empirical
means, aka ``benchmarking''.

However, I have found the current state of the art in benchmarking
lacking in its use of statistics, which motivates two distincts
directions of work: \emph{Statistical experimental planning and execution in
benchmarking} and \emph{statistical methods to optimize SPARQL queries
in a federated regime}.

\section{Problem Definition}

\subsection{In SPARQL Federation}

This part of the thesis addresses problems of query optimization in a
federated regime.

\subsection{In Benchmarking}

The contemporary problem in benchmarking is to distinguish
implementations when the measured difference is little or susceptible
to random noise.

Current common practice in benchmarking SPARQL-enabled systems is to
use or synthesize a certain dataset, then formulate a number of
queries seen as representative of SPARQL use in some way. These
queries are then executed, and some characteristic of performance is
measured, for example the time it takes for the engine to return the
full result. Commonly, this process is repeated a number of times, and
an average response time is found. The performance of different
engines can then be compared based on these averages.

In many cases, this is sufficient. Sometimes, one engine can execute a
query an order of magnitude faster than another, in which case, there
is hardly reasonable doubt which is faster.

When the difference is little, we must consider the possibility that
the measurements are impacted by random noise. Computer systems are
generally very deterministic systems, but there are nevertheless many
such scenarios: The underlying system may be doing other tasks. This
is likely to be particularly important in enterprise environments
where high-load applications are often running on virtual
machines. The engine may use randomized algorithms, or the data may be
of such a nature it triggers worst-case behaviour in situations the
query writer cannot foresee. All this contributes to that we should
consider the performance metric, whatever metric is used, as a
\emph{stochastic variable}.

Doing this opens new methodological possibilities, first and foremost
using well-established statistical hypothesis testing rather than just
comparing averages. 

With the currently published benchmarks, it is up to the user to
select an engine with the best characteristics relative to their own
expected application. Another possibility that opens is to better
answer the question ``what is the best general-purpose triple
store?''. I.e. can we collapse all the metrics into a single
\emph{test statistic} that can summarize the performance of the engine
as a whole? Also, can we provide a system to developers that they can
use to see if any changes they do to the engine has unexpected adverse
effects on other parts of the system?

The above relies on that the test data and queries are in fact
representative of the system as a whole, but this is usually not proven.
In physical science and engineering, conventional wisdom
has been that you should only vary one variable at a time to study the
effects of that one variable. In medical science, this has been
abandoned several decades ago, thanks to advances in statistics. In
e.g. a case where the researcher administrates different treatments to
terminally ill patients, some of which may be painful or shorten their
lives, experimental economy is extremely important.

Using techniques from statistical experimental design, I conjecture
that it is possible to design an experiment (i.e. a benchmark) so
that it provably representative of all realistic situations. This
would permit us to make rather categorical statements regarding
overall query engine performance.


\section{State of the Art}

\subsection{In SPARQL Federation}

\subsubsection{Technology}

I take the technology state of the art to be represented by the
current basic SPARQL 1.1 Federated Query Working
Draft\footnote{http://www.w3.org/TR/2011/WD-sparql11-federated-query-20111117/}. For
an academic treatment of the current specification, see
\cite{springerlink:10.1007/978-3-642-21064-8_1} and references therein.

\subsubsection{Science}

In \cite{springerlink:10.1007/978-3-642-21064-8_1}, the authors show
an optimization strategy based on execution order of different
algebraic elements. % TODO: wording here

In \cite{5337556}, it is shown how histograms can be computed to
assist the SemWIQ optimizer, written by the same authors.
% RDFStats

\subsection{In Benchmarking}

Numerous benchmarks have been developed for SPARQL, but
\cite{Duan:2011:AOC:1989323.1989340} showed that currently, most
benchmarks poorly represent the typical data and queries that is used
on the Semantic Web.

Most recently, \cite{mxro:Morsey2011DBpedia} addressed some of these
problems by using real data and real queries from DBpedia.

The problems addressed by these studies are almost orthogonal to the
problems considered by my proposed project. I have not to date seen
any work towards using contemporary statistical methods to evaluate
the performance of software, but I shall admit that my literature
study have been limited to RDF/OWL databases, and practical benchmarks
of databases, file systems, scientific software, etc. Thus, the
existence of relevant references in the deeper computer science
literature is a key issue I would appreciate discussing at the
Symposium.


\section{Proposed Approach and Methodology}

\subsection{In SPARQL Federation}

There are many possible approaches for this part of the thesis. At
this stage, it seems that use of selectivity estimation may be the
most feasible direction.

\subsection{In Benchmarking}

This part of the thesis seeks to use advances from statistical
experimental planning to improve accuracy and dependability of benchmarking.

\section{Conclusion}


\end{document}
