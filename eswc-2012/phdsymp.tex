\documentclass{llncs}
%\documentclass{article}
\usepackage{cite}

\title{Sharing statistics for SPARQL Federation optimization, with
  emphasis on benchmark quality}
\titlerunning{Sharing statistics and benchmarking for SPARQL federation}
\author{Kjetil Kjernsmo\inst{1}}
\institute{Department of Informatics,
Postboks 1080 Blindern,
0316 Oslo, Norway
\email{kjekje@ifi.uio.no}}


\begin{document}

\maketitle



\begin{abstract}
  Federation of semantic data on SPARQL endpoints will allow data to
  remain distributed so that they can be controlled by local curators
  and swiftly updated. There are considerable performance problems,
  which the present work proposes to address mainly by computation
  and exposure of statistical digests to assist selectivity
  estimation.

  Discontent with current state of the art in benchmarking prompts the
  author to focus on the evaluation problem and to challenge the
  notion that a ``one-size-fits-all'' benchmark is impossible.
\end{abstract}

\section{Introduction}

Query federation with SPARQL has attracted much attention
from industry and academia alike, and four implementations of basic
query federation were submitted to the SPARQL 1.1 Working Group as
input for the forthcoming
work\footnote{http://www.w3.org/2009/sparql/wiki/Feature:BasicFederatedQuery}. 
The basic query federation feature was
supported by a large number of group members, and the Last Call working
draft of the proposed standard was published on 17~November~2011.

While the basic feature set of the proposed standard can enable users
to create federated queries, it is not of great use as it requires
extensive prior knowledge of both the data to be queried and
performance characteristics of the involved query engines. Without
this knowledge, the overall performance is insufficient for most
practical applications.

To help optimize, SPARQL endpoints should expose details about both
data and performance characteristics of the engine itself. The
proposed work has two focal points: \emph{Statistical digests of
  data for optimizations} and \emph{benchmarking SPARQL engines}. 

The focus on SPARQL benchmarking is not just motivated from the
optimization perspective, as I have found the current state of the art
in SPARQL benchmarking lacking in its use of statistics. The emphasis
in the present paper is on statistics in benchmarking with the purpose
of putting it on a firmer foundation from a hypothesis testing
perspective.

I am still in an early phase of my work, and I have not yet started to
explore the scientific literature around SPARQL Federation to any
depth, currently focusing my efforts on benchmarking. The long-term
goal of my work is SPARQL Federation, but it is a minor concern in
this paper.

\section{Problem Definition}

\subsection{In SPARQL Federation}

This part of the thesis addresses problems of query optimization in a
federated regime.

When using FedBench, (see \cite{Schmidt:2011:FBS:2063016.2063054}),
the authors note that the absence of statistics has very adverse
effects on performance as join order is impossible to optimize. It is
also pointed out that it causes other problems as well, for example a
high number of requests to federation members. 

This problem has already received some attention, \cite{5337556}
addressed this problem by making it possible to exchange histograms
and other data. However, histogram approaches generally suffers from
the problem that they grow too large or become a insufficiently
accurate digest, especially in face of very heterogeneous data.

Thus, the core problem is: How do we compute and expose a digest that
is of optimal size for the query performance problem?

\subsection{In Benchmarking}

The contemporary problem in benchmarking is to distinguish
implementations when the measured difference is small or susceptible
to random noise.

Current common practice in benchmarking SPARQL-enabled systems is to
use or synthesize a certain dataset, then formulate a number of
queries seen as representative of SPARQL use in some way. These
queries are then executed, and some characteristic of performance is
measured, for example the time it takes for the engine to return the
full result. Commonly, this process is repeated a number of times, and
an average response time is found. The performance of different
engines can then be compared based on these averages.

In many cases, this is sufficient. Sometimes, one engine can execute a
query an order of magnitude faster than another, in which case, there
is hardly reasonable doubt which is faster. Small differences may seem
unimportant but may become important if they are systematic. Even if
one engine is dramatically better than another in one case, small
deficiencies may add up to make the other a better choice for most
applications anyway.

In this case, we must consider the possibility that the measurements
are impacted by random noise. Computer systems are generally very
deterministic systems, but there are nevertheless many such scenarios:
The underlying system may be doing other tasks. The garbage collection
of some languages may trigger at not easily foreseeable points. The
engine may use randomized algorithms, or the data may be of such a
nature that it triggers worst-case behaviour in situations the query
writer cannot foresee. All this contributes to that we should consider
the performance metric, whatever metric is used, as a \emph{stochastic
  variable}.

Doing this opens new methodological possibilities, first and foremost
using well-established statistical hypothesis testing rather than just
comparing averages. 

With the currently published benchmarks, it is up to the user to
select an engine with the best characteristics relative to their own
expected application. However, I am often asked the question: ``what
is the best general-purpose triple store?'' and those who ask are not
prepared to provide any further qualification. This prompts the
question: Can we collapse all the metrics into a single \emph{test
  statistic} that can summarize the performance of the engine as a
whole? Also, can we provide a system to developers that they can use
to see if any changes they do to the engine has unexpected adverse
effects on other parts of the system? For those who are able to
qualify their inquiry, weighing different parts of the system or
metrics is more useful and will yield a more tenable answer, and
motivates the question: Can we use the system to better find the
distinguishing characteristics so that we can return a question that
will actually help new users of SPARQL engines to chose? 

The above relies on data and queries that are in fact
representative of the system as a whole. It is often asserted that it
is not possible to create a ``one-size-fits-all'' benchmark (see
e.g. \cite{Schmidt:2011:FBS:2063016.2063054}) to cover all aspects of
an application. While that may be true in the general case, I want to
challenge the assumption that the best we can do is to accommodate a
multitude of dimensions and essential challenges in the case of
SPARQL.

In physical science and engineering, conventional wisdom has been that
you should only vary one variable at a time to study the effects of
that one variable. In medical science, this has been abandoned several
decades ago, thanks to advances in statistics. In e.g. a case where
the researcher administrates different treatments to terminally ill
patients, some of which may be painful or shorten their lives,
experimental economy is extremely important.

Using techniques from statistical experimental design, I propose
that it is possible to design an experiment (i.e. a benchmark) so that
we can prove it covers all realistic cases and that we can justify
why the remaining corner cases are unlikely to influence the
result. For further elaboration, see Section~\ref{sec:benchmethod}.

Thus far, the benchmarking problem has been seen as a software testing
problem, but we could also ask if the existence of benchmark data can
be exposed to help federation query optimizers along with a
statistical digest.

\section{State of the Art}

\subsection{In SPARQL Federation}

I take the technology state of the art to be represented by the
current basic SPARQL 1.1 Federated Query Working
Draft\footnote{http://www.w3.org/TR/2011/WD-sparql11-federated-query-20111117/}. 

A recent scientific treatment of the current specification, is in
\cite{springerlink:10.1007/978-3-642-21064-8-1}. In this paper, the
authors also show an optimization strategy based on execution order of
so-called well-designed patterns.

In \cite{5337556}, it is shown how various statistics-gathering
techniques, including histograms can be computed to assist the SemWIQ
optimizer, written by the same authors.
\cite{Harth:2010:DSO:1772690.1772733} introduced QTrees, which may
alleviate the problem of histogram size, but it may not solve it.

For benchmarking federated query processing strategies,
\cite{Schmidt:2011:FBS:2063016.2063054} report the development of \emph{FedBench}.



\subsection{In Benchmarking}

Numerous benchmarks have been developed for SPARQL, but
\cite{Duan:2011:AOC:1989323.1989340} showed that currently, most
benchmarks poorly represent the typical data and queries that is used
on the Semantic Web.

Most recently, \cite{mxro:Morsey2011DBpedia} addressed some of these
problems by using real data and real queries from DBpedia.

The problems addressed by these studies are almost orthogonal to the
problems considered by my proposed project. While I have heard of some
cases where a hypothesis test is used to demonstrate that one
implementation is better than another, it is seemingly not common
practice. Furthermore, I have not to date seen any work towards using
contemporary statistical methods to evaluate the performance of
software, but I shall admit that my literature study have been limited
to RDF/OWL databases, and practical benchmarks of databases, file
systems, scientific software, etc. Thus, the existence of relevant
references in the deeper computer science literature is a key issue I
would appreciate discussing at the Symposium.


\section{Proposed Approach and Methodology}

\subsection{In SPARQL Federation}

There are many possible approaches for this part of the thesis. As I
expect great advances to be made before I start tackling this problem,
I have not chosen any methodology, but work to find more
space-efficient ways to expose statistics in the service description
and standardize those seems like an interesting direction. 

To this end, I briefly looked into two approaches:
\cite{Getoor:2001:SEU:375663.375727} used Bayesian Networks and
Probabilistic Relational Models to efficiently represent the joint
distribution of database tables, a formalism that could be extended to
RDF databases.

Another approach that I have not seen used in the literature is to use
parametrized statistics. This would amount to attempt fitting data to
a known distribution function and expose which distribution and its
parameters in the service description.

Finally, I have seen little work on the problem of rapidly changing
data, so the adaption of existing techniques to such situations may
also be a interesting problem.


\subsection{In Benchmarking}\label{sec:benchmethod}

This part of the thesis seeks to use advances from statistical
experimental planning to improve accuracy and dependability of
benchmarking. Already in 1926, Ronald~Fischer noted that complex
experiments can be much more efficient than simple ones\footnote{Cited
  in http://en.wikipedia.org/wiki/Factorial\_experiment}, starting the
experimental design field. One of the simpler designs is ``fractional
factorial design'', in which several ``factors'' are studied by using
different ``treatments''. In terms of SPARQL execution, the SPARQL
engine is clearly a factor, but also for example the nestedness of
\texttt{OPTIONAL}s can be a factor, number of triples in a basic graph
pattern, etc, and the treatment is to vary these numbers to different
``levels''. The key to understanding why this can be efficient is that
these variations need not occur in the same experiment. Thus, many
combinations of factors can be studied by carefully designing queries
to cover different factors, and a formalism called ``resolution'' has
been developed to classify how well this has been achieved.

Fractional factorial design is covered in elementary text-books in
statistics but is most likely inadequate for this purpose, so I
intend to go further into the statistical literature to see if there
is methodology that is even better suited to the problem. Armed with
this, I hope to use amongst other things, the complexity analysis of
SPARQL (see e.g. \cite{Schmidt:2010:FSQ:1804669.1804675}) to find
suitable factors to see if my admittedly bold proposition that it is
possible to design a benchmark to cover all realistic cases.

%\section{Conclusion}

%\bibliographystyle{plain}
%\bibliographystyle{plain-csmin}
\bibliographystyle{splncs03}
\bibliography{selectivity,federation,benchmarks,optimization}

\end{document}
