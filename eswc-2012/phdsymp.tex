\documentclass{llncs}
%\documentclass{article}
\usepackage{cite}

\title{Sharing statistics for SPARQL Federation optimization, with
  emphasis on benchmark quality}
\titlerunning{Sharing statistics and benchmarking for SPARQL federation}
\author{Kjetil Kjernsmo\inst{1}}
\institute{Department of Informatics,
Postboks 1080 Blindern,
0316 Oslo, Norway
\email{kjekje@ifi.uio.no}}


\begin{document}

\maketitle



\begin{abstract}
  Federation of semantic data on SPARQL endpoints will allow data to
  remain distributed so that it can be controlled by local curators
  and swiftly updated. There are considerable performance problems,
  which the present work proposes to address, mainly by computation
  and exposure of statistical digests to assist selectivity
  estimation.

  Discontent with current state of the art in benchmarking prompts the
  author to focus on the evaluation problem and to investigate whether
  we can provide better advices to new users selecting a SPARQL engine
  to use.
\end{abstract}

\section{Motivation}

Query federation with SPARQL, which is a standardized query language
for the Semantic Web, has attracted much attention
from industry and academia alike, and four implementations of basic
query federation were submitted to the SPARQL 1.1 Working Group as
input for the forthcoming
work\footnote{http://www.w3.org/2009/sparql/wiki/Feature:BasicFederatedQuery}. 
This feature was
supported by many group members, and the Last Call working
draft of the proposed standard was published on 17~November~2011.

While the basic feature set of the proposed standard can enable users
to create federated queries, it is not of great use as it requires
extensive prior knowledge of both the data to be queried and
performance characteristics of the involved query engines. Without
this knowledge, the overall performance is insufficient for most
practical applications.

To aid optimization, SPARQL endpoints should expose details about both
data and performance characteristics of the engine itself. The
proposed work has two focal points: \emph{Statistical digests of
  data for optimizations} and \emph{benchmarking SPARQL engines}. 

The focus on SPARQL benchmarking is not only motivated from the
perspective of optimization, as I have found the current state of the art
in SPARQL benchmarking lacking in its use of statistics. The emphasis
in the present paper is on statistics in benchmarking with the purpose
of providing a firmer foundation on which assertions about engine
performance can be backed with evidence. 

The prior art in database theory is extensive, but I intend to focus
on aspects that sets SPARQL apart from e.g. SQL, like the quad model,
that it is a Web technology, or that data are commonly very
heterogenous.

I have not yet started to explore the scientific literature around
SPARQL Federation in any depth as I am still in an early phase of my
work. I am currently focusing my efforts on benchmarking. The
long-term goal of my work is SPARQL Federation, but that is a minor
concern in this paper. Overall, the expected contributions are:

\begin{itemize}
\item Benchmarks that are able to cover all realistic
  performance-influencing parameters for SPARQL engines and make it
  possible to weigh different parameters, as well as quantify
  unexplained differences. 

\item To assist optimization, enable endpoint service descriptions to
  expose:
\begin{itemize}
\item performance characteristics,
\item statistical digests of data that are optimized with respect to
  size and query performance.
\end{itemize}
\end{itemize}


\section{State of the Art and Open Problems}

\subsection{In SPARQL Federation}

I take the state of the art in technology to be represented by the
current basic SPARQL 1.1 Federated Query Working
Draft\footnote{http://www.w3.org/TR/2011/WD-sparql11-federated-query-20111117/}. In
addition, many have implemented federation that doesn't require 
explicit references to service endpoints, e.g.~\cite{springerlink:10.1007/978-3-642-25073-6-38}.
A recent scientific treatment of the current specification is in
\cite{springerlink:10.1007/978-3-642-21064-8-1}. In that paper, the
authors also show an optimization strategy based on execution order of
so-called well-designed patterns.

A recent review of the state of the art is in
\cite{springerlink:10.1007/978-3-642-17551-0-5}. In addition,
\cite{springerlink:10.1007/978-3-642-25073-6-38} proposes
\emph{bound joins} and proves they can dramatically reduce the number of
requests to federation members that are needed, as well as the implementation of
FedX.

It has been my intention to focus on the two problems listed in
section~3.3.1
in~\cite{springerlink:10.1007/978-3-642-17551-0-5}, i.e. strike a
balance between accuracy and index size, and updating statistics as
data changes. Notably, histogram
approaches generally suffer from the problem that they grow too large
or become an insufficiently accurate digest, especially in the face of
very heterogeneous data.  \cite{Harth:2010:DSO:1772690.1772733}
introduced QTrees, which may alleviate the problem of histogram size,
but which may not solve it.

Therefore, the core problem is: How do we compute and expose a digest that
is of optimal size for the query performance problem?

\subsection{In Benchmarking}

Numerous benchmarks have been developed for SPARQL, but
\cite{Duan:2011:AOC:1989323.1989340} showed that currently most
benchmarks poorly represent the typical data and queries that are used
on the Semantic Web.
Most recently, \cite{mxro:Morsey2011DBpedia} addressed some of these
problems by using real data and real queries from
DBpedia. \cite{Schmidt:2011:FBS:2063016.2063054} has developed a
benchmark for the federated case.

Current common practice in benchmarking SPARQL-enabled systems is to
use or synthesize a certain dataset, then formulate a number of
queries seen as representative of SPARQL use in some way. These
queries are then executed, and some characteristic of performance is
measured, for example the time it takes for the engine to return the
full result. Since there is a certain randomness in query times, this
process is repeated a number of times and an average response time is
found. The performance of different engines can then be compared,
based on these averages.

In many cases, this is sufficient. Sometimes, one engine can execute a
query in an order of magnitude faster than another. If this happens
systematically for many different queries, there is hardly reasonable
doubt as to which is faster. In most cases, the query response times
differs little, however. Small differences may seem unimportant
but may become important if they are systematic. Even if one engine is
dramatically better than another in one case, small deficiencies may
add up to make the other a better choice for most applications anyway.

In this case, we must consider the possibility that the random noise
can influence the conclusions. Whatever metric is used, it should be
treated as a \emph{stochastic variable}.

This opens new methodological possibilities, first and foremost
using well-established statistical hypothesis testing or ranking
rather than just comparing averages.

With the currently published benchmarks, it is up to the user to
select an engine with the best characteristics, relative to their own
expected application. New users often are unable to formulate
\emph{any} relevant expectations. For these users, the wide variety of
benchmarks, for instance by the TPC, offers little help as a thorough
understanding of the benchmarks and what they measure is required to
select an engine. To help these users, we should ask ourselves:
Can we collapse all the metrics into a single \emph{test
  statistic} that can summarize the performance of the engine as a
whole? Also, can we provide a system for developers that they can use
to see if any changes they do to the engine can have unexpected adverse
effects on other parts of the system? For those who can formulate
relevant expectations, weighing different parts of the system or
metrics is more useful and will yield a more tenable answer, and
motivates the question: Can we use the system to better find the
distinguishing characteristics so that we can return a question that
will actually help new users of SPARQL engines to chose one that meets
their needs?

The above relies on data and queries that are representative of the
system as a whole. It is often asserted that it is not possible to
create a ``one-size-fits-all'' benchmark but answering the above
question requires such a benchmark. The relative simplicity of RDF and
SPARQL may make this feasible.

In physical science and engineering, conventional wisdom has been that
you should only vary one variable at a time to study the effects of
that one variable. In medical science, this has been abandoned several
decades ago, thanks to advances in statistics. In for example a case where
the researcher administrates different treatments to terminally ill
patients, some of which may be painful or shorten their lives,
experimental economy is extremely important.

Using techniques from statistical experimental design, I propose
that it is possible to design an experiment (i.e. a benchmark) which
makes it possible to cover all realistic cases and with which we can justify
why the remaining corner cases are unlikely to influence the
result. For further elaboration, see Section~\ref{sec:benchmethod}.

So far, the benchmarking problem has been seen as a software testing
problem, but as stated in the introduction this is not the only
objective, we may now see if benchmark data can be exposed to help
federation query optimizers along with a statistical digest.

The problems addressed by existing benchmarks such as the ones cited
above are almost orthogonal to the problems considered by my proposed
project. While I have seen some cases that compare performance based
on box plots\footnote{See \url{http://shootout.alioth.debian.org/} for
  example}, it seems not to be common practice. Furthermore, I have not to date seen
any work towards using methods like factorial designs to evaluate
the performance of software, but there may be a limit in terms of
complexity for where it is feasible, and I will restrict myself to
SPARQL for this thesis.



\section{Proposed Approach and Methodology}

\subsection{In SPARQL Federation}

There are many possible approaches for this part of the thesis. As I
expect substantial advances to be made before I start tackling this
problem, I have not chosen any methodology, but an interesting
direction for work seems to be to find more space-efficient ways to
expose statistics in the service description and standardize them.

To this end, I have briefly looked into two approaches:
\cite{Getoor:2001:SEU:375663.375727} used Bayesian Networks and
Probabilistic Relational Models to efficiently represent the joint
distribution of database tables, a formalism that could be extended to
RDF databases.

Another approach that I have not seen used in the literature is to use
parametrized statistics. This would amount to an attempt to fit data to
a known distribution function and expose which distribution and its
parameters in the service description.

Finally, I have seen little work on the problem of rapidly changing
data, so the adaption of existing techniques to such situations may
also be an interesting problem.

The evaluation methodology for the SPARQL federation work of the
thesis will largely be covered by running the elaborate benchmark
designs of the thesis.

\subsection{In Benchmarking}\label{sec:benchmethod}

Already in 1926, Ronald~Fischer noted that complex experiments can be
much more efficient than simple ones\footnote{Cited in
  http://en.wikipedia.org/wiki/Factorial\_experiment}, starting the
experimental design field. One of the simpler designs is ``fractional
factorial design'', in which several ``factors'' are studied. In terms
of SPARQL execution, the SPARQL engine is clearly a factor, but also,
for example, the nestedness of \texttt{OPTIONAL}s can be a factor, or
the number of triples in a basic graph pattern, etc. These numbers are
varied to different ``levels''. The key to understanding why this can
be efficient is that these variations need not occur in the same
experiment. Therefore, many combinations of factors can be studied by
carefully designing queries to cover different factors, and a
formalism called ``resolution'' has been developed to classify how
well this has been achieved, partly answering the question of
evaluation methodology for this part of the thesis. We should also
validate by comparing this benchmark with conclusions from existing
benchmarks.

Factorial design inspires the present work and is covered in
elementary text-books in statistics but is inadequate for this
purpose, so I intend to go further into the statistical literature to
see if there is a methodology that is better suited to the
problem. I also intend to use existing results on complexity bounds
for SPARQL query answering to find suitable factors to see if my
admittedly bold proposition that it is possible to design a benchmark
to cover all realistic cases can be shown to hold.

With this analysis, I speculate based on superficial experience with
factorial designs and analysis of variance that certain estimated
coefficients can be exposed in the service description to give
federated query engines assistance in optimizing for performance
characteristics of certain SPARQL implementations. Adjusting how
different features are weighed will help those who know details of
their expected SPARQL use and normalization-techniques of the weights
will answer the special case of ``the best general-purpose SPARQL
engine''.

\bibliographystyle{plain-csmin}
%\bibliographystyle{abbrv}
%\bibliographystyle{jbact}
%\bibliographystyle{splncs03}
\bibliography{selectivity,federation,benchmarks,optimization}

\end{document}
