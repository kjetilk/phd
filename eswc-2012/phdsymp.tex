\documentclass{llncs}
%\documentclass{article}
\usepackage{cite}

\title{Sharing statistics for SPARQL Federation optimization, with
  emphasis on benchmark quality}
\author{Kjetil Kjernsmo}
\institute{Department of Informatics,
Postboks 1080 Blindern,
0316 Oslo, Norway}


\begin{document}

\maketitle
\email{kjekje@ifi.uio.no}


\begin{abstract}


\end{abstract}

\section{Introduction}

Query federation has been an active field for some time, but has until
the advent of the Semantic Web not been used in a highly diverse set
of endpoints, commonly they have been under the control of a single
institution. RDF has a triple-based data model for the Semantic Web,
and SPARQL is a standardized query language to query such data.

Query federation with SPARQL has attracted much attention
from industry and academia alike, and four implementations of basic
query federation were submitted to the SPARQL 1.1 Working Group as
input for the forthcoming
work\footnote{http://www.w3.org/2009/sparql/wiki/Feature:BasicFederatedQuery}. 
The basic query federation feature was
supported by a large number of group members, and the Last Call working
draft of the proposed standard was published on 17~November~2011.

While the basic feature set of the proposed standard can enable users
to create federated queries, it is not of great use as it requires
extensive prior knowledge of both the data to be queried and
performance characteristics of the involved query engines. Without
this knowledge, the overall performance is insufficient for many
practical applications.

I intend to investigate possible remedies to this problem by using
statistical techniques. 

Since the main objective of the proposed work is to create systems
that have sufficient performance for practical applications, it is of
paramount importance to have methodology that can falsify a
hypothesis about an implementation's performance.

In a federated regime, any query is likely to execute queries on
several different SPARQL implementations. Many of those are free
software so the algorithms they use could in principle be analyzed,
but in practice, this would be very time-consuming. Some
implementations may be undocumented or even trade secrets, thus
precluding any such analysis. It is therefore my intention to treat
the individual SPARQL implementations as ``black boxes'' and
exclusively evaluate the performance characteristics by empirical
means, aka ``benchmarking''.

However, I have found the current state of the art in benchmarking
lacking in its use of statistics, which motivates two distincts
directions of work: \emph{Statistical experimental planning and execution in
benchmarking} and \emph{statistical methods to optimize SPARQL queries
in a federated regime}.

I am still in an early phase of my work, and I have not yet started to
explore the scientific literature around SPARQL Federation to any
depth, currently focusing my efforts on benchmarking. Therefore, this
paper also focuses on benchmarking rather than the more long-term goal
of SPARQL Federation.

\section{Problem Definition}

\subsection{In SPARQL Federation}

This part of the thesis addresses problems of query optimization in a
federated regime.

When using FedBench, (see \cite{Schmidt:2011:FBS:2063016.2063054}),
the authors note that the absence of statistics has very adverse
effects on performance as join order is impossible to optimize. It is
also pointed out that it causes other problems as well, for example a
high number of requests to federation members. 

This problem has already received some attention, \cite{5337556}
addressed this problem by making it possible to exchange histograms
and other data. However, histogram approaches generally suffers from
the problem that they grow too large or become a insufficiently
accurate digest, especially in face of very heterogenous data.


\subsection{In Benchmarking}

The contemporary problem in benchmarking is to distinguish
implementations when the measured difference is little or susceptible
to random noise.

Current common practice in benchmarking SPARQL-enabled systems is to
use or synthesize a certain dataset, then formulate a number of
queries seen as representative of SPARQL use in some way. These
queries are then executed, and some characteristic of performance is
measured, for example the time it takes for the engine to return the
full result. Commonly, this process is repeated a number of times, and
an average response time is found. The performance of different
engines can then be compared based on these averages.

In many cases, this is sufficient. Sometimes, one engine can execute a
query an order of magnitude faster than another, in which case, there
is hardly reasonable doubt which is faster.

When the difference is little, we must consider the possibility that
the measurements are impacted by random noise. Computer systems are
generally very deterministic systems, but there are nevertheless many
such scenarios: The underlying system may be doing other tasks. This
is likely to be particularly important in enterprise environments
where high-load applications are often running on virtual
machines. The garbage collection of some languages may trigger on not
easily foreseeable points. The engine may use randomized algorithms,
or the data may be of such a nature it triggers worst-case behaviour
in situations the query writer cannot foresee. All this contributes to
that we should consider the performance metric, whatever metric is
used, as a \emph{stochastic variable}.

Doing this opens new methodological possibilities, first and foremost
using well-established statistical hypothesis testing rather than just
comparing averages. 

With the currently published benchmarks, it is up to the user to
select an engine with the best characteristics relative to their own
expected application. Another possibility that opens is to better
answer the question ``what is the best general-purpose triple
store?''. I.e. can we collapse all the metrics into a single
\emph{test statistic} that can summarize the performance of the engine
as a whole? Also, can we provide a system to developers that they can
use to see if any changes they do to the engine has unexpected adverse
effects on other parts of the system?

The above relies on that the test data and queries are in fact
representative of the system as a whole. It is often asserted that it
is not possible to create a ``one-size-fits-all'' benchmark (see
e.g. \cite{Schmidt:2011:FBS:2063016.2063054}) to cover all aspects of
an application. While that may be true in the general case, I want to
challenge the assumption that the best we can do is to accommodate a
multitude of dimensions and essential challenges in the case of
SPARQL.

In physical science and engineering, conventional wisdom has been that
you should only vary one variable at a time to study the effects of
that one variable. In medical science, this has been abandoned several
decades ago, thanks to advances in statistics. In e.g. a case where
the researcher administrates different treatments to terminally ill
patients, some of which may be painful or shorten their lives,
experimental economy is extremely important.

Using techniques from statistical experimental design, I conjecture
that it is possible to design an experiment (i.e. a benchmark) so that
we can prove it covers all realistic cases and that we can justify
why the remaining corner cases are unlikely to influence the result.


\section{State of the Art}

\subsection{In SPARQL Federation}

I take the technology state of the art to be represented by the
current basic SPARQL 1.1 Federated Query Working
Draft\footnote{http://www.w3.org/TR/2011/WD-sparql11-federated-query-20111117/}. 

A recent scientific treatment of the current specification, is in
\cite{springerlink:10.1007/978-3-642-21064-8-1}. In this paper, the
authors also show an optimization strategy based on execution order of
different algebraic elements. % TODO: wording here

In \cite{5337556}, it is shown how various statistics-gathering
techniques, including histograms can be computed to assist the SemWIQ
optimizer, written by the same authors.
\cite{Harth:2010:DSO:1772690.1772733} introduced QTrees, which may
alleviate the problem of histogram size, but it may not solve it.

For benchmarking federated query processing strategies,
\cite{Schmidt:2011:FBS:2063016.2063054} report the development of \emph{FedBench}.



\subsection{In Benchmarking}

Numerous benchmarks have been developed for SPARQL, but
\cite{Duan:2011:AOC:1989323.1989340} showed that currently, most
benchmarks poorly represent the typical data and queries that is used
on the Semantic Web.

Most recently, \cite{mxro:Morsey2011DBpedia} addressed some of these
problems by using real data and real queries from DBpedia.

The problems addressed by these studies are almost orthogonal to the
problems considered by my proposed project. While I have heard of some
cases where a hypothesis test is used to demonstrate that one
implementation is better than another, it is seemingly not common
practice. Furthermore, I have not to date seen any work towards using
contemporary statistical methods to evaluate the performance of
software, but I shall admit that my literature study have been limited
to RDF/OWL databases, and practical benchmarks of databases, file
systems, scientific software, etc. Thus, the existence of relevant
references in the deeper computer science literature is a key issue I
would appreciate discussing at the Symposium.


\section{Proposed Approach and Methodology}

\subsection{In SPARQL Federation}

There are many possible approaches for this part of the thesis. As I
expect great advances to be made before I start tackling this problem,
I have not chosen any methodology, but work to find more
space-efficient ways to expose statistics in the service description
and standardize those seems like an interesting direction. 

To this end, I briefly looked into two approaches:
\cite{Getoor:2001:SEU:375663.375727} used Bayesian Networks and
Probabilistic Relational Models to efficiently represent the joint
distribution of database tables, a formalism that could be extended to
RDF databases.

Another approach that I have not seen used in the literature is to use
parameterized statistics. This would amount to attempt fitting data to
a known distribution function and expose which distribution and its
parameters in the service description.

Finally, I have seen little work on the problem of rapidly changing
data, so the adaption of existing techniques to such situations may
also be a interesting problem.


\subsection{In Benchmarking}

This part of the thesis seeks to use advances from statistical
experimental planning to improve accuracy and dependability of benchmarking.

\section{Conclusion}

\bibliographystyle{plain}
\bibliography{selectivity,federation,benchmarks}

\end{document}
