\documentclass{llncs}
%\documentclass{article}
\usepackage{cite}

% TODO:
% References
% POST in relation to graph store
% Weakness of RDF
% Reformulate something on SPARQL about enabling
% More writing around factors?

\title{The necessity of hypermedia RDF and an approach to achieve it}
\titlerunning{A hypermedia RDF approach}
\author{Kjetil Kjernsmo\inst{1}}
\institute{Department of Informatics,
Postboks 1080 Blindern,
0316 Oslo, Norway
\email{kjekje@ifi.uio.no}}


\begin{document}

\maketitle



\begin{abstract}
This paper will give an overview of the practical implications of the
HATEOAS constraint of the REST architectural style, and in that light argue why
hypermedia RDF is a practical necessity. We will then sketch a
vocabulary for hypermedia RDF using Mike Amundsen's H~Factor
classification as motivator. Finally, we will briefly argue that
SPARQL is important when making non-trivial traversals of Linked Data
graphs, and see how a bridge between Linked Data and SPARQL may be
created with hypermedia RDF.
\end{abstract}

\section{Introduction}

Mike Amundsen defines hypermedia types\footnote{\url{http://amundsen.com/hypermedia/}} as 
\begin{quote}
Hypermedia Types are MIME media types that contain native
hyper-linking semantics that induce application flow. For example,
HTML is a hypermedia type; XML is not.
\end{quote}
Furthermore, he defines a classification scheme called H~Factor as ``a
measurement of the level of hypermedia support and sophistication of a
media-type.'' The REST \& WOA Wiki defines ``the Hypermedia
Scale''\footnote{\url{http://restpatterns.org/Articles/The\_Hypermedia\_Scale}},
where the categorization is based on capabilities to do CRUD (Create,
Read, Update, Delete) operations.  Considered on their own, RDF
serializations are hypermedia types, but only at the \textsf{LO}
(outbound links) and \textsf{CL} (control data for links) levels (see
footnotes for details on this notation), and just an R~Type (read) on
the Hypermedia Scale. We aim at improving this situation.

Amundsen argues in a blog
post\footnote{\url{http://amundsen.com/blog/archives/1083}} that the
Semantic Web community should not pursue an API for RDF, but rather
make RDF serializations more powerful hypermedia types. He argues that
a key factor in the success of the Web is that messages not only
contain data but also application control information, and that this is
needed for the Web to scale.

Semantic Web services are likely to be an important source of data for
future applications, but for foreseeable future they are just one of many. A
key promise of the Semantic Web is the ability to integrate many data
sources easily. There are two key issues, first read-write operations
must be similar to how it is done with other hypermedia types, to make
it possible to use generic code to minimize the effort required to
support RDF. Importantly, if interacting with Linked Data requires
extensive out-of-band information, it will be harder to use than media
types that do not. Secondly, it requires relevant links and that the
resulting graph can be traversed by reasonable means.

While links are abundant across the Linked Open Data (LOD) cloud, this
paper will argue that key links are missing to make it possible to
traverse the resulting graph and to enable read-write
operations. Moreover, we will argue that this shortcoming is due to
that the community does not adequately take into account the
constraint known as ``Hypermedia as the Engine of Application State''
(abbreviated HATEOAS) from the REST architectural style, see
\cite{Fielding_2000_Architectural-Styles}~Chapter~5.

In a read-only situation, the HATEOAS constraint requires that the
application can navigate from one resources to another using the
hypermedia links in the present resource \emph{only}, they should not
require any out-of-band information. It is very common that
specifications that advertise themselves as RESTful has a read-write
API that is specified in terms of URI structure as well as HTTP
verbs. This is not a violation of the HATEOAS constraint \textit{per
  se}, but it would usually be superfluous as the same information
\emph{must} be available in a hypermedia message in order to satisfy
the HATEOAS constraint. The constraint does not require this
information to be available in \emph{all} messages, nor does it
require that all resources can be reached from any other resource, and
so, it is not a very strict constraint.



Since RDF is built on URIs, many of which can be dereferenced to
obtain further links, it lends itself well to RESTful protocols, but
it is debatable whether a protocol can be said to be RESTful if the
links don't enable any useful interactions. Importantly, if there is
to be such a thing as a RESTful read-write Semantic Web protocol,
there must be something in the RDF itself that can be used by
practical applications to write data. Therefore, whether the HATEOAS
constraint is satisfied must be judged on the basis of the practical
applications the protocol enables.

\section{Required links}

AtomPub %TODO: ref
is a widely cited as a good example of a RESTful protocol and
we note that it has a single service endpoint. From there, you can
navigate to what you need. This motivates the first discussion topic
of this paper:

\begin{question}
Is a single service endpoint enough for Linked Data?
\end{question}

We note that the distributed graph structure of the LOD Cloud makes
this awkward: For every new data source encountered in a graph
traversal, a new service endpoint must be queried. Moreover, it would
be awkward if fine-grained access controls for writing are being used
to record permissions for all resources in a single service
description. It seems likely that a few triples attached to each
information resource are better suited in most cases.

\section{Defining hypermedia RDF}

We noted that for a read-write RESTful protocol, we need to say in the
RDF message itself what kind of operations can be made. We also note
that only information resources can be manipulated, and we argued that
it should be possible to add the required triples to every
resource. We should be able to define hypermedia RDF in terms of a
minimal vocabulary, but like the H~Factor web page, we will find it
instructive to use examples. In the following, we use Turtle syntax,
with prefixes omitted for brevity. Also, we note that in many cases,
we are making statements about the current resource, which given a
reasonable base URI can be written as \texttt{<>}. We have not found
the following factors to be relevant to RDF: \textsf{LE} (embedded
links), \textsf{CR}, \textsf{CU} (control data for read and update
requests respectively) and \textsf{LT} (templated queries); and
\textsf{LO} and \textsf{CL} are trivially supported. The remaining
are:


\begin{description}

\item[\textsf{LN}] Support for non-idempotent updates (HTTP POST) 

\begin{verbatim}
<> hm:canBe hm:mergedInto ;
   hm:createSimilarAt <../> .
\end{verbatim}

The first triple says that the current resource can be merged with
another resource. In a typical HTTP case, this would be achieved by
POSTing to the current resource with an RDF payload, and the server
would perform a RDF merge of the payload with the current resource.

The second triple exists to make it possible to POST an RDF payload to
some URI and the server will itself assign a URI to the posted RDF
payload.

\item[\textsf{LI}] Support for idempotent updates (HTTP PUT, DELETE) 

\begin{verbatim}
<> hm:canBe hm:replaced, hm:deleted .
\end{verbatim}


The first triple says that the current resource may be replaced by
PUTting an RDF payload to the resource URI. The second triple says
that the resource may be deleted, typically by a HTTP DELETE on the
resource URI.


\item[\textsf{CM}] Support for indicating the interface method for requests
  (e.g. HTTP GET, POST, PUT, DELETE methods).

For example (with similar triples for other HTTP methods):
\begin{verbatim}
hm:replaced hm:httpMethod "PUT" .
\end{verbatim}

\end{description}

We see that with a simple vocabulary, RDF serializations can become a
very powerful hypermedia types. We also note that this vocabulary
enables agents to do the same operations as the SPARQL 1.1 Graph Store
HTTP
Protocol\footnote{\url{http://www.w3.org/TR/2011/WD-sparql11-http-rdf-update-20110512/}},
while being fully RESTful. The Protocol specification is currently not
RESTful per the discussion above as it requires extensive out-of-band
information, even though it was a key design goal. It should also be
possible to use this on the default (nameless) graph by assigning it a
name in the service description rather than defining it in an
out-of-band specification like is currently being done.

Also note the \texttt{hm:mergedInto} term was motivated by that POST
operations in this specification results in an RDF Merge of the
payload into the named graph. 

\begin{question}
Should the SPARQL 1.1 Graph Store HTTP Protocol be replaced by a
vocabulary like the above?
\end{question}

\subsection{New H~Factors}

Mike Amundsen solicits feedback on the completeness of his
classification scheme. I suggest the following as discussion items:

\begin{itemize}

\item Support for self-description.

The self-describing nature of RDF is an important characteristic of
the model and arguably an important characteristic of RDF as
hypermedia as agents may be able to infer possible interactions based
on vocabulary definitions.

\item Support for supplying control data relevant to subsequent requests.

It may be useful for agents to know e.g. what formats they can send to
a given resource before the request is made. This may require a new
control data factor. An example of this use may be e.g.:

\begin{verbatim}
<> hm:acceptsFormat <http://www.w3.org/ns/formats/Turtle> .
\end{verbatim}

\item A factor encompassing RaUL templates. 

  The RaUL\footnote{\url{http://vocab.deri.ie/raul}} goes beyond HTTP
  GET queries, which is what the \textsf{LT} factor is about, but can
  be used to formulate templated queries similar to those in HTML, or
  indeed to generate HTML fragments for the same purpose. This is also
  enables useful interactions, albeit quite different from those of
  this paper.


\end{itemize}


\begin{question}
Can we create a better vocabulary for hypermedia RDF than the sketch above?
\end{question}


\section{Bridging LOD and SPARQL}

In many cases, it is sufficient to get a small number of resources to
collect the data needed for a given usage, but slightly more involved
queries (e.g. ``what kind of connections exists between Kate Bush, Roy
Harper and bands that have sold more than 200 million albums?'') would
likely cause thousands of resources to be downloaded and examined. For
this, more advanced graph pattern matching, as well as more advanced
mechanisms for source selection, is
required. \cite{springerlink:10.1007/978-3-642-25073-6-38} provides
some techniques that should prove very valuable in this respect and so
it becomes important that SPARQL can be used with Linked Data in a
RESTful manner.

In the Linked Data Design
Issue\footnote{\url{http://www.w3.org/DesignIssues/LinkedData}} Tim
Berners-Lee notes ``To make the data be effectively linked, someone
who only has the URI of something must be able to find their way the
SPARQL endpoint.'', but this is generally not possible today without
requiring out-of-band information.

The triples needed to do this are already defined in
VoID\footnote{\url{http://vocab.deri.ie/void}} and are in use:
\begin{verbatim}
<> void:inDataset [ void:sparqlEndpoint </sparql> . ] .
\end{verbatim}

\section{Conclusion}

We have shown how RDF serializations can become very powerful
hypermedia types by using a simple vocabulary. As a consequence,
developers can use Linked Data in a read-write scenario without
specialized knowledge about RDF APIs or other out-of-band
information. We have argued briefly that some triples should be added
to every information resource, but note that most triples are only
relevant to write-operations and should only appear in that case. We
also noted that SPARQL is important for non-trivial traversal of
Linked Data. To sum up, the addition of the following triples would
make life easier for developers of applications that use Semantic Web
data:

\begin{verbatim}
<> hm:canBe hm:mergedInto, hm:replaced, hm:deleted ;
   hm:createSimilarAt <../> ;
   hm:acceptsFormat <http://www.w3.org/ns/formats/Turtle> ;
   void:inDataset [ void:sparqlEndpoint </sparql> . ] .
\end{verbatim}

\bibliographystyle{plain-csmin}
%\bibliographystyle{jbact}
%\bibliographystyle{splncs03}
\bibliography{federation,webarch,webframeworks}

\end{document}
