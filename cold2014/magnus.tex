\documentclass{llncs}
%\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{wrapfig}

\title{Performance of two different ontological and data access methods}
\author{Magn\'{u}s D\ae hlen \and Kjetil Kjernsmo}
\institute{Department of Informatics,
Postboks 1080 Blindern,
N-0316 Oslo, Norway \email{\{magnudae,kjekje\}@ifi.uio.no} }


\subtitle{---Draft submitted to COLD 2014---}


\begin{document}
\maketitle

\begin{abstract}
  We evaluate two different approaches that have been used in the
  literature and in practice by different groups publishing metadata
  about cars when consumed by a prototype application. These
  approaches are characterized by a generic vs. domain specific
  ontology; by a constrained tree-oriented API vs. openly queryable
  data; encouraging vs discouraging reasoning, etc.  We have
  implemented a prototype application where a potential user may query
  selected properties of cars, and we investigate how the different
  choices made by the original designers influence the performance of
  the application. For the evaluation, we employ the statistical
  disipline of Design of Experiments.

\end{abstract}

\section{Introduction}

The retrieval of information from different sources and then combine
them to allow a user to find a certain combination of properties that
suit their purpose is an archetypical Semantic Web use case. We have
chosen to focus on a product selection use case, specifically on cars,
since some car makers have embraced the Semantic Web vision and chosen
to share detailed data on their products.

The findings, we assert, have a broader validity, given the generic
nature of the problem they are trying to solve, and so our
recommendations should have broad relevance to similar use cases.

We present two different approaches, one promoted by Renault, see
\cite{SemWebAppRes} and \cite{ren1}, and another made by Martin~Hepp
in collaboration with Volkswagen resulting in amongst other things,
the Car Options Ontology~\cite{COO}. We shall compare them to find
their strengths and weaknesses. Each approach contains an ontology and
has its own way of representing data.  To make this comparison we have
used data about the same domain, data about car models and their
component constraints.  The first approach is a generic ontology. By
generic it means that it can be used to represent any product model
with component constraints. The second approach is a domain specific
ontology which as the name implies, is only applicable with one
particular domain. In this thesis that domain is about car models.

We will also show how to create a viable web application which
utilizes such complex data, mainly for the purpose of conducting
performance tests to determine the weaknesses of each approach. This
will be done with complex data found on the web today from different
car manufacturers.  With performance we mean the response time between
a HTTP~\cite{http}} post operation against the application and when
the application presents the user with an answer. The application will
contain the possibility to do HTTP posts against both approaches.

There will be several options on how to query the data because of all
the different specifications.  That is why we have chosen to use the
testing approach \emph{Design of Experiments} (DoE). This approach
will be further explained alongside the results in
Section~\ref{Results}. The evaluation will be based on four
experiments testing several aspects of the approaches. They will help
us determine what kind of factors are significant to the performance.

\section{Related work}
Complex products and specifying configurations has been a research
topic for over a decade.  The possibility to personalize more and more
products is why several research articles have proposed different
approaches on how to handle these configurations. Most of the research
are around finding the ultimate solution with a specification
system. In 1999, M. Aldanondo et. al proposed how to structure a
system to handle configurations and their constraints. This included
proposed definitions for products, configurations and
configurators. The paper focused on making a generic solution to fit
several manufacturers.~\cite{OldConf} In a newer article,
H. Afsarmanesh and M. Shafahi (2013) proposed a complex product
specification system.~\cite{NewConf} This include object modelling and
a user interface. They have focused on supporting stakeholders in the
specification process.

Unfortunately these articles do not present any research done with
semantic technologies. The use of semantic technologies on complex
products is a young field of research. The only thing done here is
what Renault and Volkswagen have presented. Both of these car
manufacturers have presented the public with two different
solutions. They have also shown how to present the data on the web and
the complexity around their solutions.

The Volkswagen solution builds upon GoodRelations, which is a web
vocabulary for e-commerce and was launched in 2008 and is now
widespread. GoodRelations.~\cite{GR} GoodRelations can be used for
detailed information about products to be sold online.

% TODO: Har litt på følelsen at dette blir for tynt for et paper,
% mulig det burde vært mer generelt om Goodrelations, etc. Vi kan jo
% bare se det an.

\section{Evaluation platform}

We set out to evaluate different approaches based on a real use case
where a user is seeking to buy a new car based on detailed data
published by car manufacturers. Today only Renault, of all the car
manufacturers, has opened their data using semantic
technologies. Volkswagen had their data published until the start of
2013, but unfortunately they canceled their Semantic Web project and
the data were removed.  Fortunately, their ontologies are still open
for the community to use. Eventually, we found Daimler very
forthcoming, as they offered data about their A- and B-class cars.
The Daimler data used a custom XML-based format, so we lifted the data
using a Python script into the ontologies developed by Martin Hepp and
Volkswagen.

To be able to put large stress on the application, which again would
allow us to bring out key strengths and weaknesses of the two
different approaches, we chose to create an application where a user
choose several or all car options specifications.

The application was implemented using Apache Maven, Spring and Apache
Tomcat as well as Jena.

The application present users with a web interface where they can
input values for any specification they might want to search for.  In
Figure \ref{initialView} we can see how the application initially
looks for a user. Here the user can input values for the different
specifications, for instance fuel type or CO$_2$ emission. After a
user is done choosing specifications he can execute the actual car
model search. The application extracts the data from the form and
comprises it into another format that can easily be used later on. The
next step begins in the search module. Here it starts a thread for
each possible car model to execute each individual search. In the end
the user is presented with a set of valid models to choose from, which
contain the chosen specifications. In Figure \ref{afterSearch}, we
can see what the user is presented with after a successful query
execution, in this case seventeen different models. In this run, the
user have searched for a model with diesel fuel, automatic
transmission, four or more gears and a maximum of six seats. We also
have the values and URIs for each of these valid models, which means
that the application could present the user with more information than
it does today.

\begin{figure}
  \centering
      \includegraphics[width=12cm]{initialView.png}
  \caption{Initial view for a user}\label{initialView}
\end{figure}

\begin{figure}
  \centering
      \includegraphics[width=12cm]{afterSearch.png}
  \caption{The view after a search}\label{afterSearch}
\end{figure}
There are done some pre-computations to make the application a little
more effective. This includes extracting all the model names from both
ontologies, initiate all possible partially defined products as a
PartialCar object and creating some internal structure. These
pre-computations were done so that it would be possible to present the
user with the name of each car model, not just the URI. We will also
describe more in depth how the car model search is done later on.

% TODO differences between Daimler and Renault
% TODO Alignment

\section{Experiments}\label{Results}

To perform a statistically sound experiment, we turn to techniques
from statistics known as Design of Experiments (DoE). For an
introduction to the use of DoE in Semantic Web research, see~\cite{Kjern}.

\subsection{Planning the experiment}
Like other approaches, there are several steps in doing experiments with
DoE.  First one has to state an objective which is important to give
the experiment a purpose.  The next step is to choose a response. The
response is the experimental outcome or observation. There may be
multiple responses in an experiment. In this experiment there will be
one response and that is the response time on a HTTP post against the
application.  These first two steps are the same for all the
experiments in this thesis.

The next two steps of planning an experiment is specific to each
experiment itself. This means that these steps will be described more
in depth in each experiment because it may vary depending on the
actual experiment.  The third step in planning an experiment is
choosing factors and levels. This is a key part of DoE. A factor is a
variable that is studied in the experiment, and to be able to study
this factor it is needed to use two or more values of this
factor. These values are referred to as levels.  The levels will allow
us to examine the influence of each factor. It is important to
identify the key factors in the planning stage. This is to get the
maximum effect out of the experiment.  Factors may be
\emph{quantitative} or \emph{qualitative}. Quantitative factors
are often numerical values that represent an interval. For instance
the weight of a car is considered a Quantitative value. Qualitative
factors are predefined values within a known set of values.  In our
case a Qualitative factor can be the type of fuel, for instance
Diesel.

The fourth step in planning is to choose the experimental plan. Here
we will use one specific approach. The approach is called \emph{full
  factorial experiment} which we will go into more detail about in the
next sub-section. There are other approaches like fractional factorial
experiment, but they were not applicable or relevant to this thesis.
The last three steps of planning the experiment are performing the
experiment, analyzing the output and in the end drawing
conclusions. More about these steps will be taken individually at each
experiment.~\cite{PlanExp}


\subsection{Full factorial experiment}
A full factorial experiment is when one got $k$ factors and $n$ amount
of levels. This means that there is $n^k$ \emph{factorial designs}
which result in $n^k$ designs to execute.  In the thesis every
experiment will only have two levels, but the factors may vary. This
means that we will use a $2^k$ full factorial design for every
experiment. The experiments consist of $2^k$ combinations with $k$
factors. For instance if we have three factors where every factor got
two levels we get $2^3$ designs, also called \emph{runs}. This results
in a \emph{planning matrix} that shows the actual runs in the
experiment as tabulated in Table~\ref{designsspec}. This contains the
chosen factors and their levels.


\begin{table}
\begin{center}
    \begin{tabular}{ | l | l  l  l |}
    \hline
    {\bf Run} &  {\bf Fuel type} & {\bf Transmission} & {\bf Speed} \\ \hline
	 1 & Diesel & Automatic & 100 \\ \hline
	 2 & Petrol & Automatic & 100 \\ \hline
	 3 & Diesel & Manual & 100 \\ \hline
	 4 & Petrol & Manual & 100 \\ \hline
	 5 & Diesel & Automatic & 200 \\ \hline
	 6 & Petrol & Automatic & 200 \\ \hline
	 7 & Diesel & Manual & 200 \\ \hline
	 8 & Petrol & Manual &  200 \\ \hline
    \end{tabular}
\end{center}
\caption{A filled planning matrix with 3 factors}\label{designsspec}
\end{table}


\subsection{Running the experiments}
In this section we will present several experiments with different
amount of factors.  The objective is to find out how the ontologies
will perform in comparison and identifying which factors affect the
outcome of each experiment. We did the ontology alignment and
development of the application to prevent the loss of precision with
either ontology.

In the experiments the definition for VSO/COO is \emph{-vso} and for
CO is \emph{-co}.  These two definitions stand for which ontology
are tested in a particular run. In practice this means that when we
send in the keyword \emph{-vso} we only run the specification search
against the car models represented by VSO/COO, in our case the data
from Daimler. This is referred to as the ontology factor in each
experiment and is a factor in all the experiments which are presented
in this thesis. The ontology factor represents the difference in how
the data are represented and how the ontologies are structures, and
with this factor we want to determine how this affects the overall
performance.  We have programmed the application so that the querying
of the ontologies are done as similar as possible on both levels. This
was done to eliminate any interference from the application. There are
some differences in the programming due that the ontologies were
different and they will be discussed later on.  In Chapter
\ref{prototype}, the application and its differences are explained in
depth.

Before starting on the experiment we had to find a way to do the
actual experiment. We wanted to first create the planning matrix and
then execute each run as a HTTP post. To do this we needed to make a
small script to calculate the $2^k$ runs and perform the HTTP
posts. We chose to use the programming language \emph{python} to do
this. That was because python was a familiar language and it also had
a package for DoE\footnote{http://pythonhosted.org/pyDOE/}. With the
DoE package we just declared the amount of factors and the matrix was
created. At that point the matrix looked more like Table
\ref{designs}, which meant that we had to substitute the + and - with
the levels of each factor.  Further in the script we used the package
\emph{request}\footnote{http://requests.readthedocs.org/en/latest/}
to do the HTTP posts in python. Below one can see the line of code
executing the request.

\scriptsize\begin{lstlisting}
 r = requests.post('http://localhost:8080/linkedopendata-magnudae/index', 
		  data=payload)
\end{lstlisting}
\normalsize

The \emph{payload} is a dictionary of all the values to post to the
application.  After the request we saved the response which contained
the response time of a run. After each request we saved the run plus
the response time and wrote it to file on CSV format. To evaluate the
result we used the programming language R with the packages
\emph{DoE.base}~\cite{DoEBase}, \emph{FrF2}~\cite{FrF2} and
\emph{BsMD}~\cite{BsMD}. R allowed us to parse the CSV files created
from the experiments and output them in two types of graphs.  A
similarity with all the experiments were that they all contained at
least the factors ontology, transmission and fuel type.

To evaluate the graphs we have used a method in the \emph{FrF2} R
package called \emph{DanielPlot}. This method assumes that the
estimated effects are normally distributed with means equal to the
effects. The means of all estimated effects are zero. Resulting in a
plot where the estimated effect would end up on a straight line. This
plot is then testing whether all the estimated effects have the same
distribution. All deviations of the straight line indicates a
significant factor. In our results we have used the absolute value of
the effects, also called a half-normal plot. The advantages of using
the half-normal plot is that all large estimated effects will end up
in the top right corner. This means that the highest performance
significance will end up in
the top right corner.~\cite{Plotting} \\
Here is a quick explanation of the axes shown in each graph.  The X
axis will show the absolute effect of each factor, shown in time. The
values on the X axis is represented in seconds which means that the a
plot will have a significance measured in seconds.  The Y axis shows
the half-normal scores. There are two way of representing the graph
and that is with either half-normal plots or normal plots. With
half-normal the effects on the X axis are shown with the absolute
effect. This means that all factors are placed with a positive
half-normal score. This is to avoid visual misleading scores because
with the normal scores, deviations from the line on negative scores
might confuse the reader who are evaluating the charts.  The scores
them selves are a measure for which factor that is the most
significant.  We have also set the \emph{alpha} to $0.05$ which
means that we will tolerate a false percentage of $5\%$.

All of the planning matrices, graphs and results will be presented
with each experiment later on in this section.  The discussion and
interpretation of the results will be presented in Chapter
\ref{Discussion}.  Before starting the experiments we will present
abbreviations for each factor because their full name made the graphs
totally illegible. These abbreviations will be used in the graphs and
tables further on in this section.  The results from all the
experiments can be found in appendix \ref{appD}.
\begin{itemize}
  \item Ontology = O
  \item Fuel Type = FT
  \item Transmission = T
  \item Weight = W
  \item Total Weight = WT
  \item Emission = E
  \item Nr. of Gears = G
  \item Seating Capacity = SC
  \item Fuel Consumption = FC
  \item Doors = D
\end{itemize}

We ran several experiments, both randomized and unrandomized
experiments, with 3, 4, 6 and 10 factors. Here, we report on the
unrandomized 3 and 10 full factorial experiments:
  
\subsubsection{Experiment with three factors}
The first experiment that was done, was the experiment with only three
factors. The goal here was to see how the ontologies behave with a
small amount of querying and factors.  Table \ref{facandlevExp1} show
the different factors and their levels. We chose to start with a small
experiment using only simple factors which had few possible
values. That is why we chose transmission and fuel type as factors as
well as the ontology factor. With these three factors we got $2^3$
number of runs.  This meant that we had 8 runs to execute against our
application. This experiment was also replicated four times to get the
most accurate results. The experiment was small so the replication did
not cause any issues.  The results were calculated by finding the
average response time for each run between all four replications.

\begin{table}[H]
\begin{center}
    \begin{tabular}{ | l | l l |}
    \hline
    {\bf Factor} & {\bf Level 1} & {\bf Level 2} \\ \hline
	Ontology & -vso & -co \\ \hline 
	Fuel type & Diesel & Unleaded Petrol \\ \hline 
	Transmission & Manual Gearbox & Automatic Gearbox \\ \hline 
    \end{tabular}
\end{center}
\caption{Factors and levels for experiment 1}\label{facandlevExp1}
\end{table}

The next step was to create the planning matrix for this
experiment. As mention in the beginning of this section we used a
package in python to create and fill out the planning matrix. Table
\ref{3factor} shows all the 8 runs for the experiment with three
factors. We also ran a randomized version the experiment to check
whether the randomization would influence the experiment. The results
were almost equal which eliminated randomization as a factor.  Both
results can be found in appendix \ref{appD}. After each run the
response time were logged to that particular run. The response time
were then used to check for significant effect when the experiment
were evaluated
\begin{table}
\begin{center}
    \begin{tabular}{ | l | l l l |}
    \hline
    {\bf Run} & {\bf Ontology} & {\bf Fuel type} & {\bf Transmission} \\ \hline
	1 & -vso & Diesel & Automatic Gearbox \\ \hline 
	2 & -co & Diesel & Automatic Gearbox \\ \hline 
	3 & -vso & Unleaded Petrol & Automatic Gearbox \\ \hline 
	4 & -co & Unleaded Petrol & Automatic Gearbox \\ \hline 
	5 & -vso & Diesel & Manual Gearbox \\ \hline 
	6 & -co & Diesel & Manual Gearbox \\ \hline 
	7 & -vso & Unleaded Petrol & Manual Gearbox \\ \hline 
	8 & -co & Unleaded Petrol & Manual Gearbox \\ \hline 
    \end{tabular}
\end{center}
\caption{Planning matrix for experiment with three factors}\label{3factor}
\end{table}

Like the rest of the experiments we used DanielPlot to evaluate and a
package in R to visualize the result. The Figure \ref{3factorGraph}
shows the results.  Here we see no significant factors. Still there
are small effects from several factors. In Table \ref{3factorEffect}
we can see the time effects from largest to smallest.  We see that the
ontology factor has the largest effect on approximately 2.2 seconds
which means that one of the ontologies overall is 2.2 seconds
faster. By reading the results from the linear model it is the factor
level \emph{-vso} which has an overall faster response time.


\begin{figure}
 \includegraphics[width=12cm]{2factorfinal.pdf}
  \caption{Resulting graph for the three factor experiment}\label{3factorGraph}
\end{figure}

\begin{table}
\begin{center}
    \begin{tabular}{ | l l |}
    \hline
    {\bf Factors} & {\bf Absolute Effect}  \\ \hline
	  O     & 2.2463476\\ \hline
	  O:T    &0.4536135 \\ \hline
	  O:FT:T &0.4534723 \\ \hline
	  T      &0.4513712 \\ \hline
	  FT:T   &0.4499820\\ \hline
	  O:FT   &0.1240831 \\ \hline
	  FT     &0.1189629 \\ \hline
    \end{tabular}
\end{center}
\caption{Table of effects for the three factor experiment}\label{3factorEffect}
\end{table}

\subsubsection{Experiment with ten factors}
The last experiment we did was a ten factor experiment. The goal here
was to see which factors would be deemed significant in a complete car
model search. With the results from this experiment we could discuss
several aspects of using the different ontologies.  Here we added 4
new factors in comparison to the six factor experiment. The newly
added factors were \emph{weight}, \emph{nr. of doors},
\emph{nr. of seats} and \emph{fuel consumption}. We used the same
approach here as in the previous experiments to calculate the average
value for the new factors. The average values can be seen in the
planning matrix in appendix \ref{appB}. With 10 factors the amount of
runs were $2^{10}$. This results in 1024 number of runs which made the
planning matrix too big to show here.

In the graph \ref{10factorGraph} we can see the results from running
all the 1024 runs.  We can see that there are a lot of significant
effects present here. There are more than any of the previous
experiments, but it is also more factors and factor interactions.
Unfortunately there are so many significant factors that there are
some of them that are illegible, but most of them have a low impact on
the significance graph. Like the six factor experiment we also see
here that there are three groups of significant factors. The first
group where most of them are placed range from around 2 seconds effect
to 11. Here there are a lot of factor interactions and why they are
there will be explained in the discussion.  The second group is very
like the second group we saw in the six factor experiment, the only
difference is that the total weight (WT) factor is substituted with
fuel consumption (FC). The last group is the same for all the
experiments, except the two factor experiment. Here we find the
ontology factor.  In Table \ref{10factorEffect} we can see the top
twenty absolute effects. This includes all the significant factors
from the last group and the second one, but also 5 from the first
group.  We can see that emission also here are present in a lot of the
entries, but so is ontology as well. The new thing here is the fuel
consumption has entered the most significant factors, and is by itself
higher than emission. The linear model again tells us that the option
\emph{-vso} has a significantly lower response time than
\emph{-co}

When we accumulated the results from all the runs we ended up with the
total amount of response time to be approximately 30095 seconds, which
is 8 hours, 21 minutes and 35 seconds.  This were the total response
time after running all the runs. This did not take into account all
the other steps during the experiments which took time, like writing
results to file and initiating the application.  This discouraged the
replication of the experiment due to the size and was also a reason
why we did not do experiments with more factors.




\begin{table}
\begin{center}
    \begin{tabular}{ | l l |}
    \hline
    {\bf Factors} & {\bf Absolute Effect}  \\ \hline
      O & 58.1785542324229 \\ \hline
      E:FC & 45.6971032050781 \\ \hline
      O:E:FC & 45.5502538925781\\ \hline
      FC & 38.6140886777344\\ \hline
      O:FC & 38.5571969433593\\ \hline
      O:E & 35.3967871113281\\ \hline
      E & 35.3827983613282\\ \hline
      O:T & 33.1249425527344\\ \hline
      T & 33.1154579746093\\ \hline
      O:T:E:FC & 29.3077075019531\\ \hline
      T:E:FC & 29.2986405957031\\ \hline
      O:T:FC & 28.2420862089844\\ \hline
      T:FC & 28.2325333808593\\ \hline
      O:T:E & 25.3121093378906\\ \hline
      T:E & 25.3039236347656\\ \hline
      WT:E & 11.9191989902344\\ \hline
      O:WT:E & 11.8908780371094\\ \hline
      O:WT:FC & 11.7234715332032\\ \hline
      WT:FC & 11.7140515488281\\ \hline
      O:D:E & 10.1241599902344\\ \hline
    \end{tabular}
\end{center}
\caption{The top twenty significant effects of the ten factor graph}\label{10factorEffect}
\end{table}


\begin{figure}
 \includegraphics[width=14cm]{10factorAvarageDanielPlot.pdf}
  \caption{Resulting graph for the ten factor experiment}\label{10factorGraph}
\end{figure}

\section{Conclusions}

We recommend that both alternatives should be used together for
representing complex products. 
Using only CO there should be done some alterations to reduce
bottleneck issues. We would not recommend using the data represented
by Renault today, because it is not robust enough and has a too long
response time for bigger operations.  It is also important that the
data representation is done properly to ensure readability for both
humans and machines.  Today, the Renault representation is neither
very understandable for machine nor for human. This is reflected in
the discussion about the application development.  The results from
the experiments done against the application shows that the current
representation is not feasible with today's performance standards and
user expectations. This is the area where most of the workload is on
the use of the ontologies.  This means that for a third party user VSO
in conjunction with COO are the best solution to work with product
models. However, this approach is limited to only represent data about
car models. It would also require some reasoning to be effective, but
with the technology and knowledge today that should not be a to
difficult issue. For other users wanting to represent constraints
between product models, CO can be a viable options if the proper
alterations is done. It can be very useful when working with several
data domains, but then the compatibility issue has to be solved.  One
solution could be to use the compatibility approach that COO
offers. This would result in adding several new triples in each
lexicon to avoid the configuration traversals.  It would also add the
need for reasoning to be effective, which is opposite of what Renault
proposed. If the compatibility issue is solved we believe the
performance can be feasible for almost every scenario. It will demand
more from the third party user, but can utilize more than just data
about car models. To make it even more effective, there should be done
several other alterations to both the representation and the ontology
as discussed in Section \ref{Alterations}.

Our recommendation is based on giving the user freedom to query the
data in several ways.  A future goal would be to do a more thorough
test of the ontologies for all possible scenarios, since the
versatility of the ontologies are huge. We believe that ultimately the
data from manufacturers and others can be profitable for several
online vendors and third party sites. Other manufactures of complex
product could profit from using semantic
technologies to represent their data.

Based on these investigations we conclude:
\begin{itemize}
 \item Both ontology approaches can be useful, but the generic needs several alterations to be more robust.
 \item The representation of the data is just as important as the ontology structure.
 \item Data should be as machine readable as possible, and properties denote more information, rather than classes if it is possible.
 \item To speed up the performance of the generic approach, the compatibility issue has to be resolved. One solution is to use the approach proposed in COO.
 \item Reasoning over the product models should be applied to increase efficiency and versatility.
\end{itemize}



% TODO: In conclusion, discuss external validity.

\end{document}


