

\section[Future Work]{Future work; Discussion of Other Problems}\label{sec:future}

As noted in Section~\ref{sec:motivation}, it is clear that evaluating
arbitrary SPARQL queries are in general unsustainable, and it is
likely that the problems in Section~\ref{sec:perfproblems} are partly
to blame for it. Our work, and most of the related work in
Section~\ref{sec:related} approach this problem from the direction of
moving stress from the server, but it is also clear that another way
to address Problem~\ref{prob:sparqlcomplex} is for the SPARQL engine
to identify queries it would be too heavy to answer already when
parsing or planning, and also to expose that in the service description
so that clients can avoid asking them. The HTTP/2 specification
\cite{rfc7540} has some generic facilities that may be helpful: 

\begin{quote}
\texttt{ENHANCE\_YOUR\_CALM (0xb)}:  The endpoint detected that its peer is
      exhibiting a behavior that might be generating excessive load.
\end{quote}

In Section~\ref{sec:understandquery}, it is noted that
Problem~\ref{prob:microcontroller} makes it important for a query
planner that integrates LDF query answering to find a balance between
client and server capability.

The lack of a formalisation of the notion of a useful cache entry is
noted in Section~\ref{sec:analpre}, and while there has been some work
on this topic, I am not aware of any attempt to do so in the context
of the present topic.


Problem~\ref{prob:queryeq} is has an interesting possible solution in
the application of hashing algorithms that have a reasonable chance of
returning the same digest if two queries are equivalent. As noted in
Section~\ref{sec:relcache}, \cite{kaseicache} can be used for this. It
is also interesting to discuss the approach \cite{papailiou2015graph}
took to canonical labelling in HTTP context:

The \httph{ETag} header, discussed in Section~\ref{sec:httpcache}, can
be used for a SPARQL result. Such a canonical label could be used in
conjunction with an algorithm that tracks changes to the RDF graph to
produce an \httph{ETag}. For slow-changing Linked Data, which is a
common case, it is a possibility that a canonical label in combination
with a time-stamp of the entire RDF Graph could yield considerable
benefits. However, the presented algorithm does not cover SPARQL in
its entirety, for example, it does not handle named graphs, solution
modifiers and filters, nor is it clear how it handles property paths
(which in many cases should be trivial, since a property path often
has an equivalent Basic Graph Pattern). Union queries or negation is
also not mentioned, so more work is needed for this method to have
general applicability. Nevertheless, if the entire Group Graph Pattern
of a certain query is supported, and the query projects all variables,
e.g. it wraps the pattern with \sparql{SELECT *} or
\sparql{CONSTRUCT}, the \httph{ETag} would be trivial to construct
using the presented algorithm in a non-trivial number of beneficial
cases. With this, cached result verification could be done by a
mediator using HTTP requests with the \httph{If-None-Match} header and
so make it possible to use parts of the engine of this paper usable in
the Internet infrastructure.


Problems~\ref{prob:unknowndist},~\ref{prob:unknownconddist}~and~\ref{prob:largestats}
has a promising approach in \cite{5767868}, but a way to expose this
information to a client or a proxy is lacking. We saw in
\cite{splendid} that a VoID description can be used with some success,
but this needs further elaboration. Histograms have also been proposed
for this purpose in \cite{Umbrich2011}.

I have not approached any of the problems of
Section~\ref{sec:semproblems}, mainly because I think it is an open
question which of these approaches will be adopted in practice, and
whatever the solution may be, their designers should make sure they
are orthogonal to the other problems discussed here. Thus, it was
natural to consider them out of scope. 

Clearly, Problem~\ref{prob:lawsfuture} is a very interesting one, with
important implications for evaluation, but also because it shifts the
focus of our attention. In this work, the methodology used to study
Problem~\ref{prob:sanity} could be used to gain such understanding. As
noted in Section~\ref{sec:history}, this study is partly motivated by
my subjective experience of why the Web was so successful. Clearly,
this should be studied more objectively, but I am not aware of any
such study. Such study could also be applied to see if the experiences
from the emergence of the Web can be applied to accelerating the
Semantic Web, like I subjectively propose.

Problem~\ref{prob:tpf} was along with
problems~\ref{prob:sparqlcomplex},~\ref{prob:endpointunpred}~and~\ref{prob:syntacticcache}
part of the motivation of \cite{ldf1} and \cite{verborgh2014querying},
and this builds in part on the insight I contributed when addressing
Problem~\ref{prob:lapis}. Their solution was to define an ontology to
describe an interface that can answer a single triple pattern, but
since it is hypermedia, it can be described in the messages of the
protocol. However, the solution in \cite{verborgh2014querying} does
not take into account Problem~\ref{prob:microcontroller}, so query
planning that shifts the burden to a cost-model driven balance between
server, proxy and client is an interesting direction. In this context,
it should be noted that HTTP/2 \cite{rfc7540} is intended to make
multiple requests less expensive, and it is interesting to study the
practical impact of this as it could impact the cost of making many
HTTP requests substantially.

I believe that the core of Problem~\ref{prob:dynaprog} lies in the
prevalence of statically typed languages. From an engineering
perspective, static typing seems like a good idea, since errors should
surface at compile time. However, it seems like it comes at a high
cost: Programmers seem incapable of thinking beyond the constraints it
sets. Nevertheless, the theoretical framework that seems to solve the
problem was proposed in 1991 in the book ``The Art of the Metaobject
Protocol'' \cite{kiczales1991art}. Perl has a mature implementation of
the metaobject protocol, and the topic has been discussed on several
hackathons, and some code has been written, but the solution seems
elusive.

Finally, the problems~\ref{prob:badphil}~and~\ref{prob:badstats}
should prompt an elaborate approach to understand and apply the
practical implications of contemporary philosophy of
science. \cite{Mayo2005-MAYEAP} proposes a framework that defines what
constitutes a \emph{severe} test, and one possible direction is to
apply this to query endpoint evaluation.

\section{Conclusions}\label{sec:conclusions}

The conclusions of the papers can be summarised as follows:

Hypermedia can contribute a ``View Source'' capability to application
development and data exploration, and thinking about an RDF statement
with control information as a natural language sentence can make RDF
understandable to many developers. Therefore, it is one key to
developing applications on the Semantic Web.


When engineering systems to encourage contributions from many
different parties, it is important to carefully consider how such
systems can be extended and adapted, and how contributions can be made
a part of the systems and so enhance the base that researchers and
other users can benefit from. Traits ease the task of extending
database systems with experimental features and eases the usage of
optimisations in underlying systems.

Caching in the Web is a prominent topic, but we found that while the
adoption of caching and conditional requests as defined in HTTP is not
widespread, the adoption is sufficient to start using them in
practical systems. We also found that many resources change at a very
slow pace, but that it is not reflected in the freshness lifetime set
by the server, but may more commonly be computed by common heuristics.


Evaluation methodologies suffer from a poor foundation in philosophy
of science, and research into the epistemology of the field is
important. This is due to several interrelated factors: The field of
research differs from natural science in that it studies a human
artefact that does not yet exist. An example of this problem is that
there is no formulated coherent theory of the Semantic Web, so the
external validity of any hypothesis relative to such a theory can
always be contested. If one would wish to follow the criterion that
science is distinguished by falsifiability, it is currently difficult
to tell if the evaluation or the hypothesis is to be rejected.

Moreover, along the above directions, and based on the fact that the
author has a large intellectual investment in the success of the
Semantic Web, the author admits that the study is lacking in terms of
objectivity, but notes that the problems are of a general nature.

Empirical evaluations should be developed from the principles of
philosophy of science and statistics, and the practice of benchmarking
does not adequately follow these principles. Statistical Design of
Experiments provides a path to a critical practice of evaluations,
needed to improve the foundations of the field. The experiment we
designed provided for both an instructive example of a new methodology
and how to use the methodology to evaluate the experiment itself.

Finally, the work extended to a SPARQL query engine that could run on
a proxy. It could cache intermediate results of triple pattern
matches, either in real time or by prefetching results that might be
valuable. In query planning and evaluation, it could use the cache, a
remote SPARQL endpoint and a Triple Pattern Fragments
server. Unfortunately, this work had to be ended before it could reach
a conclusion, but a detailed account of the work is included in
Chapther~\ref{sec:tpfcacheplanning}.
