\chapter{Introduction}

\section{Semantic Web Technology}\label{sec:semwebtech}

The World Wide Web, or just the Web for short, is a well-known global
information space invented by Tim Berners-Lee in 1989, that further emerged
in the 1990s. It is characterised first and foremost by its
universality, anyone can set up a computer, connect it to the Internet and
start serving data or documents from it, and further adapt it to their purpose. 

The Semantic Web is an extension of the Web that has been under
development since 1997\footnote{The first working group draft of the
  RDF specification is dated 1997-08-01, see
  \url{http://www.w3.org/TR/WD-rdf-syntax-971002/}.}, to extend the
Web with languages for expressing information in a machine processable
form\cite{semwebroadmap}.

\subsection{Basic technology}

The Semantic Web is built on a number of specifications that has been
developed under the auspices of the World Wide Web Consortium. The
core technology is known as the Resource Description Framework
(RDF). The framework is defined in a suite of specifications, defining
concepts and abstract syntax, semantics, and several concrete
syntaxes.

Conceptually, an RDF statement is a triple, where the terms are a
subject, a predicate and an object. The building blocks for these
triples are literals, blank nodes and Internationalized Resource
Identifiers (IRI). IRIs builds on the familiar URL: Uniform Resource
Locator, informally known as a Web address,
e.g. \url{https://www.w3.org/RDF/} is an address for a document about
RDF. It is also the document that lists the specifications that the
following outline relies on. 
On the Semantic Web, the URL is first generalised to a Uniform
Resource Identifier, URI, that can not only name a document on the
Web, but any resource. A resource can be a physical thing, like a
person, or an abstract thing, like the type of weather at some place
at some time, numbers, strings, etc. Then, URIs are extended to IRIs
to enable the use of any character in them. Literals are strings with
a datatype (denoted by a IRI), and optionally a language tag. Finally,
blank nodes are unnamed nodes. RDF defines which of these building
blocks may be used for what term: The subject may be a IRI or a blank
node, the predicate only a IRI, whereas the object may be a literal,
blank node or IRI.

The use of IRIs makes it possible to view RDF statements as a directed
graph, where the subject IRI is a vertex, the predicate is an edge to
an object vertex\footnote{A working draft of the RDF Semantics
  specification dated 23 January 2003, see
  \url{https://www.w3.org/TR/2003/WD-rdf-mt-20030123/}, described the
  graph structure as a ``partially labeled directed pseudograph with
  unique node labels''.}.

For historical reasons, the first syntax that was standardised for RDF
was an syntax based on the tree-structured Extensible Markup Language
(XML). The simplest standardised syntax is known as N-Triples, where
each statement is written on a single line, where each IRI is written
out in full. The basic building blocks of N-Triples has been reused to
create a compact and natural text form syntax, known as Turtle. Turtle
has gained traction, and is well established as a common syntax. A
simple graph expressed in Turtle could be:

\begin{example}{Turtle syntax}\label{ex:turtle}
\small
\begin{verbatim}
@prefix foaf: <http://xmlns.com/foaf/0.1/>.
@prefix dct:  <http://purl.org/dc/terms/>.

<http://www.kjetil.kjernsmo.net/foaf#me> a foaf:Person ;
  foaf:name "Kjetil Kjernsmo" ;
  foaf:made <http://folk.uio.no/kjekje/2013/iswc.pdf> .
<http://folk.uio.no/kjekje/2013/iswc.pdf> a foaf:Document ;
  dct:title "Introducing Statistical Design of [...]" ; 
  dct:creator <http://www.kjetil.kjernsmo.net/foaf#me>, 
              <http://[...]/person/john-s-tyssedal> .
\end{verbatim}
\normalsize
\end{example}

The example opens with prefix declarations, to allow abbreviating
IRIs, e.g. the predicate \rdfterm{foaf:name} is an abbreviation of the
IRI \rdfterm{\nolinkurl{http://xmlns.com/foaf/0.1/name}}. The IRI
\rdfterm{\nolinkurl{http://www.kjetil.kjernsmo.net/foaf\#me}} is a name for this
author. The \rdfterm{a} predicate is standards-defined abbreviation
for \rdfterm{\nolinkurl{http://www.w3.org/1999/02/22-rdf-syntax-ns\#type}}, which
is used for declaring the subject an instance of a class, in this case
\rdfterm{foaf:Person}. Then, the \rdfterm{foaf:made} predicate
is used to link to an author-published version of one of the
publications of this dissertation, and further information is then
provided about this publication, including a link to the co-author. As
we see, semicolons are used to separate statements that share a
subject, commas separate statements with the same subject and
predicate, while periods terminate a statement. \texttt{[...]} is just
to shorten the lines in the example and is not a part of the syntax.

Note that all the IRIs uses the HTTP scheme. The Hypertext Transfer
Protocol (HTTP) is the main application protocol used on the Internet
for the Web, and is also the protocol used most for the Semantic
Web. It is a request-response protocol in a client-server computing
model, but also accommodates for intermediate proxies. HTTP messages
consist of a header, typically with metadata, and a body, which is the
content itself. The standard defines several headers that can be used
to control caching on either the client, the server or an intermediate
proxy.

Also, note that all IRIs in this example are \emph{dereferenceable},
meaning that if an HTTP client is used, a representation of the
resource can be retrieved across the Internet. This, along with that
it is expressed as RDF, makes the above so-called Linked Data.

HTTP IRIs are not the only possible choice, to the contrary, the
scheme and what may be associated with it, such as a protocol, is an
orthogonal issue to the data model and syntax, and any IRI scheme can
be used.

\subsection{Interacting With Data}\label{sec:introinteract}

To read and write this graph, several techniques have been
developed. For reading, one is to simply parse the string into a
suited data structure and use any tool available in the chosen
programming language to examine the data structure, or simply write
data to e.g. a Turtle string.

A more sophisticated approach is to match parts of the graphs using
\emph{triple patterns}. A triple pattern may contain variables for
certain terms, for example:

\begin{example}{A triple pattern}
\begin{verbatim}
?subject a ?class .
\end{verbatim}
\end{example}
This triple pattern will match the two triples 
\begin{example}{Triples matched by triple pattern}
\small
\begin{verbatim}
<http://www.kjetil.kjernsmo.net/foaf#me> a foaf:Person .
<http://folk.uio.no/kjekje/2013/iswc.pdf> a foaf:Document .
\end{verbatim}
\normalsize
\end{example}
from the above example (prefixes omitted for brevity). Frameworks in
popular programming languages commonly have a method that implements
this.

The triple pattern is also a fundamental building block of the SPARQL
query language, which is the language that is the main topic of this
dissertation. SPARQL is an extensive, standardised query language with
both read and write operations. The syntax is near Turtle, but as the
above triple patterns, introduces variables. A set of triple patterns
may be used to create a conjunctive query, and along with a filter
that can be used to further constrain the query with boolean
expressions. Together, they are known as a Basic Graph Pattern
(BGP). More advanced parts of the language are built around BGPs.

SPARQL also introduced a mechanism for naming a graph or parts of a
graph with a IRI, an approach that was subsequently adopted by
RDF. Therefore, RDF is today often implemented not just expressed as
a triple, but may be a expressed with a quad.

To execute a query, a SPARQL engine would typically parse the query to
create a tree of algebra objects (in the case where an object oriented
programming language is used) in the process. Then, a query planner
component would traverse the algebra tree, and for each algebra object
or branch of algebra objects, the query planner finds various ways to
access the data in the underlying store or execute other
operations. In many programming frameworks, the only way to access
data is through matching a triple pattern. If that is the case and
the incoming query consists of just a BGP, the query planner's task
would simply be to match all the triple patterns and join their
results, and finally apply any constraining filters. 

However, even though the result does not depend on the order of
execution of a BGP, the time it takes to find the result often does. For
example, consider the following BGP:
\begin{example}{A Basic Graph Pattern}\label{ex:bgp}
\begin{verbatim}
?subject foaf:name ?name ;
         foaf:workplaceHomepage <http://example.com/> ;
         foaf:made ?contribution .
\end{verbatim}
\end{example}
In most cases, if the triple pattern with the \rdfterm{foaf:name}
predicate is evaluated first, it is likely to match many resources,
since nearly everything may have a name, and therefore, it does not
reduce the number of statements that needs to be searched when
matching the next triple pattern by much. Instead, the
\rdfterm{foaf:workplaceHomepage} may be matched first, as we may guess
the fact that the object is given will be more restrictive, but that
too fails if everyone in the database works for the same organisation
and has used this predicate. It is the query planner's task to choose
between equivalent plans to ensure that the most efficient evaluation
is chosen. As is clear from this example, it is often insufficient to
rely on such heuristics, and therefore, it is common to rely on a
statistical digest in the query planner. Still, the task of query
planning remains a very complex one, and often, insufficient
statistics is available, so that planning is done using heuristics or
based on assumptions that do not hold. These are only some of the
concerns that affect performance of the overall query engine.

Indeed, detailed knowledge of the data, such as a comprehensive
statistical digest may be available close to the database, but is not
exposed in ways that higher levels in the technology stack can make
use of. In our case, this is especially true for a caching proxy.

More techniques have been developed for writing data. The simplest is
to write RDF in one of its serialisations to a file. Basic operations
like writing single triples or quads may be done with a framework
directly onto a triplestore. Since write operations are specified in
HTTP, write operations may use HTTP verbs if they are authorised to do
so. Finally, SPARQL has been extended with an update language in
version 1.1.

\subsection{Hypermedia}

However, in terms of utility when programming, advanced query
languages such as SPARQL may not be needed for many applications, and
may represent an overly high barrier to entry.

Hypermedia is the idea that all information needed to drive
interaction in an application must be readily available in the
request-response dialogue. For human interactions, this has been
understood as a requirement for a long time, one has to tell the user
how to interact with the application, if not, the user is unlikely to
be able to solve the task. Contemporary efforts now extend this to
machine-machine interactions. 

In this context, hypermedia types is a way to classify what kind of
operations a certain media type can help the application perform.

\section{Research Problems Overview}\label{sec:problemsum}

The present work is located at the confluence of several contemporary
efforts in the Semantic Web community. The focus is on query answering
with the SPARQL query language, with emphasis on exploiting the World
Wide Web, but it touches upon query federation, hypermedia, empiricial
methods for evaluating performance, standards compliance and even
philosophy of science.

The overarching problem this work tries to address is this:

\begin{framed}
\begin{quote}
To achieve a Semantic Web where anyone can say anything about
anything, and where everyone is enabled to analyse data that have been
contributed, there is a difficult balance between centralising
infrastructure and decentralising it, and between processing on
clients and servers.
\end{quote}
\end{framed}

A comprehensive treatment of research problems is given in
Section~\ref{sec:problems}, but to understand the following
contributions, a superficial overview is provided:

It is difficult to many developers to interact with RDF, and
contributions to alleviate that are summarised in
Section~\ref{sec:schm}. 

We have found that in few cases, at least when evaluating SPARQL
Engines, the evaluation has had a basis in sound statistics or in
philosophy of science and therefore the validity of the conclusions
drawn from them should be challenged. We summarise contributions to a
new direction to the statistical problem in Section~\ref{sec:scdoe}
and a philosophical discussion in Section~\ref{sec:scphil}.

To create a framework that allows the flexible and at the same time
efficient manner is difficult. Conventionally, the query engine would
break down a query to individual triple pattern matches, which did not
allow for much optimisation, or to easily modify the query
planning. Our contributions towards these problems are summarised in
Section~\ref{sec:scdev}.

As noted, the HTTP standard and the Web architecture allows for
caching responses, and in Section~\ref{sec:scsurvey} we detail
contributions to understand the contemporary usage of these
mechanisms. This is particularly interesting in the context of SPARQL
query execution on the Web, as currently, the infrastructure cannot
support evaluating arbitrary queries across the Web in a reliable
manner. A situation where the client, a caching proxy and a server may
share the work when evaluating a query results in a number of research
problems that are detailed in Section~\ref{sec:perfproblems}, and we
attempted to contribute an approach to some of these problems in
Chapter~\ref{sec:tpfcacheplanning} without reaching any solid
conclusions.



\section{Contributions}\label{sec:contribsum}

In this section, we summarise the contributions of this work, with
reference to the detailed problem descriptions in
Section~\ref{sec:problems}, as well as
Section~\ref{sec:papersincontext}, where the papers are described and
put in context. 

\subsection{Hypermedia}\label{sec:schm}

The following contributions were made towards Problem~\ref{prob:lapis}
in the paper described in Section~\ref{sec:conlapis}:

\begin{enumerate}
\item A thorough discussion of the implications of Hypermedia
  Types\cite{hypermediatypes} for RDF.
\item A sketch of a vocabulary for read-write RDF hypermedia.
\end{enumerate}

\subsection{Design of Experiments}\label{sec:scdoe}

The following contributions were made towards Problem~\ref{prob:badstats}
in the paper described in Section~\ref{sec:condoe}:


\begin{enumerate}
\item Introduction of a path for critical practice of evaluations, that makes
  use of contemporary statistical techniques, to establish a practice
  that can be used to refute assertions on performance.
\item A didactical experiment to help researchers understand the statistics.
\item The novel application of a well-established method in
  statistics, rarely used in Computer Science, to SPARQL endpoint
  evaluation.
  
\end{enumerate}

\subsection{Development problems}\label{sec:scdev}

The following contributions were made towards
problems~\ref{prob:breakdown},~\ref{prob:complexapi}~and~\ref{prob:dontjustpass}
in the paper described in Section~\ref{sec:conpush}:

\begin{enumerate}
\item A framework to enable the use of low-level optimisations in
  databases.
\item Simplification when implementing experimental features in
  SPARQL.
\item Novel programming techniques for custom query planners.
\end{enumerate}

\subsection{Survey of HTTP Caching}\label{sec:scsurvey}

The following contribution was made towards Problem~\ref{prob:sanity}
in the papers described in Section~\ref{sec:consanity}:

\begin{enumerate}
\item An understanding of actual usage of caching headers (metadata)
  on the open Semantic Web.
\end{enumerate}

\subsection{Philosophy of Science}\label{sec:scphil}

The following contribution was made towards Problem~\ref{prob:badphil}
in the paper described in Section~\ref{sec:conphil}:

\begin{enumerate}
\item A provocation to discuss epistemological questions.
\end{enumerate}

\subsection{Software}

In addition to papers, the following contributions were made in terms
of Free Software:

\begin{enumerate}
\item 5 packages that have been accepted into the recent
  versions of the Debian GNU/Linux operating system.
\item Further 5 packages that already were accepted in Debian have been
  enhanced in the course of this work.
\item Contributions to the code base were accepted by another 5
  external projects.
\end{enumerate}
