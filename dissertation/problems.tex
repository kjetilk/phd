\section{Problems}\label{sec:problems}

We seek to formulate concrete problems that should be solved to
realise the vision formulated in Section~\ref{sec:motivation}. It is
important to note that while some of these issues may be problematic
for the whole Semantic Web community, their only intended scope is the
presented vision.

\subsection{Performance and stability problems}\label{sec:perfproblems}

\begin{problem}
The complexity of SPARQL is high.
\end{problem}

\subsection{Problems of shared understanding}\label{sec:semproblems}

In \cite{berners2000weaving}, Tim Berners-Lee take schema
heterogeneity for granted and that it would be resolved by using
inference, saying:

\begin{quote}
When, eventually, thousands of forms are linked together through the
field for ``family name'' or ``last name'' or ``surname,'' then anyone
analyzing the Web would realize that there is an important common
concept here.
\end{quote}

He further goes on to explain how linking such terms will help make
that inference, and how partial understanding is common in everyday
life and will also be sufficient in my cases on the Web. The practice
of using inference to find a common understanding is now known as
ontology matching or ontology alignment, and is a very active area of
research.

% TODO, take some open problems in ontology alignment

\begin{problem}
How can suitable inference engines be made available to all users?
\end{problem}


However, we shall note that there are two directions that do not
require inference to the same extent: 

Schema.org is sponsored by Google, Microsoft, Yahoo and Yandex using a
community process to create and maintain a single, coordinated
ontology. With their market shares, they have a reasonable chance of
getting wide-spread adoption. However, we see also the key problem:

\begin{problem}
With a centralised ontology approach, how are anybody enabled to say
anything?
\end{problem}

The next possibility is that authors converge towards using a single
vocabulary per domain, but with no central control. The Linked Open
Vocabularies is a useful resource for work in this direction, see
\cite{lov2}. This may occur for economical reasons: If this happens to
be the least expensive way to achieve useful data integration, authors
may realise that this is a reasonable action to take. While this does
not eliminate the need for inference, it may greatly reduce it.

\begin{problem}
How can the adoption of existing vocabularies be encouraged? 
% TODO: Find the ref by Frank on the economics
\end{problem}


\subsection{Problems with evaluations}\label{sec:evalproblems}

To understand whether a result from a study on e.g. performance is
valid, an empirical study must be conducted, as a SPARQL system is
generally too complex to be evaluated solely by formal
methods. Conventionally, one or more benchmarks have been used for
this purpose, and there exists a number such
benchmarks. Unfortunately, due to this complexity, they have to test a
very large number of parameters, but are unable to do that, and they
may also oversimplify the test by attempting to neutralise certain
optimisation techniques. Further, they have no structured approach to
investigate flaws in the benchmark itself, and have no meaningful
summary statistics that can be used to understand the overall
performance. While one may standardise benchmarks, doing so makes it
impossible to test assertions that are outside of that standard. 

\begin{problem}
Evaluation methodologies are not founded on sound statistics.
\end{problem}

It is clear that neither the vision presented in
Section~\ref{sec:motivation} nor the more advanced vision presented in
\cite{berners2001semantic} exists as of today. In both cases, Semantic
Web would have to be deployed at a very different scale than what it
is today. If the Semantic Web is sufficiently successful to realise
these visions, it is also likely that it will be very different from
today in terms of data profiles, workloads, etc. Since data profiles
and workloads are important to device a test to evaluate assertions
about performance, the fundamental problem arises that we currently
test against something that does not exist, and this differs markedly
from the methodology of natural science.

Frank van Harmelen has encouraged a direction that seeks to study laws
of the Semantic Web information universe\footnote{see
  e.g. \url{http://videolectures.net/iswc2011_van_harmelen_universal/}
  and
  \url{http://sssw.org/2015/invited-speakers-tutors/frank-vanharmelen/}}.
If this direction is pursued with success, then it may be possible to
establish laws that can be used to generate data and workloads that
can support realistic experiments even though a future Semantic Web
may be several orders of magnitude larger than today's.


\begin{problem}
Establish laws of the Semantic Web information universe can reasonably be
expected to have validity far beyond the present.
\end{problem}




\begin{problem}
Evaluation methodologies has little foundation in philosophy of science.
\end{problem}


\subsection{Development problems}\label{sec:devproblems}

There are several issues that make Semantic Web development difficult,
that are connected with development in current programming languages
and current Semantic Web libraries.

This is very important, because even though we have argued that the
programmer needs to be eliminated from the data integration task,
there are several functions where they need to be involved, including
end-user application development, and in the short term, certainly
also the data integration task. 

Following the discussion in Section~\ref{sec:history}, the
availability of tools and examples so that the vast majority of active
Web developers can quickly publish and make use of RDF is a key to the
success of the Semantic Web.

As pointed out in \cite{darobin1}, there are few things that are
simpler in contemporary programming than to parse a string containing
a tree-formatted data structure into a tree that can be accessed
directly. However, RDF makes the assumption that the natural form of
data is not a tree, but a graph, and in the general case, it must be
addressed as such.

\begin{problem}\label{prob:graph}
Addressing graph data in a programming language.
\end{problem}

The initial learning by example by ``View Source'' of HTML markup can
be generalised to RDF by considering the term \emph{hypermedia}. The
most important aspect of hypermedia is, as argued in
\cite{Fielding_2000_Architectural-Styles}, that hypermedia can be used
to drive the interaction of applications, and that all the information
needed to do so is in the messages that are passed between the server
and the client, no information beyond that is needed. As such, the
importance of ``View Source'' extends well beyond the original
motivation of learning by example. More concretely, hypermedia can be
used to describe simple query interfaces, define how read-write
operations are to be made, etc. %TODO more here

\begin{problem}\label{prob:lapis}
Developers who are not well versed in RDF need a readily
understandable format that details how they can interact with an RDF
server that offers a read-write interface.
\end{problem}

\begin{problem}\label{prob:tpf}
Since the SPARQL language is described in an external specification,
it cannot be hypermedia.
\end{problem}

Until recently, a query engine would need to break down a SPARQL query
to individual triple pattern matches, which would then use for example
the \jcode{listStatements()} method of Jena, \jcode{filter()} method
of Sesame or \pcode{get\_statements} of \pmodule{RDF::Trine}, so that only
individual triple patterns could be evaluated against the underlying
triplestore.  The latter also had a \pcode{get\_pattern} method that
could be implemented if the underlying store had a way to optimise
Basic Graph Patterns. Sesame, on the other hand, allow implementations
to get a query representation, which it must then evaluate.
Nevertheless, this resulted in many problems:

\begin{problem}
By breaking down the query down to individual triple patterns, the
query engine cannot take advantage of optimisations that involve
multiple triple patterns or other parts of the query.
\end{problem}

\begin{problem}
If methods such as \pcode{get\_pattern} are added for every part of the
query, it would increase the complexity of the API dramatically.
\end{problem}

\begin{problem}
By merely passing the entire query representation to an
implementation, the burden of evaluating the entire query is also
transferred, which makes it harder to use default implementations for
most of the query.
\end{problem}

The same kind of problems occur in many operations, such as parsing
and serialisation, where an underlying implementation might have
relevant information to enhance e.g. the performance of an operation,
but where the API does not allow it to expose it so that upper layers
can take advantage of it.

Another type of problems occur when working with dynamic RDF that may
mix terminological and assertional information with object-oriented
programming. LITEQ \cite{leinberger2014semantic} has a modern approach
to a static case, where the developer gets substantial support from
their Integrated Development Environment when programming, and where
static typing is seen as a virtue that helps quality assurance of the
code.

However, a more interesting case is where an application (in a broad
sense) can adapt to the data it is seeing. For example, say that there
is an implementation of a Boat and a Car. It may not have been
anticipated when the application was first developed, but the
application sees an AmphibiousVehicle. Since it already knows how to
propel a Car and Boat, it should be able to adapt to the situation
given the context of whether the vehicle is on the road or in water.

\begin{problem}
Current programming paradigm deals poorly with dynamic RDF data
containing a mix of terminological and assertional information.
\end{problem}
