\section{Problems}\label{sec:problems}

We seek to formulate concrete problems that should be solved to
realise the vision formulated in Section~\ref{sec:motivation}. It is
important to note that while some of these issues may be problematic
for the whole Semantic Web community, their only intended scope is the
presented vision.

\subsection{Performance and Stability Problems}\label{sec:perfproblems}

It was found in \cite{perez2009semantics} that

\begin{problem}\label{prob:sparqlcomplex}
SPARQL has some highly complex parts.
\end{problem}

In particular, nested \sparql{OPTIONAL}s will be difficult to
evaluate. A Cartesian join is also likely to cause trouble, but even
seemingly straightforward queries may be heavy. We also note from
experience that:

\begin{problem}\label{prob:endpointunpred}
Public endpoints may have unpredictable usage patterns.
\end{problem}

Moreover, an ambition of the Semantic Web is to deal well with
heterogeneous data, and it has been known for long \cite{1385469} that:

\begin{problem}\label{prob:zipf}
The distribution of data is often dominated by a power-law.
\end{problem}
However, 
\begin{problem}\label{prob:uniform}
The data distribution is often unknown to the query engine, and so,
following the principle of indifference, assumes uniform
distributions.
\end{problem}

Heterogeneity should also be expected in different types of servers
and clients, in particular

\begin{problem}\label{prob:microcontroller}
A client running on a microcontroller does not have significant
processing capability.
\end{problem}



Caching plays an important role in the Web architecture
\cite{Jacobs:04:AWW}, because previous responses may be reused to
satisfy a current request. However, there are many problems that need
to be solved to use it for SPARQL query evaluation:

\begin{problem}\label{prob:syntacticcache}
SPARQL endpoints, which are based on HTTP, will only enable caching
syntactically identical queries by  default.
\end{problem}

\begin{problem}\label{prob:cachecartesian}
Caching of results of individual triple patterns may cause subsequent
queries to break into blocks that must be evaluated with Cartesian joins.
\end{problem}

\begin{problem}\label{prob:queryeq}
Determining that two queries will return the exact same answer is a
complex problem.
\end{problem}

\begin{problem}\label{prob:sanity}
It is not known how much of the Semantic Web exposes information that
could be used in HTTP caching.
\end{problem}

\begin{problem}\label{prob:nostats}
A caching proxy far removed from the database it caches may have
little or no statistics for query optimisation.
\end{problem}



\subsection{Problems of Shared Understanding}\label{sec:semproblems}

In \cite{berners2000weaving}, Tim Berners-Lee et al take schema
heterogeneity for granted and postulate that it would be resolved by using
inference, saying:

\begin{quote}
When, eventually, thousands of forms are linked together through the
field for ``family name'' or ``last name'' or ``surname,'' then anyone
analyzing the Web would realize that there is an important common
concept here.
\end{quote}

He goes on to explain how linking such terms will help make
that inference, and how partial understanding is common in everyday
life and will also be sufficient in my cases on the Web. The practice
of using inference to find a common understanding is now known as
ontology matching or ontology alignment, and is a very active area of
research.\todo{take some open problems in ontology alignment}

\begin{problem}\label{prob:infgivecode}
How can suitable inference engines be made available to all users?
\end{problem}


However, we shall note that there are two directions that do not
require inference to the same extent: 

Schema.org is sponsored by Google, Microsoft, Yahoo and Yandex using a
community process to create and maintain a single, coordinated
ontology. With their market shares, they have a reasonable chance of
getting wide-spread adoption. However, we see also the key problem:

\begin{problem}\label{prob:sayanything}
With a centralised ontology approach, how can we ensure that anybody is enabled to say
anything?
\end{problem}

The next possibility is that authors converge towards using a single
vocabulary per domain, but with no central control. The Linked Open
Vocabularies is a useful resource for work in this direction, see
\cite{lov2}. This may occur for economical reasons: If this happens to
be the least expensive way to achieve useful data integration, authors
may realise that this is a reasonable action to take. While this does
not eliminate the need for inference, it may greatly reduce it.

\begin{problem}\label{prob:useexist}
How can the adoption of existing vocabularies be encouraged? 
\todo{ Find the ref by Frank on the economics}
\end{problem}


\subsection{Problems With Evaluations}\label{sec:evalproblems}

To understand whether a result from a study on for example, performance is
valid, an empirical study must be conducted, as a SPARQL system is
generally too complex to be evaluated solely by formal
methods. Conventionally, one or more benchmarks have been used for
this purpose, and there exists a number of such
benchmarks. Unfortunately, due to this complexity, they have to test a
very large number of parameters, but are unable to do that, and they
may also oversimplify the test by attempting to neutralise certain
optimisation techniques. Further, they have no structured approach to
investigate flaws in the benchmark itself, and have no meaningful
summary statistics that can be used to understand the overall
performance. While one may standardise benchmarks, doing so makes it
impossible to test assertions that are outside of that standard. 

\begin{problem}\label{prob:badstats}
Evaluation methodologies are not founded on sound statistics.
\end{problem}

It is clear that neither the vision presented in
Section~\ref{sec:motivation} nor the more advanced vision presented in
\cite{berners2001semantic} exists as of today. In both cases, Semantic
Web would have to be deployed at a very different scale than what it
is today. If the Semantic Web is sufficiently successful to realise
these visions, it is also likely that it will be very different from
today in terms of data profiles, workloads, etc. Since data profiles
and workloads are important to devise a test to evaluate assertions
about performance, the fundamental problem arises that we currently
test against something that does not exist, and this differs markedly
from the methodology of natural science.

Frank van Harmelen has encouraged a direction that seeks to study laws
of the Semantic Web information universe\footnote{See
  e.g. \url{http://videolectures.net/iswc2011_van_harmelen_universal/}
  and
  \url{http://sssw.org/2015/invited-speakers-tutors/frank-vanharmelen/}}.
If this direction is pursued with success, then it may be possible to
establish laws that can be used to generate data and workloads that
can support realistic experiments even though a future Semantic Web
may be several orders of magnitude larger than that of today.


\begin{problem}\label{prob:lawsfuture}
Establish laws of the Semantic Web information universe that can reasonably be
expected to have validity far beyond the present.
\end{problem}

Even if laws have been found that can make extrapolations across
orders of magnitude tenable, evaluations still pose significant
epistemological problems. For example, falsificationism, as argued by
Karl Popper and adopted in \cite{avinatguide} will be unsuitable, for
reasons that more recent philosophers have pointed out, see
\cite{chalmers1999whatis} for a comprehensive discussion: If it is
insisted upon that hypotheses must be falsifiable, how can a researcher
know if their hypothesis should be rejected, or if it is something
wrong with the law, or perhaps the experiment itself?


\begin{problem}\label{prob:badphil}
Evaluation methodologies have little foundation in philosophy of science.
\end{problem}


\subsection{Development Problems}\label{sec:devproblems}

There are several issues that make Semantic Web development difficult,
that are connected with development in current programming languages
and current Semantic Web libraries.

This is very important, because even though we have argued that the
programmer needs to be eliminated from the data integration task,
there are several functions where they need to be involved, including
end-user application development, and in the short term, certainly
also the data integration task. 

Following the discussion in Section~\ref{sec:history}, the
availability of tools and examples so that the vast majority of active
Web developers can quickly publish and make use of RDF is a key to the
success of the Semantic Web.

As pointed out in \cite{darobin1}, there are few things that are
simpler in contemporary programming than to parse a string containing
a tree-formatted data structure into a tree that can be accessed
directly. However, RDF makes the assumption that the natural form of
data is not a tree, but a graph, and in the general case, it must be
addressed as such.

\begin{problem}\label{prob:graph}
Addressing graph data in a programming language.
\end{problem}

The initial learning by example by ``View Source'' of HTML markup can
be generalised to RDF by considering the term \emph{hypermedia}. The
most important aspect of hypermedia is, as argued in
\cite{Fielding_2000_Architectural-Styles}, that hypermedia can be used
to drive the interaction of applications, and that all the information
needed to do so is in the messages that are passed between the server
and the client, no information beyond that is needed. As such, the
importance of ``View Source'' extends well beyond the original
motivation of learning by example. More concretely, hypermedia can be
used to describe simple query interfaces, define how read-write
operations are to be made, etc. \todo{more here}

\begin{problem}\label{prob:lapis}
Developers who are not well versed in RDF need a readily
understandable format that details how they can interact with an RDF
server that offers a read-write interface.
\end{problem}

\begin{problem}\label{prob:tpf}
Since the SPARQL language is described in an external specification,
it cannot be hypermedia.
\end{problem}

Until recently, a query engine would need to break down a SPARQL query
to individual triple pattern matches, which would then use for example
the \jcode{listStatements()} method of Jena, \jcode{filter()} method
of Sesame or \pcode{get\_statements} of \pmodule{RDF::Trine}, so that only
individual triple patterns could be evaluated against the underlying
triplestore.  The latter also had a \pcode{get\_pattern} method that
could be implemented if the underlying store had a way to optimise
Basic Graph Patterns. Sesame, on the other hand, allow implementations
to get a query representation, which it must then evaluate.
Nevertheless, this resulted in many problems:

\begin{problem}\label{prob:breakdown}
By breaking down the query down to individual triple patterns, the
query engine cannot take advantage of optimisations that involve
multiple triple patterns or other parts of the query.
\end{problem}

\begin{problem}\label{prob:complexapi}
If methods such as \pcode{get\_pattern} are added for every part of the
query, it would increase the complexity of the API dramatically.
\end{problem}

\begin{problem}\label{prob:dontjustpass}
By merely passing the entire query representation to an
implementation, the burden of evaluating the entire query is also
transferred, which makes it harder to use default implementations for
most of the query.
\end{problem}

The same kind of problems occur in many operations, such as parsing
and serialisation, where an underlying implementation might have
relevant information to enhance, for example, the performance of an operation,
but where the API does not allow it to expose it so that upper layers
can take advantage of it.

Another type of problems occur when working with dynamic RDF that may
mix terminological and assertional information with object-oriented
programming. LITEQ \cite{leinberger2014semantic} has a modern approach
to a static case, where the developer gets substantial support from
their Integrated Development Environment when programming, and where
static typing is seen as a virtue that helps quality assurance of the
code.

However, a more interesting case is where an application (in a broad
sense) can adapt to the data it is seeing. For example, say that there
is an implementation of a \rdfterm{Boat} and a \rdfterm{Car}. It may
not have been anticipated when the application was first developed,
but the application sees an \rdfterm{AmphibiousVehicle}. Since it
already knows how to propel a \rdfterm{Car} and \rdfterm{Boat}, it
should be able to adapt to the situation given the context of whether
the vehicle is on the road or in water.

\begin{problem}\label{prob:dynaprog}
Current programming paradigm deals poorly with dynamic RDF data
containing a mix of terminological and assertional information.
\end{problem}


\subsection{Problems Addressed by This Work}\label{sec:probaddress}

Given the technical experience outlined in Section~\ref{sec:history},
I first addressed Problem~\ref{prob:lapis}, by suggesting a simple
vocabulary for the server to tell the client what primitive operations
it is allowed to perform on a given resource. See
Section~\ref{sec:conlapis} for more on this paper.

My next concern were the implications of the
problems~\ref{prob:zipf}~and~\ref{prob:uniform} in a federated SPARQL
engine. While studying the evaluations done in the relevant literature
in the field, I decided to start an effort to address
Problem~\ref{prob:badstats} first by adopting methodology from the
statistical field of ``Design of Experiments'' (DoE). See
Section~\ref{sec:condoe} for details on this work.

Then, as \cite{buil2013sparql} confirmed my suspicions that there were
severe stability problems with SPARQL endpoints,
Problem~\ref{prob:syntacticcache} became my main focus, and I decided
to write a query planner that could reuse previous results or prefetch
data the system deemed relevant, but then we realised that the current
programming paradigm was quite unhelpful when customising a query
planner for such a purpose. Rather than writing a prototypical query
engine for illustrative purposes, we wanted to integrate it into a
fully compliant SPARQL query engine, and address
problems~\ref{prob:breakdown},~\ref{prob:complexapi}~and~\ref{prob:dontjustpass}. This
is explained in Section~\ref{sec:conpush}.


Before addressing Problem~\ref{prob:syntacticcache}, I found that the
understanding of the actual usage of caching was inadequate, so I
decided to solve Problem~\ref{prob:sanity} first. This work is
discussed in Section~\ref{sec:consanity}.

Problem~\ref{prob:badphil} has been a growing concern throughout this
work. A proper evaluation should be an important part of the
dissertation, and yet, the problems discussed in
Section~\ref{sec:evalproblems} are so severe that the time frame of
this work does not allow a satisfactory solution to these
problems. The paper discussed in Section~\ref{sec:conphil} is
motivated in part from these problems.

Finally, the intention was to bring convergence in the topics of this
thesis around a system to cache results that could be reused in
subsequent queries. As noted in Section~\ref{sec:motivcache}, this was
motivated by opportunity as well as problems, but it is intended to
address
problems~\ref{prob:syntacticcache}~and~\ref{prob:cachecartesian}.
Triple Pattern Fragments shifts most of the processing from servers to
clients, but Problem~\ref{prob:microcontroller} makes that in some
cases infeasible.  In addition,
problems~\ref{prob:zipf}~and~\ref{prob:uniform} are compounded by
Problem~\ref{prob:nostats}, but alleviated somewhat by the presence of
cardinality estimates given by Triple Pattern Fragments, which we
exploit.


