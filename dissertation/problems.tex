\section{Problems}\label{sec:problems}

We seek to formulate concrete problems that should be solved to
realise the vision formulated in Section~\ref{sec:motivation}. It is
important to note that while some of these issues may be problematic
for the whole Semantic Web community, their only intended scope is the
presented vision. Moreover, many of them are not addressed in this
thesis, but discussed for comprehensiveness. An overview of the
problems and their relation to the published works are included in
Table~\ref{tab:problempaper}.

\subsection{Performance and Stability Problems}\label{sec:perfproblems}

As discussed in Section~\ref{sec:prelimquery}, 

\begin{problem}\label{prob:sparqlcomplex}
Some features of SPARQL may cause excessive endpoint load.
\end{problem}

We also note from experience that:

\begin{problem}\label{prob:endpointunpred}
Public endpoints may have unpredictable usage patterns.
\end{problem}

Moreover, an ambition of the Semantic Web is to deal well with
heterogeneous data and it has been known for long \cite{1385469} that
power laws are frequently encountered when fitting a distribution to
data. As noted in the discussion of Example~\ref{ex:bgp} in
Section~\ref{sec:introinteract}, a query planner would often rely on
statistics, but:

\begin{problem}\label{prob:unknowndist}
The data distribution is often unknown to the query engine.
\end{problem}
Only simple summary statistics may be available. The discussion of
Example~\ref{ex:bgp} illustrates that even if the distribution of data
given for example a predicate is known, 
\begin{problem}\label{prob:unknownconddist}
\emph{conditional} data distributions are usually unknown to the query engine.
\end{problem}
For example, the population of geographical places may be uniformly
distributed in a given data set, but the population given that the
places are of type city may be a power law. This could have very
significant impact on the overall performance, since in the latter
case, the query planner may consider a BGP like:
\begin{example}{Skew distribution}
\begin{verbatim}
?place ex:population ?population ;
       a ex:City .
\end{verbatim}
\end{example}
as a unit.

Even if this statistics could be found, it is not clear that it should:
\begin{problem}\label{prob:largestats}
Comprehensive statistics may be costly to compute and requires much
storage space.
\end{problem}
Therefore, it is important to find a balance.



Heterogeneity should also be expected in different types of servers
and clients. In particular, microcontrollers and to a lesser extent,
mobile phones, are resource constrained devices. Thus:

\begin{problem}\label{prob:microcontroller}
Clients run on devices with a wide range of processing capability.
\end{problem}



Caching plays an important role in the Web architecture
\cite{Jacobs:04:AWW}, because previous responses may be reused to
satisfy a current request. However, there are many problems that need
to be solved to use it for SPARQL query evaluation:

\begin{problem}\label{prob:syntacticcache}
SPARQL endpoints, which are based on HTTP, will only enable caching
syntactically identical queries by  default.
\end{problem}

The performance issue discussed in Section~\ref{sec:prelimquery} has
relevance in the context of caching:

\begin{problem}\label{prob:cachecartesian}
Caching of results of individual triple patterns may cause subsequent
queries to break into blocks that must be evaluated with Cartesian joins.
\end{problem}

To understand this, consider Example~\ref{ex:cartesian}, and extend
the BGP to a query with 3 triple patterns. In addition to the two
triple patterns in Example~\ref{ex:cartesian}, add a triple pattern that
connect the two, for example
\sparql{?person~foaf:made~?paper~.}. If the result of this
triple pattern is cached, what is left to evaluate are the two triple
patterns in Example~\ref{ex:cartesian}, which is likely a very
expensive operation. Another approach to this problem is that of
\cite{papailiou2015graph}, as they insist on caching only connected 
subgraphs.

It would be useful when caching if one could compare the incoming
query to a query with an already cached result and determine that they
were equivalent, but unfortunately:
\begin{problem}\label{prob:queryeq}
Determining that two queries will return the exact same answer is a
complex problem.
\end{problem}


Metadata is important for query planning, and as explained in
Section~\ref{sec:httpcache}, the HTTP protocol provides facilities for
sharing such information. However:
\begin{problem}\label{prob:sanity}
It is not known how much of the Semantic Web exposes information that
could be used in HTTP caching.
\end{problem}


Even though statistics may be available internally in a query engine,
a caching proxy is unlikely to have direct access to that data, and therefore:
\begin{problem}\label{prob:nostats}
A caching proxy far removed from the database it caches may have
little or no statistics for query optimisation.
\end{problem}


\subsection{Problems of Shared Understanding}\label{sec:semproblems}

In \cite{berners2000weaving}, Tim Berners-Lee et al take schema
heterogeneity for granted and postulate that it would be resolved by using
inference, saying:

\begin{quote}
When, eventually, thousands of forms are linked together through the
field for ``family name'' or ``last name'' or ``surname,'' then anyone
analyzing the Web would realize that there is an important common
concept here.
\end{quote}

He goes on to explain how linking such terms will help make
that inference, and how partial understanding is common in everyday
life and will also be sufficient in my cases on the Web. The practice
of using inference to find a common understanding is now known as
ontology matching or ontology alignment, and is a very active area of
research. This problem, as well as related problems, such as entity
resolution, are considered orthogonal to the problems in this thesis,
so the below problems are issues that arise from the ideal of an open
decentralised Web. These problems are therefore not discussed further.

\begin{problem}%\label{prob:infgivecode}
How can suitable inference engines be made available to all users?
\end{problem}


However, we shall note that there are two directions that do not
require inference to the same extent: 

Schema.org is sponsored by Google, Microsoft, Yahoo and Yandex using a
community process to create and maintain a single, coordinated
ontology. With their market shares, they have a reasonable chance of
getting wide-spread adoption. However, we see also the key problem:

\begin{problem}%\label{prob:sayanything}
With a centralised ontology approach, how can we ensure that anybody is enabled to say
anything?
\end{problem}

The next possibility is that authors converge towards using a single
vocabulary per domain, but with no central control. The Linked Open
Vocabularies is a useful resource for work in this direction, see
\cite{lov2}. This may occur for economical reasons: If this happens to
be the least expensive way to achieve useful data integration, authors
may realise that this is a reasonable action to take. While this does
not eliminate the need for inference, it may greatly reduce it.

\begin{problem}%\label{prob:useexist}
How can the adoption of existing vocabularies be encouraged? 
\end{problem}


\subsection{Problems With Evaluations}\label{sec:evalproblems}

To understand whether a result from a study on for example,
performance is valid, an empirical study must be conducted, as a
SPARQL system is generally too complex to be evaluated solely by
formal methods. Conventionally, one or more benchmarks have been used
for this purpose, and there exists a number of such
benchmarks. Unfortunately, due to the complexity of the systems under
evaluation, benchmarks have to test a very large number of parameters,
which is methodologically beyond their reach. They may also
oversimplify the test by attempting to neutralise certain optimisation
techniques. Further, they have no structured approach to investigate
flaws in the benchmark itself, and have no meaningful summary
statistics that can be used to understand the overall
performance. While one may standardise benchmarks, doing so makes it
impossible to test assertions that are outside of that standard.

\begin{problem}\label{prob:badstats}
Evaluation methodologies are not founded on sound statistics.
\end{problem}

It is clear that neither the vision presented in
Section~\ref{sec:motivation} nor the more advanced vision presented in
\cite{berners2001semantic} exists as of today. In both cases, Semantic
Web would have to be deployed at a very different scale than what it
is today. If the Semantic Web is sufficiently successful to realise
these visions, it is also likely that it will be very different from
today in terms of data profiles, workloads, etc. Since data profiles
and workloads are important to devise a test to evaluate assertions
about performance, the fundamental problem arises that: 
\begin{problem}\label{prob:testingnonexist}
We currently test against something that does not exist and therefore
usage patterns cannot be predicted,
\end{problem}
and this differs markedly from the methodology of natural science.

Frank van Harmelen has encouraged a direction that seeks to study laws
of the Semantic Web information universe\footnote{See
  e.g. \url{http://videolectures.net/iswc2011_van_harmelen_universal/}}.
If this direction is pursued with success, then it may be possible to
establish laws that can be used to generate data and workloads that
can support realistic experiments even though a future Semantic Web
may be several orders of magnitude larger than that of today.


\begin{problem}\label{prob:lawsfuture}
No methodology has been established to find laws of the Semantic Web
information universe that can reasonably be expected to have validity
far beyond the present.
\end{problem}

Even if laws have been found that can make extrapolations across
orders of magnitude tenable, evaluations still pose significant
epistemological problems. For example, falsificationism, as argued by
Karl Popper and adopted in \cite{avinatguide} will be unsuitable, for
reasons that more recent philosophers have pointed out, see
\cite{chalmers1999whatis} for a comprehensive discussion: If it is
insisted upon that hypotheses must be falsifiable, how can a researcher
know if their hypothesis should be rejected, or if it is something
wrong with the law, or perhaps the experiment itself?


\begin{problem}\label{prob:badphil}
Evaluation methodologies have little foundation in philosophy of science.
\end{problem}


\subsection{Development Problems}\label{sec:devproblems}

There are several issues that make Semantic Web development difficult,
that are connected with development in current programming languages
and current Semantic Web libraries.

This is very important, because even though we have argued that the
programmer needs to be eliminated from the data integration task,
there are several functions where they need to be involved, including
end-user application development, and in the short term, certainly
also the data integration task. 

Following the discussion in Section~\ref{sec:history}, the
availability of tools and examples so that the vast majority of active
Web developers can quickly publish and make use of RDF is a key to the
success of the Semantic Web.

As pointed out in \cite{darobin1}, there are few things that are
simpler in contemporary programming than to parse a string containing
a tree-formatted data structure into a tree that can be accessed
directly. However, RDF makes the assumption that the natural form of
data is not a tree, but a graph, and in the general case, it must be
addressed as such.

\begin{problem}\label{prob:graph}
Addressing graph data in a programming language.
\end{problem}

The initial learning by example by ``View Source'' of HTML markup can
be generalised to RDF by considering the term \emph{hypermedia}. The
most important aspect of hypermedia is, as argued in
\cite{Fielding_2000_Architectural-Styles}, that hypermedia can be used
to drive the interaction of applications, and that all the information
needed to do so is in the messages that are passed between the server
and the client, no information beyond that is needed. As such, the
importance of ``View Source'' extends well beyond the original
motivation of learning by example. More concretely, hypermedia can be
used to describe simple query interfaces, define how read-write
operations are to be made, etc.

\begin{problem}\label{prob:tpf}
Since the SPARQL language is described in an external specification,
it cannot be hypermedia.
\end{problem}

It is likely that the adoption of the open Semantic Web would be
accelerated if large numbers of developers felt the technology was
more accessible to them. There are a number of broad as well as
detailed problems that must be solved to achieve this:


\begin{problem}\label{prob:lapis}
Developers who are not well versed in RDF need a readily
understandable format that details how they can interact with an RDF
server that offers a read-write interface.
\end{problem}


Until recently, a query engine would need to break down a SPARQL query
to individual triple pattern matches, see
Section~\ref{sec:introinteract} for how frameworks tend to interact
with data. Typically, the query engine would then use for example
the \jcode{listStatements()} method of Jena, \jcode{filter()} method
of Sesame or \pcode{get\_statements} of \pmodule{RDF::Trine}, so that only
individual triple patterns could be evaluated against the underlying
triplestore.  The latter also had a \pcode{get\_pattern} method that
could be implemented if the underlying store had a way to optimise
Basic Graph Patterns. Sesame, on the other hand, allow implementations
to get a query representation, which it must then evaluate.
Nevertheless, this resulted in many problems:

\begin{problem}\label{prob:breakdown}
By breaking down the query down to individual triple patterns, the
query engine cannot take advantage of optimisations that involve
multiple triple patterns or other parts of the query.
\end{problem}

For example, an underlying SQL store could not compile one query for a
BGP, and could not use a \jcode{WHERE} clause to evaluate a SPARQL
\sparql{FILTER}. To remedy this, one could have a method for
evaluating BGPs with (such as the  \pcode{get\_pattern} method of
\pmodule{RDF::Trine}) and without filters, but:

\begin{problem}\label{prob:complexapi}
If methods such as \pcode{get\_pattern} are added for every part of the
query, it would increase the complexity of the API dramatically.
\end{problem}

\begin{problem}\label{prob:dontjustpass}
By merely passing the entire query representation to an
implementation, the burden of evaluating the entire query is also
transferred, which makes it harder to use default implementations for
most of the query.
\end{problem}

The same kind of problem occurs in many operations, such as parsing
and serialisation, where an underlying implementation might have
relevant information to enhance, for example, the performance of an operation,
but where the API does not allow it to expose it so that upper layers
can take advantage of it.

Another type of problem occurs when working with dynamic RDF that may
mix terminological and assertional information with object-oriented
programming. LITEQ \cite{leinberger2014semantic} has a modern approach
to a static case, where the developer gets substantial support from
their Integrated Development Environment when programming, and where
static typing is seen as a virtue that helps quality assurance of the
code.

However, a more interesting case is where an application (in a broad
sense) can adapt to the data it is seeing. For example, say that there
is an implementation of a \rdfterm{Boat} and a \rdfterm{Car}. It may
not have been anticipated when the application was first developed,
but the application sees an \rdfterm{AmphibiousVehicle}. Since it
already knows how to propel a \rdfterm{Car} and \rdfterm{Boat}, it
should be able to adapt to the situation given the context of whether
the vehicle is on the road or in water.

\begin{problem}\label{prob:dynaprog}
Current programming paradigms deal poorly with dynamic RDF data
containing a mix of terminological and assertional information.
\end{problem}



