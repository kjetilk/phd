\section{Possible directions of work}

\emph{This section is a temporary discussion area}

We need to discuss the future strategy for this work. I think the main
discussion point is the tension between elaborately develop theory
that then can be applied to derive publishable results, vs. the
practical time constraints of Ph.D. work. Another issue is that I'd
like to be mindful of the realities of adoption in standards and
adoption of standards.

\subsection{VoID extensions}

The SPLENDID paper provides a pretty clear path forward to really
solve the \langcase: Certain statistics could be amended to a new
release of VoID for cases where it very clearly would have impact. It
would be up to the data management software to identify those case
based on the work I publish.

There are two basic classes of solutions to this problem: One is to
allow further nesting of dataset partitions. VoID currently allow
class and property partitions, where the class partitions are the most
important in SPLENDID, property partitions is the most interesting for
conditional statistics. Within property partitions, one could imagine
a nested object partition, and the \langcase{} could be solved
conclusively by specifying the number of triples containing each
language. If we did the same thing with subject partitions, it would
be a very good solution to this and similar problems. However, it
would be a rather verbose solution. 

The second option is based on what the database literature knows as
most-frequent-values histograms. This is a technique used by
PostgreSQL. The (over-)simplified version of this is to say in a VoID
description: 

\begin{verbatim}
_:foo void:propertyPartition 
  [
    void:property dct:language ;
    void:mostFrequentObjects lang:nb .
].
\end{verbatim}
This would declare \rdfterm{lang:nb} as the most frequent object for
\rdfterm{dct:language}, and this simple example would solve the
\langcase. 

While rare objects might more seldomly occur in queries, they may
contribute very significantly to the performance if they are joined
first if they do. Thus, it might be worthwhile to have a property
\rdfterm{void:leastFrequentOjects} too, as well as corresponding
properties for subjects.


However, this isn't a histogram, it merely mentioning the objects that
would most likely be useful. To get a histogram we would need a
similar \rdfterm{void:frequency} attribute and both this and
\rdfterm{void:mostFrequentObjects} would have to accept ordered
lists. 

Another option would be to do something like:
\begin{verbatim}
_:foo void:propertyPartition 
  [
    void:property dct:language ;
    void:mostFrequentObjects 
    [
      void:object lang:nb ;
      void:frequency "0.012" .
    ]
].
\end{verbatim}
but this solution would be less general but of similar complexity than
the nested solution above.

I find the prospect of writing a first paper to explore these different
extensions to VoID very appealing. I would need to put down some
substantial work on notation, but not intending an very elaborate
theory, which is something that requires \SRL. The paper could discuss
the different approaches above, their merits and drawbacks. After
publishing, I could work with the W3C Semantic Web Interest Group to
get one of the solutions into VoID.


\subsection{\SRL}

Using \SRL{} in some form, whether it is in the familiar end of
Bayesian Networks or in the relatively adventerous end of methods that
support cycles, is much more elaborate than the above strategy. I'm
quite convinced that I need to publish a paper on this topic as well,
and arguably, if I should focus at the dissertation as a whole rather
than the individual parts, it makes sense to develop this theory
first, and then derive the VoID extensions from that.

There's a whole zoo of methods in \cite{srlbook}, and at this point,
just choosing a methodology is difficult. On page~191 in this book,
there is a hypertext with typed links-example, which is very close to
the RDF model, so it is clear that there are suitable examples to work
from. Also, I have a feeling that a general methodology to learn a
Conditional Random Field from a cyclic RDF graph would be a very
significant addition to the literature, with broad applicability way
beyond the selectivity optimization use case.

However, there seems that nobody in Oslo know the CRF theory, it seems
to have been an active field of study for only about 7~years, and we don't
know anyone who works on this, so it seems to be very ambitious to
embark on. 

In the acyclic case, as mentioned in section~\ref{sotasrl}, there
seems to be a pretty clear avenue for progress, and substantial
progress could be done in exploring acyclic graph patterns that are
common and how to expose them compactly.


\subsection{Fully indexed stores}

Some time ago, I wrote a partial paper about how fully indexed stores
like Hexastore \cite{hexastore} could easily find the cardinality of
any given triple pattern. It would be interesting to publish that soon
in some form.

\subsection{Conclusion}

These are my current opinions: 

At present, I feel the best way to progress is to publish a paper on
VoID extensions as well as fully indexed stores in a conditional
regime. I should do some work to get the notation and theory well
founded, but defer \SRL{} for later.

The 3.~semester evaluation committee also critisized the lack of a
paper, and said that I should produce a paper, at least a draft before
the end of the year. If I embark on a study of \SRL{}, this would not
be possible.

The study of \SRL{} is very interesting, so I should look into that
once the first paper has been published, but possibly seek partners or
an external resource. 

Amongst the below references, the most important are
\cite{selectivityPRM}, \cite{Lin:2011:LRB:2063016.2063042} and
\cite{splendid}.
