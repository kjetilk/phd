\section{The Papers in Context}\label{sec:papersincontext}

This section will put the included papers into context, and enumerates
the problem (from Section~\ref{sec:problems}) each paper is intended
to address. It also enjoys the benefit of hindsight, and will
therefore address some of the shortcomings that have surfaced since
their publication.

\subsection{Read-write Hypermedia}\label{sec:conlapis}

This work is a paper entitled ``The necessity of hypermedia RDF and an
approach to achieve it'' that was presented in the First Linked APIs
workshop at the Ninth Extended Semantic Web Conference. I was the sole
author. The citation is:

\begin{quote}
Kjetil Kjernsmo.
\newblock The Necessity of Hypermedia RDF and an Approach to Achieve it.
\newblock In {\em Proceedings of the First Linked APIs workshop at the Ninth
  Extended Semantic Web Conference}, May 2012.
\end{quote}

This paper is relevant to the topic of the dissertation for two
reasons: First, it shows how hypermedia may be used to drive a
read-write application, not just read-only, which is the focus of most
of the discussion. Second, it provides some of the basis for
\cite{ldf1}, which is important in the query planner.

The main idea in this paper is to address Problem~\ref{prob:lapis} 
by looking upon RDF as a natural language
sentence with subject, predicate and object, and that sentence should
then be sufficiently human-readable to be straightforward to
understand when a developer does ``View Source''.

Its main weakness is how it deals with authentication: The basic idea
is that when a client visits a resource, it will be told what
write operations it can perform. However, write operations will rarely
be permitted unless the client has authenticated and been authorised
to do so, and so, it assumes that credentials will be supplied with
the first read request. While the HTTP standard permits clients to
supply credentials with such requests, in practice, it rarely if ever
happens, possibly for privacy reasons. Thus, the server will have to
challenge the client first, but this should not happen on a resource
that the client is permitted to read without authentication. 
Therefore, the proposed hypermedia needs a property so that the client
understands what to do if it wants to do write operations.

\subsection{Design of Experiments}\label{sec:condoe}

This work resulted in a paper that was nominated for Best Paper on the
Evaluation track on the International Semantic Web Conference 2013,
entitled ``Introducing Statistical Design of Experiments to SPARQL
Endpoint Evaluation'', which was co-authored with John
S. Tyssedal. The citation is:

\begin{quote}
Kjetil Kjernsmo and John~S. Tyssedal.
\newblock Introducing Statistical Design of Experiments to SPARQL Endpoint
  Evaluation.
\newblock In Harith Alani, Lalana Kagal, Achille Fokoue, Paul Groth, Chris
  Biemann, Josiane~Xavier Parreira, Lora Aroyo, Natasha Noy, Chris Welty, and
  Krzysztof Janowicz, editors, {\em The Semantic Web – ISWC 2013}, volume
  8219 of {\em Lecture Notes in Computer Science}, pages 360--375. Springer
  Berlin Heidelberg, 2013.
\end{quote}


Problem~\ref{prob:badstats} motivates this paper. 
To understand the context of this study, I noted none of the
benchmarks that were in use for the evaluation of SPARQL engines could
be used for basic statistical hypothesis testing, and clearly,
hypothesis testing must be a fundamental requirement. Then, I noted
substantial criticism forwarded by
\cite{Duan:2011:AOC:1989323.1989340} and \cite{MontoyaVCRA12}.

However, this study merely scratches the surface. To establish a
Design of Experiment-based methodology would be a multi-year project
in its own right, clearly beyond the scope of this
dissertation. Unfortunately, this also implies that this dissertation
does not feature an evaluation that lives up to the rigorous standards
I am advocating as it is unattainable within the time frame of this
work.

Originally, this paper was intended as the start of a methodology to
evaluate a federated system that addressed the
problems~\ref{prob:unknowndist},~\ref{prob:unknownconddist}~and~\ref{prob:largestats},
but as \cite{buil2013sparql} confirmed my suspicions that there were
severe stability problems with SPARQL endpoints, my attention shifted
towards the more immediate Problem~\ref{prob:syntacticcache}.

\subsection{Pushing Complexity Down the Stack}\label{sec:conpush}

This work resulted in a workshop paper entitled ``Pushing complexity
down the stack'', presented in the Developers Workshop at the
International Semantic Web Conference, by the lead author Gregory Todd
Williams. The citation is:

\begin{quote}
Gregory~Todd Williams and Kjetil Kjernsmo.
\newblock Pushing Complexity Down the Stack.
\newblock In {\em Proceedings of the ISWC Developers Workshop 2014}, volume
  1268 of {\em {CEUR} Workshop Proceedings}. CEUR-WS.org, May 2014.
\end{quote}


This work was the outcome of several hackathons that I organised in
the Perl community, where
problems~\ref{prob:breakdown},~\ref{prob:complexapi}~and~\ref{prob:dontjustpass}
were discussed at length after having become a major practical
issue. It is not very well known that Gregory Todd Williams' SPARQL
implementation written in Perl is one of the SPARQL 1.1 reference
implementations. It is also fully compliant with the
specification. Through our work, and the availability of traits-based
programming in Perl, see \cite{traits}, we had extensive discussions
on how to leave optimisations to lower levels, e.g. the underlying
database engine, when they had the potential to provide a better plan
or cost estimate. While it is certainly possible to do this without
this new programming paradigm, traits made the methodology much more
comprehensible, and so helped the subsequent work substantially.

This work also resulted in the ``Attean''
framework\footnote{\url{https://metacpan.org/release/Attean}}, also
mainly authored by Gregory Todd Williams with some smaller
contributions from me. It is also available in the Linux distribution
Debian (and derivatives, such as Ubuntu) as \texttt{libattean-perl}.

\subsection{Survey of HTTP Caching}\label{sec:consanity}

I committed to do this survey after Jürgen Umbrich challenged me to see
if the standards were actually followed, and thus address
Problem~\ref{prob:sanity} before further work to solve practical
caching problems. Rather than just survey the
usage for SPARQL Endpoints, I decided to do it for the breadth of the
Semantic Web. This resulted in a research track paper on the Extended
Semantic Web Conference 2015 entitled ``A Survey of HTTP Caching
Implementations on the Open Semantic Web'' with citation:

\begin{quote}
Kjetil Kjernsmo.
\newblock A Survey of HTTP Caching Implementations on the Open Semantic Web.
\newblock In Fabien Gandon, Marta Sabou, Harald Sack, Claudia d’Amato,
  Philippe Cudré-Mauroux, and Antoine Zimmermann, editors, {\em The Semantic
  Web. Latest Advances and New Domains}, volume 9088 of {\em Lecture Notes in
  Computer Science}, pages 286--301. Springer International Publishing, 2015.
\end{quote}

and a technical report that elaborates on data reduction
methods and statistical methods with the citation:

\begin{quote}
Kjetil Kjernsmo.
\newblock Addendum to a survey of {HTTP} caching on the {Semantic Web}.
\newblock Technical Report 444, Department of Informatics, University of Oslo,
  March 2015.
\end{quote}

The most important contribution of this paper was an understanding the
actual usage of caching headers, but it was also intended to raise the
awareness of these important Web standards in the Semantic Web community.

Although the support for caching headers is not yet widespread, it is
sufficient to perform experiments on real-world data.

The main shortcoming of this paper was that I didn't attempt to
validate the recorded freshness lifetime against the true change
frequency recorded by \cite{dyldo2} and discussed by
\cite{Dividino2015}. It would be interesting to see how common it is
that the given freshness lifetime, both given as standard-compliant
caching and from existing heuristics, is wrong. There are two types of
errors: That the freshness lifetime is too short, i.e., we could have
cached it longer, and that it is too long, i.e., we would serve stale
data.

Also, the investigation would be deeper if it tried to address
Problem~\ref{prob:lawsfuture}. A single analysis would not be
sufficient to do that, laws would require multiple studies over time.

\subsection{Philosophy of Science}\label{sec:conphil}

This direction was motivated by a dissatisfaction with the methodology
used in the literature, specifically benchmarking, to test assertions
of performance. That led to exploration of statistical methods referenced
in Section~\ref{sec:condoe}, and to the organising of the Empirical 2014
Workshop at the Extended Semantic Web Conference (which I was
unfortunately prevented from attending myself). Frustrated by
the slow progress of the last 16 years, and the difficulty in comparing
methods to the more mature natural sciences, I still felt the need to
address Problem~\ref{prob:badphil} and express a philosophical view of 
science, which I did by submitting an
article to the ``Negative or inconclusive results in Semantic Web''
workshop at the Extended Semantic Web Conference 2015. The essay, see
\cite{kjernsmo_noise_2015} was tangential to the Call for Papers, but
the chairs decided to accept it as an interview. With Ruben Verborgh
in the role of the interviewer, we reformulated it and it was published in the
proceedings and is included in the dissertation. It was performed with
Jacco van Ossenbruggen as the interviewer in front of the
audience. The essay, at
\url{http://folk.uio.no/kjekje/2015/noise-essay.html} is however a
more thorough account. The citation of the interview is:
\begin{quote}
Kjetil Kjernsmo and Ruben Verborgh.
\newblock How can scientific methods provide guidance for semantic web research
  and development.
\newblock In {\em Workshop on Negative or Inconclusive Results in Semantic Web
  Proceedings}, volume 1435 of {\em {CEUR} Workshop Proceedings}. CEUR-WS.org,
  2015.
\end{quote}


As a philosophical text, the contributions are hard to detail. It
should establish Problem~\ref{prob:testingnonexist} as a fundamental
problem. It is further
written in a provocative style, to encourage discussions of
epistemological questions. It also points out shortcomings in current
practice, and it is clear that a proper evaluation of the work
outlined in Chapter~\ref{sec:tpfcacheplanning} would be a major
undertaking. 

In retrospect, the greatest weakness of the essay is that I failed to
recognise the impact of Frank van Harmelen's call for studies that can
establish laws of the Semantic Web information universe, as discussed
in Section~\ref{sec:evalproblems}. To my defence, I was only made
aware of this keynote by the reviewer.

\section{Unpublished Work on Query Caching}\label{sec:unpublished}

Finally, the intention was to bring convergence in the topics of this
thesis around a system to cache results that could be reused in
subsequent queries. As noted in Section~\ref{sec:motivcache}, this was
motivated by opportunity as well as problems, but it is intended to
address
problems~\ref{prob:syntacticcache}~and~\ref{prob:cachecartesian}. This
work was not finished within the time frame of the dissertation, but
is discussed at length in Chapter~\ref{sec:tpfcacheplanning}. This
work further relates to the problems in Section~\ref{sec:problems} as follows:
Triple Pattern Fragments shifts most of the processing from servers to
clients, but Problem~\ref{prob:microcontroller} makes that infeasible
in some cases.  In addition,
problems~\ref{prob:unknowndist},~\ref{prob:unknownconddist}~and~\ref{prob:largestats},
are compounded by Problem~\ref{prob:nostats}, but alleviated somewhat
by the presence of cardinality estimates given by Triple Pattern
Fragments, which we exploit.


