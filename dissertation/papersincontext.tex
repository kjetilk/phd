\section{The papers in context}

This section will put the included papers into context. It also enjoys
the benefit of hindsight, and will therefore address some of the
shortcomings that have surfaced since their publication.

\subsection{Read-write hypermedia}\label{sec:conlapis}

This work is a paper entitled ``The necessity of hypermedia RDF and an
approach to achieve it'' that was presented in the First Linked APIs
workshop at the Ninth Extended Semantic Web Conference. I was the sole
author. The reference is \cite{kjernsmo_lapis_2012}.

%TODO: not the Dagstuhl?

This paper is relevant to the topic of the dissertation for two
reasons: First, it shows how hypermedia may be used to drive a
read-write application, not just read-only, which is the focus of most
of the discussion. Second, it provides some of the basis for
\cite{ldf1}, which is important in the query planner.

The main idea in this paper is to look upon RDF as a natural language
sentence with subject, predicate and object, and that sentence should
then be sufficiently human-readable to be straightforward to
understand when a developer does ``View Source''.

Main contributions:
\begin{enumerate}
\item A thorough discussion of the implications of Hypermedia
  Types\cite{hypermediatypes} for RDF.
\item A sketch of a vocabulary for read-write RDF hypermedia.
\end{enumerate}

Its main weakness is how it deals with authentication: The basic idea
is that when a client visits a resource, it will be told what
write operations it can perform. However, write operations will rarely
be permitted unless the client has authenticated and been authorised
to do so, and so, it assumes that credentials will be supplied with
the first read request. While the HTTP standard permits clients to
supply credentials with such requests, in practice, it rarely if ever
happens, possibly for privacy reasons. Thus, the server will have to
challenge the client first, but this should not happen on a resource
that the client is permitted to read. Thus, the proposed hypermedia
needs a property so that the client understands what to do if it wants
to do write operations.

\subsection{Design of Experiments}\label{sec:condoe}

This work resulted in a paper that was nominated for Best Paper on the
Evaluation track on the International Semantic Web Conference 2013,
entitled ``Introducing Statistical Design of Experiments to SPARQL
Endpoint Evaluation'', which was co-authored with John
S. Tyssedal. The reference is \cite{kjernsmo_doe_intro}.

To understand the context of this study, I noted none of the
benchmarks that was in use for the evaluation of SPARQL engines could
be used for basic statistical hypothesis testing, and clearly,
hypothesis testing must be a fundamental requirement. Then, I noted
substantial criticism forwarded by
\cite{Duan:2011:AOC:1989323.1989340} and \cite{MontoyaVCRA12}.

However, this study merely scratches the surface. To establish a
Design of Experiment-based methodology would be a multi-year project
in its own right, clearly beyond the scope of this
dissertation. Unfortunately, this also implies that I do not have the
rigorous evaluation that I would preferred to have in this
dissertation, it is unattainable.

Main contributions:
\begin{enumerate}
\item Introduction of a path to critical practice of evaluations, that makes
  use of contemporary statistical techniques, to establish a practice
  that can be used to refute assertions on performance.
\item A didactical experiment to help researchers understand the statistics.
\item The novel application of a well established method in
  statistics, rarely used in Computer Science, to SPARQL endpoint
  evaluation.
  
\end{enumerate}

\subsection{Pushing complexity down the stack}\label{sec:conpush}

This work resulted in a workshop paper entitled ``Pushing complexity
down the stack'', presented in the Developers Workshop at the
International Semantic Web Conference, by the lead author Gregory Todd
Williams, we were the two authors. The reference is
\cite{williamspushing}.

Main contributions:
\begin{enumerate}
\item A framework to enable the use of low-level optimisations in
  databases.
\item Simplification when implementing experimental features in
  SPARQL.
\item Make it possible to compose custom query planners that doesn't
  require inheritance.
\end{enumerate}

This work was the outcome of several hackathons that I had organised
in the Perl community. It is not very well known that Gregory Todd
Williams' SPARQL implementation written in Perl is one of the SPARQL
1.1 reference implementations, and is known to be fully compliant with
the specification. Through our work, and the availability of
traits-based programming in Perl, see \cite{traits}, we had extensive
discussions on how to leave optimisations to lower levels, e.g. the
underlying database engine, when they had the potential to provide a
better plan or cost estimate. While it is certainly possible to do
this without this new programming paradigm, traits made the
methodology much more comprehensible, and so helped the subsequent
work substantially.

This work also resulted in the ``Attean''
framework\footnote{\url{https://metacpan.org/release/Attean}}, also
mainly authored by Gregory Todd Williams with some smaller
contributions from me. It is also available in the Linux distribution
Debian (and derivatives, such as Ubuntu) as \texttt{libattean-perl}.

\subsection{Survey of HTTP caching}\label{sec:consanity}

I committed to do the survey after JÃ¼rgen Umbrich challenged me to see
if the standards were actually followed. Rather than just survey the
usage for SPARQL Endpoints, I decided to do it for the breadth of the
Semantic Web. This resulted in a research track paper on the Extended
Semantic Web Conference 2015 entitled ``A Survey of HTTP Caching
Implementations on the Open Semantic Web'' with reference
\cite{kjernsmo_survey_2015} and a technical report
\cite{kjernsmo_add_survey_2015} that elaborates on data reduction
methods and statistical methods.

The most important contribution of this paper was an understanding the
actual usage of caching headers, but it was also intended to raise the
awareness of these important Web standards in the Semantic Web community.

Although the support for caching headers is not yet widespread, it is
sufficient to perform experiments on real-world data.

\subsection{Philosophy of Science}\label{sec:conphil}

Dissatisfaction with the methodology used in the literature,
specifically benchmarking, to test assertions of performance lead to
exploration of statistical methods in referenced in
Section~\ref{sec:condoe}, and to organise the Empirical 2014 Workshop
at the Extended Semantic Web Conference (which I was unfortunately
prevented from attending myself). Still, frustrated by the slow
progress of the last 16 years, and difficulty with comparing methods
to the more mature natural sciences, I still felt the need to express
a philosophical view of science, which I did by submitting an article
to the ``Negative or inconclusive results in Semantic Web'' at the
Extended Semantic Web Conference 2015. The essay, see
\cite{kjernsmo_noise_2015} was tangential to the Call for Papers, but
the chairs decided to accept it as an interview. With Ruben Verborgh
in the role of the interviewer, we reformulated it as
\cite{kjernsmo_noise_2015_interview}. It was performed with  Jacco van
Ossenbruggen as the interviewer in front of the audience.

As a philosophical essay, the contributions are hard to detail. It is
written in a provocative style, to encourage discussions of
epistemological questions. It also points out shortcomings in current
practice, and it is clear that a proper evaluation of the work
outlined in Chapter~\ref{sec:tpfcacheplanning} would be a major
undertaking. 

In retrospect, the greatest weakness of the essay is that I failed to
recognise the impact of Frank van Harmelen's call for studies that can
establish laws of the Semantic Web information universe, as discussed
in Section~\ref{sec:evalproblems}. To my defence, I was only made
aware of this keynote by the reviewer.
