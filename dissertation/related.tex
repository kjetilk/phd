\section{Related work}\label{sec:related}

There is a substantial amount of relevant literature and ongoing
research that is well aligned with the present work, either because
the authors share parts of the motivation, or more serendipitous
reuse.

The related work on the topic of evaluations based on contemporary
statistics, and on the topic of philosophy is scarce, and have been
dealt with in the papers. For the other topics dealt with in this
thesis, a more extensive treatment is needed to place this work in
context:

\subsection{Hypermedia}

Hypermedia, in the context of the Web, has been strongly influenced by
Fielding's definition of the REST architectural style, see
\cite{Fielding_2000_Architectural-Styles}~Chapter~5, in particular, a
constraint that he formulated, known as ``Hypermedia as the Engine of
Application State'' (abbreviated HATEOAS). The implications for an
application using RDF is that the application should be able to tell
from the RDF data \emph{only} how any interactions should occur. The
semantics expressed with RDF may enable this.

An example of this use is the work considered in
Section~\ref{sec:conlapis}, another example is the new direction
proposed by Verborgh et al. \cite{ldf1} who wanted a system that would
be able to answer individual triple patterns to enable clients to
answer queries, and to do so, they employed hypermedia, and formulated
this as Triple Pattern Fragments. The key to this hypermedia is that
an answer does not only include the data, but also metadata,
importantly a cardinality estimate for the triple pattern, and also
control information, that tells a client explicitly in every response
how further interaction should be done, like obtaining further data.

Overall, this study share many of the observations and goals that
motivate this study. Their stated goal is to transfer as much as
possible of the burden of evaluating a query from the server to the
client. This can happen because a simple pattern match for a single
triple pattern need not have a SPARQL parser, nor a query
planner. Moreover, the server may materialise responses to frequent
triple patterns and store them in a file system, which can be managed
by a simple Web server. The results may also be paged, so that each
individual response may be kept small. 

However, it is not necessarily the case that evaluating the entire
query on the client side is the most efficient, depending in
particular on the perspective of the different
actors. Chapter~\ref{sec:tpfcacheplanning} adopts Triple Pattern
Fragments, but makes different assumptions, e.g. tries to avoid a
large number of HTTP requests. 

Also, TPF mandates that every response must contain a cardinality
estimate for every triple pattern, an operation that may be quite
expensive for the server, and
%TODO: ref characteristic sets?
indeed, this information may be so expensive to provide that SPARQL
planners may not provide it to keep the cost of planning itself
down. 

In \cite{verborgh2014querying}, they demonstrated SPARQL evaluation
over TPF, and TPF has become an active area of research.

Under the auspices of the  World Wide Web Consortium a standard for
read-write applications using RDF has been developed: The Linked Data
Platform \cite{ldp1}. The abstract in the specification describes it
as:
\begin{quote}
Linked Data Platform (LDP) defines a set of rules for HTTP operations
on web resources, some based on RDF, to provide an architecture for
read-write Linked Data on the web.
\end{quote}
While developers who are familiar with HTTP-based applications and
Linked Data may feel familiar with the platform, it still requires
that they read and understand the specification to be able to use it,
and therefore, it is not hypermedia, and I think it misses out on many
of the benefits of hypermedia. A discussion at the Developers Workshop
at the Extended Semantic Web Conference 2015\footnote{see
  \url{https://www.youtube.com/watch?v=F4WN4XEpViA} for a recording of
  the discussion session.} found rough consensus around a comment
formulated by me:
\begin{quote}
LDP has to be superseded by hypermedia in some way.
\end{quote}


\subsection{Query answering with cache}\label{sec:relcache}

First, we note some claims that SPARQL query caching is not possible:
In \cite{hogan2014paths}, the authors examine cacheable as one of the
desiderata for sustainable data access. They claim, without further
justification, that SPARQL isn't cacheable. With this work and much of
the below related work, I suggest they are mistaken. Not only can
whole SPARQL queries be cached, but it is possible to break down the
query to parts that can be cached.

Two papers that span nearly two decades of research are of particular
interest: The HERMES system \cite{adali1996query} considered query
processing and cost-model based optimisation in a mediator system
where the mediator does not have access to source statistics
information. Recently, Papailiou et al. \cite{papailiou2015graph}
made an extensive study of SPARQL caching, addressing some of the same
problems as the present work.

Even though these do not share our perspective, which is deployment on
proxies in the Internet and integration with HTTP-based caching, these
contributions provide important context. 

HERMES \cite{adali1996query} provided motivation for
\ref{sec:conpush}, but is also important prior art for the work in
Chapter~\ref{sec:tpfcacheplanning}. An important concept in the HERMES
system is ``invariants''. They are, in our terminology, patterns that
do not need to be looked up remotely, but can be integrated in the
query answer by the mediator. The expectation in HERMES is that such
invariants will be encoded by a domain expert. The Attean framework
provides facilities to do so by using traits. Attean can also pass the
task of computing a scalar cost metric by a similar mechanism, but
HERMES has a more advanced cost model in that it has a cost vector
with estimates for time required to find the first answer, time to
find all answers and the cardinality of the answer set. Caching
summary statistics is another important feature of HERMES, and it also
contributes lossy and lossless summaries, for the purpose of saving
space and reduce the time it takes to compute a cost estimate. HERMES
further has a query rewriter and optimiser, that rewrites the query
taking into account the cache and the invariants. While HERMES is more
general than what I detail in Chapter~\ref{sec:tpfcacheplanning}, we
note that while invariants could be coded in Attean, a SPARQL query
cache should be oblivious to domain knowledge and therefore shouldn't
assume any invariants, except in the possible but unlikely case where
the remote server declares that an infinite freshness lifetime for the
result of a certain graph pattern. While statistics summarisation is
relevant and should be studied in future work, we note that with a
Linked Data Fragments server, a cardinality estimate is required to be
available for every triple pattern, and is also cacheable with the
same constraints as the data themselves.

Interestingly, \cite{papailiou2015graph} opens by noting that many
optimisations used by SQL databases are ineffective in RDF databases,
due to that RDF schema, even if they exist, are not as helpful as the
obligatory schema in SQL databases. This provides an opportunity for
novel research. The solution proposed in this paper is not a cache on
a proxy or a mediator like my primary focus is. The execution engine
in this case, has access to both the primary indexes of the RDF store
and the cache. A key contribution of the paper is a canonical
labelling algorithm that can create a string that will be unique for a
Basic Graph Pattern including optional blocks. This is done by
transforming a SPARQL query to a directed vertex-coloured graph, where
each triple pattern is represented by a vertex, and triple patterns
that share a common variable is linked with an edge. For each vertex,
a label that consists of three IDs are created. Bound terms are
translated to IDs using a dictionary, while variables are translated
to zero. Then, the graph is directed and edges labelled according to
the type of join they represent. The vertex and edge labels are then
translated to non-negative integers (colours) using a sort key, to
form an directed vertex-and-edge-coloured graph that is the authors
claim is isomorphic with the original SPARQL. Finally, the graph is
transformed to a simpler directed vertex-coloured graph. With this
graph, the author's method can produce a canonical label for the query
graph.

The query planner first examines all connected subgraphs of a full
SPARQL query, and generates canonical labels for each one. Since it
insists on caching connected subgraphs, it avoids the problem I
encountered in Chapter~\ref{sec:tpfcacheplanning} that the cache may
break up connected subgraphs and thus create Cartesian joins. To
further enhance the query planner, the cache can optionally include
indexing. The authors also introduce the concept of
\textit{profitable} query patterns. It is not quite clear from the
text, but it seems this implies that the cache can also prefetch
results into the cache, if it finds that the result may benefit
subsequent queries.


The canonical labelling algorithm presented in this paper provides an
opportunity for further work, when considered in an HTTP context: The
\httph{ETag} header, defined in RFC7232 \cite{rfc7232}, can optionally
contain an opaque identifier for a specific version of a resource
representation, e.g. a SPARQL result. Such a canonical label could be
used in conjunction with an algorithm that tracks changes to the RDF
graph to produce an \httph{ETag}. For slow-changing Linked Data, which
is a common case, it is a possibility that a canonical label in
combination with a time-stamp of the entire RDF Graph could yield
considerable benefits. However, the presented algorithm does not cover
SPARQL in its entirety, for example, it does not handle named graphs,
solution modifiers and filters, nor is it clear how it handles
property paths (which in many cases should be trivial, since a
property path often has an equivalent Basic Graph Pattern). Union
queries or negation is also not mentioned, so more work is needed for
this method to have general applicability. Nevertheless, if the entire
Group Graph Pattern of a certain query is supported, and the query
projects all variables, e.g. it wraps the pattern with \sparql{SELECT
  *} or \sparql{CONSTRUCT}, the \httph{ETag} would be trivial to
construct using the presented algorithm in a non-trivial number of
beneficial cases. With this, cached result verification could be done
by a mediator using HTTP requests with the \httph{If-None-Match}
header and so make it possible to use parts of the engine of this
paper usable in the Internet infrastructure.

Olaf Hartig has a large number of papers that are adjacent to the
present work, including his Ph.D. dissertation \cite{hartig2014querying} where he
explores querying Linked Data, i.e. query data that is not contained
in an \textit{a priori} defined collection of RDF data, but where resources
are traversed to compute a result. Hartig provides solid formal
foundations for such query processing, as well as computational
feasibility, soundness and completeness. The most relevant paper to
this work is \cite{hartig2011caching}.

Acosta et al. has published a poster on ``SHEPHERD''
\cite{acosta2014shepherd}, presumed to be have ongoing, to
create a SPARQL processor that takes into account observations done by
the SPARQLES survey \cite{buil2013sparql}, to decompose a SPARQL query into
subqueries that is indicated to be easier for the server to evaluate,
and so is well aligned with our goal to ease the load of the
server. It is not quite clear from the brief description in 
\cite{acosta2014shepherd} whether the SHEPHERD system can cache results locally.

The following studies do not consider how the results of one query can
benefit the execution of a subsequent query. Nevertheless, they are
important to consider as prior art:
Umbrich et al. \cite{umbrich2012hybrid} considered a situation where
parts of the graph change at different paces. For the slow-changing
parts, they prefetch an entire dataset to a local store to relieve
remote endpoints from some processing, and thereby speed up query
responses. Of central importance to this approach is the notion of
coherence of query patterns, defined through the ratio between results
that were found in the live remote endpoint but not in the local cache
and the results from the live remote endpoint. Based on this, the need
for correct, up to date answers can be balanced against the need for
good response times.  This approach differs radically from our
approach in Chapter~\ref{sec:tpfcacheplanning} in that it replicates
an entire dataset rather than just fragments.

Naturally, the problem of caching in a proxy has much in common with
the federation problem, as the cache, as seen from the query execution
engine's perspective, could be view as yet another possible data
source, and the number of HTTP requests to the remote endpoint can be
wisely assumed to be kept small. Schwarte et
al. \cite{springerlink:10.1007/978-3-642-25073-6-38} addressed these
problems with FedX, in which they sought to group connected subgraphs
with their concept of exclusive groups.  Montoya et
al. \cite{montoya2015federated} propose a query federation system that
minimise the data needed to be transferred while preserving
completeness in the common case where the data relevant to a query is
available on different endpoints. GÃ¶rlitz and Staab \cite{splendid}
exploited cardinality estimates sometimes available in VoID
\cite{voidnote} descriptions to improve join order execution plans. It
could be advantageous for the system in
Chapter~\ref{sec:tpfcacheplanning} to exploit as well, and
straightforward to implement, but since it has not seen wide uptake
and the statistics is mandated by Triple Pattern Fragments, and
therefore available in some cases to the planner, we chose not to.

Martin et al. \cite{sparqlproxy} implemented a reverse caching proxy
that controlled the changes to the dataset rather than rely on HTTP
headers to track changes, and so could relieve the endpoint of the
burden of evaluating all queries. Such a proxy would work poorly in
the scenario considered in Chapter~\ref{sec:tpfcacheplanning}, as
there is no way to mandate use of the proxy in this scenario.

Williams and Weaver \cite{kaseicache} consider the implications of the
HTTP protocol on query caching in a quad store on the indexing methods
that were employed. They propose to introduce a modification timestamp
in index structures, and find that the query engine would be able to
identify pieces that have been changed during execution, and could
therefore expose a \httph{Last-Modified} header and validate an
earlier response in accordance with RFC7232 \cite{rfc7232} cheaper
than to evaluate the query. This also represent an alternative to HTTP
\httph{ETag} as in the discussion of \cite{papailiou2015graph}.

Dividino et al. \cite{Dividino2015} proposed strategies to keep caches
of Linked Open Data up to date, in face of changing data. The authors
sets out to learn the change frequency of resources, and based on that
schedule updates of the resources. This does then not depend on the
resource metadata (neither HTTP headers nor metadata in the RDF
itself, which I studied in \cite{kjernsmo_survey_2015}), which is
motivated by their finding that only 8\% of \httph{Last-Modified}
headers they surveyed reflected the true change frequency. While I
have no reason to doubt this finding, and the monitoring approach is
interesting, I think the dismissal of resource metadata is
insufficiently motivated, and therefore the evaluation is
inadequate. First, the \httph{Last-Modified} header is far from the
only cache-relevant header, as should be evident by the fact that it
is not defined in the cache standard, RFC7234, but in the conditional
request standard, RFC7232. Moreover, RFC7234 contains a Section~4.2.2.
titled ``Calculating Heuristic Freshness''. It is my opinion that the
paper should be framed within the loose constraints of this section,
where it could make a substantial contribution. As it stands, it would
serve to violate Web standards, which in my opinion, is something the
Semantic Web community should be more cautious about. RFC7234's
Section~4.2.2. also proposes a simple heuristic for calculating
freshness lifetime. I found that the distribution of this heuristic
freshness lifetime agrees well with The Dynamic Linked Data
Observatory (DyLDO) \cite{dyldo2}. However, a weakness of my study
which could make this observation irrelevant, is that I considered
only summary statistics. If the change frequency of individual
resources were wrong, in particular, if the use of the simple
heuristic lead to using stale content, it would be inappropriate to
use the simple heuristic.  RFC7234's explicitly prohibits the use of a
heuristic if an explicit expiration time is present, yet the paper
does not attempt to include those in their assessment. Furthermore, I
found in \cite{kjernsmo_survey_2015} that cache prohibitions were
common. While they in many cases may be misguided, they nevertheless
express a strong statement from the server administrators, that should
be respected. RDF data, e.g. \rdfterm{dct:valid} and
\rdfterm{dct:modified} could also be used to compute a expiration
time. Finally, an opaque validator, \httph{ETag} is also a part of
RFC7232. After considering cache prohibitions, explicit expiration
times, opaque validators, simpler heuristics, etc., the space where
learning the change frequency could be applied is likely much
smaller. However, if understood as a sophisticated heuristic within
the constraints of RFC7234's Section~4.2.2., the contribution is
valuable.

%\subsection{Tools for developer efficiency}
%\todo{more here}


While the approaches are diverse, these studies pull in the same
direction: Motivated by the problems of maintaining SPARQL endpoints,
they all seek to share the burden, and so make query answering
economically feasible.



%%  LocalWords:  summarisation Schwarte
