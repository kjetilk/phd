\section{Related work}\label{sec:related}

There is a substantial amount of relevant literature and ongoing
research that is well aligned with the present work, either because
the authors share parts of the motivation, or more serendipitous
reuse.

The related work on the topic of evaluations based on contemporary
statistics, and on the topic of philosophy is scarce, and have been
dealt with in the papers. For the other topics dealt with in this
thesis, a more extensive treatment is needed to place this work in
context:

\subsection{Hypermedia}

Verborgh et al. \cite{ldf1} share many of the observations and goals
that motivate this study. They introduce a hypermedia system that can
be used to answer individual triple patterns (known as Triple Pattern
Fragments (TPF)). Their stated goal is to transfer as much as possible
of the burden of evaluating a query from the server to the
client. This can happen because a simple pattern match for a single
triple pattern need not have a SPARQL parser, nor a query
planner. Moreover, the server may materialise responses to frequent
triple patterns and store them in a file system, which can be managed
by a simple Web server. The results may also be paged, so that each
individual response may be kept small. However, TPF mandates that
every response must contain a cardinality estimate for every triple
pattern, an operation that may be quite expensive for the server, and
%TODO: ref characteristic sets?
indeed, this information may be so expensive to provide that SPARQL
planners may not provide it to keep the cost of planning itself
down. In \cite{verborgh2014querying}, they demonstrated SPARQL evaluation over TPF,
and further improvements have been made in more recent papers.\todo{more here}

\subsection{Query answering with cache}

Two papers that span nearly two decades of research are of particular
interest: The HERMES system \cite{adali1996query} considered query
processing and cost-model based optimisation in a mediator system
where the mediator does not have access to source statistics
information. Recently, Papailiou et al. \cite{papailiou2015graph}
made an extensive study of SPARQL caching, addressing some of the same
problems as the present work.

Even though these do not share our perspective, which is deployment on
proxies in the Internet and integration with HTTP-based caching, these
contributions provide important context. 

HERMES \cite{adali1996query} provided motivation for
\ref{sec:conpush}, but is also important prior art for the work in
Chapter~\ref{sec:tpfcacheplanning}. An important concept in the HERMES
system is ``invariants''. They are, in our terminology, patterns that
do not need to be looked up remotely, but can be integrated in the
query answer by the mediator. The expectation in HERMES is that such
invariants will be encoded by a domain expert. The Attean framework
provides facilities to do so by using traits. Attean can also pass the
task of computing a scalar cost metric by a similar mechanism, but
HERMES has a more advanced cost model in that it has a cost vector
with estimates for time required to find the first answer, time to
find all answers and the cardinality of the answer set. Caching
summary statistics is another important feature of HERMES, and it also
contributes lossy and lossless summaries, for the purpose of saving
space and reduce the time it takes to compute a cost estimate. HERMES
further has a query rewriter and optimiser, that rewrites the query
taking into account the cache and the invariants. While HERMES is more
general than what I detail in Chapter~\ref{sec:tpfcacheplanning}, we
note that while invariants could be coded in Attean, a SPARQL query
cache should be oblivious to domain knowledge and therefore shouldn't
assume any invariants, except in the possible but unlikely case where
the remote server declares that an infinite freshness lifetime for the
result of a certain graph pattern. While statistics summarisation is
relevant and should be studied in future work, we note that with a
Linked Data Fragments server, a cardinality estimate is required to be
available for every triple pattern, and is also cacheable with the
same constraints as the data themselves.

Interestingly, \cite{papailiou2015graph} opens by noting that many
optimisations used by SQL databases are ineffective in RDF databases,
due to that RDF schema, even if they exist, are not as helpful as the
obligatory schema in SQL databases. This provides an opportunity for
novel research. The solution proposed in this paper is not a cache on
a proxy or a mediator like my primary focus is. The execution engine
in this case, has access to both the primary indexes of the RDF store
and the cache. A key contribution of the paper is a canonical
labelling algorithm that can create a string that will be unique for a
Basic Graph Pattern including optional blocks. This is done by
transforming a SPARQL query to a directed vertex-coloured graph, where
each triple pattern is represented by a vertex, and triple patterns
that share a common variable is linked with an edge. For each vertex,
a label that consists of three IDs are created. Bound terms are
translated to IDs using a dictionary, while variables are translated
to zero. Then, the graph is directed and edges labelled according to
the type of join they represent. The vertex and edge labels are then
translated to non-negative integers (colours) using a sort key, to
form an directed vertex-and-edge-coloured graph that is the authors
claim is isomorphic with the original SPARQL. Finally, the graph is
transformed to a simpler directed vertex-coloured graph. With this
graph, the author's method can produce a canonical label for the query
graph.

The query planner first examines all connected subgraphs of a full
SPARQL query, and generates canonical labels for each one. Since it
insists on caching connected subgraphs, it avoids the problem I
encountered in Chapter~\ref{sec:tpfcacheplanning} that the cache may
break up connected subgraphs and thus create Cartesian joins. To
further enhance the query planner, the cache can optionally include
indexing. The authors also introduce the concept of
\textit{profitable} query patterns. It is not quite clear from the
text, but it seems this implies that the cache can also prefetch
results into the cache, if it finds that the result may benefit
subsequent queries.


The canonical labelling algorithm presented in this paper provides an
opportunity for further work, when considered in an HTTP context: The
\httph{ETag} header, defined in RFC7232 \cite{rfc7232}, can optionally
contain an opaque identifier for a specific version of a resource
representation, e.g. a SPARQL result. Such a canonical label could be
used in conjunction with an algorithm that tracks changes to the RDF
graph to produce an \httph{ETag}. For slow-changing Linked Data, which
is a common case, it is a possibility that a canonical label in
combination with a time-stamp of the entire RDF Graph could yield
considerable benefits. However, the presented algorithm does not cover
SPARQL in its entirety, for example, it does not handle named graphs,
solution modifiers and filters, nor is it clear how it handles
property paths (which in many cases should be trivial, since a
property path often has an equivalent Basic Graph Pattern). Union
queries or negation is also not mentioned, so more work is needed for
this method to have general applicability. Nevertheless, if the entire
Group Graph Pattern of a certain query is supported, and the query
projects all variables, e.g. it wraps the pattern with \sparql{SELECT
  *} or \sparql{CONSTRUCT}, the \httph{ETag} would be trivial to
construct using the presented algorithm in a non-trivial number of
beneficial cases. With this, cached result verification could be done
by a mediator using HTTP requests with the \httph{If-None-Match}
header and so make it possible to use parts of the engine of this
paper usable in the Internet infrastructure.


\todo{import from paper}

Olaf Hartig has a large number of papers that are adjacent to the
present work, including his Ph.D. dissertation \cite{hartig2014querying} where he
explores querying Linked Data, i.e. query data that is not contained
in an \textit{a priori} defined collection of RDF data, but where resources
are traversed to compute a result. Hartig provides solid formal
foundations for such query processing, as well as computational
feasibility, soundness and completeness. The most relevant paper to
this work is \cite{hartig2011caching}.\todo{more here}

Acosta et al. has published a poster on ``SHEPHERD''
\cite{acosta2014shepherd}, presumed to be have ongoing, to
create a SPARQL processor that takes into account observations done by
the SPARQLES survey \cite{buil2013sparql}, to decompose a SPARQL query into
subqueries that is indicated to be easier for the server to evaluate,
and so is well aligned with our goal to ease the load of the
server. It is not quite clear from the brief description in 
\cite{acosta2014shepherd} whether the SHEPHERD system can cache results locally.

Montoya et al. \cite{montoya2015federated} propose a query federation
system that takes advantage of client-side data replication. Both
these studies claim to support the entire SPARQL query language, but
in both cases, it is not clear that they actually do that.\todo{more here}

Umbrich et al. \cite{umbrich2012hybrid} considered a situation where
parts of the graph change at different paces. For the slow-changing
parts, they prefetch an entire dataset to a local store to relieve
remote endpoints from some processing, and thereby speed up query
responses. \todo{more here}

Martin et al. \cite{sparqlproxy} implemented a reverse caching proxy
that controlled the changes to the dataset, and so could relieve the
endpoint of the burden of evaluating all queries.\todo{more here}

Dividino et al. \cite{Dividino2015} proposed strategies to keep caches
of Linked Open Data up to date, in face of changing data. The authors
sets out to learn the change frequency of resources, and based on that
schedule updates of the resources. This does then not depend on the
resource metadata (neither HTTP headers nor metadata in the RDF
itself, which I studied in \cite{kjernsmo_survey_2015}), which is
motivated by their finding that only 8\% of \httph{Last-Modified}
headers they surveyed reflected the true change frequency. While I
have no reason to doubt this finding, and the monitoring approach is
interesting, I think the dismissal of resource metadata is
insufficiently motivated, and therefore the evaluation is
inadequate. First, the \httph{Last-Modified} header is far from the
only cache-relevant header, as should be evident by the fact that it
is not defined in the cache standard, RFC7234, but in the conditional
request standard, RFC7232. Moreover, RFC7234 contains a Section~4.2.2.
titled ``Calculating Heuristic Freshness''. It is my opinion that the
paper should be framed within the loose constraints of this section,
where it could make a substantial contribution. As it stands, it would
serve to violate Web standards, which in my opinion, is something the
Semantic Web community should be more cautious about. RFC7234's
Section~4.2.2. also proposes a simple heuristic for calculating
freshness lifetime. I found that the distribution of this heuristic
freshness lifetime agrees well with The Dynamic Linked Data
Observatory (DyLDO) \cite{dyldo2}. However, a weakness of my study
which could make this observation irrelevant, is that I considered
only summary statistics. If the change frequency of individual
resources were wrong, in particular, if the use of the simple
heuristic lead to using stale content, it would be inappropriate to
use the simple heuristic.  RFC7234's explicitly prohibits the use of a
heuristic if an explicit expiration time is present, yet the paper
does not attempt to include those in their assessment. Furthermore, I
found in \cite{kjernsmo_survey_2015} that cache prohibitions were
common. While they in many cases may be misguided, they nevertheless
express a strong statement from the server administrators, that should
be respected. RDF data, e.g. \rdfterm{dct:valid} and
\rdfterm{dct:modified} could also be used to compute a expiration
time. Finally, an opaque validator, \httph{ETag} is also a part of
RFC7232. After considering cache prohibitions, explicit expiration
times, opaque validators, simpler heuristics, etc., the space where
learning the change frequency could be applied is likely much
smaller. However, if understood as a sophisticated heuristic within
the constraints of RFC7234's Section~4.2.2., the contribution is
valuable.

\subsection{Tools for developer efficiency}
\todo{more here}


While the approaches are diverse, these studies pull in the same
direction: Motivated by the problems of maintaining SPARQL endpoints,
they all seek to share the burden, and so make query answering
economically feasible.



%%  LocalWords:  summarisation
