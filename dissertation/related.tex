\section{Related work}\label{sec:related}

There is a substantial amount of relevant literature and ongoing
research that is well aligned with the present work, either because
the authors share parts of the motivation, or more serendipitous
reuse.

The related work on the topic of evaluations based on contemporary
statistics, and on the topic of philosophy is scarce, and have been
dealt with in the papers. For the other topics dealt with in this
thesis, a more extensive treatment is needed to place this work in
context:

\subsection{Hypermedia}

Verborgh et al. \cite{ldf1} share many of the observations and goals
that motivate this study. They introduce a hypermedia system that can
be used to answer individual triple patterns (known as Triple Pattern
Fragments (TPF)). Their stated goal is to transfer as much as possible
of the burden of evaluating a query from the server to the
client. This can happen because a simple pattern match for a single
triple pattern need not have a SPARQL parser, nor a query
planner. Moreover, the server may materialise responses to frequent
triple patterns and store them in a file system, which can be managed
by a simple Web server. The results may also be paged, so that each
individual response may be kept small. However, TPF mandates that
every response must contain a cardinality estimate for every triple
pattern, an operation that may be quite expensive for the server, and
%TODO: ref characteristic sets?
indeed, this information may be so expensive to provide that SPARQL
planners may not provide it to keep the cost of planning itself
down. In \cite{verborgh2014querying}, they demonstrated SPARQL evaluation over TPF,
and further improvements have been made in more recent papers.\todo{more here}

\subsection{Query answering with cache}

\todo{\cite{adali1996query}}

Olaf Hartig has a large number of papers that are adjacent to the
present work, including his Ph.D. dissertation \cite{hartig2014querying} where he
explores querying Linked Data, i.e. query data that is not contained
in an \textit{a priori} defined collection of RDF data, but where resources
are traversed to compute a result. Hartig provides solid formal
foundations for such query processing, as well as computational
feasibility, soundness and completeness. The most relevant paper to
this work is \cite{hartig2011caching}.\todo{more here}

Acosta et al. has published a poster on ``SHEPHERD''
\cite{acosta2014shepherd}, presumed to be have ongoing, to
create a SPARQL processor that takes into account observations done by
the SPARQLES survey \cite{buil2013sparql}, to decompose a SPARQL query into
subqueries that is indicated to be easier for the server to evaluate,
and so is well aligned with our goal to ease the load of the
server. It is not quite clear from the brief description in 
\cite{acosta2014shepherd} whether the SHEPHERD system can cache results locally.

Recently, Papailiou et al. \cite{papailiou2015graph} made an extensive
study of SPARQL caching, addressing some of the same problems as the
present work, except for our emphasis on deployment on proxies in the
Internet and integration with HTTP-based caching.  \todo{reread the
  paper more here}

Montoya et al. \cite{montoya2015federated} propose a query federation
system that takes advantage of client-side data replication. Both
these studies claim to support the entire SPARQL query language, but
in both cases, it is not clear that they actually do that.\todo{more here}

Umbrich et al. \cite{umbrich2012hybrid} considered a situation where
parts of the graph change at different paces. For the slow-changing
parts, they prefetch an entire dataset to a local store to relieve
remote endpoints from some processing, and thereby speed up query
responses. \todo{more here}

Martin et al. \cite{sparqlproxy} implemented a reverse caching proxy
that controlled the changes to the dataset, and so could relieve the
endpoint of the burden of evaluating all queries.\todo{more here}

Dividino et al. \cite{Dividino2015} proposed strategies to keep caches
of Linked Open Data up to date, in face of changing data. The authors
sets out to learn the change frequency of resources, and based on that
schedule updates of the resources. This does then not depend on the
resource metadata (neither HTTP headers nor metadata in the RDF
itself, which I studied in \cite{kjernsmo_survey_2015}), which is
motivated by their finding that only 8\% of \httph{Last-Modified}
headers they surveyed reflected the true change frequency. While I
have no reason to doubt this finding, and the monitoring approach is
interesting, I think the dismissal of resource metadata is
insufficiently motivated, and therefore the evaluation is
inadequate. First, the \httph{Last-Modified} header is far from the
only cache-relevant header, as should be evident by the fact that it
is not defined in the cache standard, RFC7234, but in the conditional
request standard, RFC7232. Moreover, RFC7234 contains a Section~4.2.2.
titled ``Calculating Heuristic Freshness''. It is my opinion that the
paper should be framed within the loose constraints of this section,
where it could make a substantial contribution. As it stands, it would
serve to violate Web standards, which in my opinion, is something the
Semantic Web community should be more cautious about. RFC7234's
Section~4.2.2. also proposes a simple heuristic for calculating
freshness lifetime. I found that the distribution of this heuristic
freshness lifetime agrees well with The Dynamic Linked Data
Observatory (DyLDO) \cite{dyldo2}. However, a weakness of my study
which could make this observation irrelevant, is that I considered
only summary statistics. If the change frequency of individual
resources were wrong, in particular, if the use of the simple
heuristic lead to using stale content, it would be inappropriate to
use the simple heuristic.  RFC7234's explicitly prohibits the use of a
heuristic if an explicit expiration time is present, yet the paper
does not attempt to include those in their assessment. Furthermore, I
found in \cite{kjernsmo_survey_2015} that cache prohibitions were
common. While they in many cases may be misguided, they nevertheless
express a strong statement from the server administrators, that should
be respected. RDF data, e.g. \rdfterm{dct:valid} and
\rdfterm{dct:modified} could also be used to compute a expiration
time. Finally, an opaque validator, \httph{ETag} is also a part of
RFC7232. After considering cache prohibitions, explicit expiration
times, opaque validators, simpler heuristics, etc., the space where
learning the change frequency could be applied is likely much
smaller. However, if understood as a sophisticated heuristic within
the constraints of RFC7234's Section~4.2.2., the contribution is
valuable.

\subsection{Tools for developer efficiency}
\todo{more here}

\hskip

While the approaches are diverse, these studies pull in the same
direction: Motivated by the problems of maintaining SPARQL endpoints,
they all seek to share the burden, and so make query answering
economically feasible.


