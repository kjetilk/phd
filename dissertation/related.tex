\section{Related Work}\label{sec:related}

There is a substantial amount of relevant literature and ongoing
research that are well aligned with the present work, either because
the authors share parts of the motivation, or this work finds serendipitous
reuse of prior work.

The related work on the topic of evaluations based on contemporary
statistics, and on the topic of philosophy is scarce, and has been
dealt with in the papers. For the other topics dealt with in this
thesis, a more extensive treatment is needed to place this work in
context:

\subsection{Hypermedia}

Hypermedia, in the context of the Web, has been strongly influenced by
Fielding's definition of the REST architectural style, see
\cite{Fielding_2000_Architectural-Styles}~Chapter~5, in particular, a
constraint that he formulated, known as ``Hypermedia as the Engine of
Application State'' (abbreviated HATEOAS). The implications for an
application using RDF is that the application should be able to tell
from the RDF data \emph{only} how any interactions should occur. The
semantics expressed with RDF may enable this.

An example of this use is the work considered in
Section~\ref{sec:conlapis}, another example is the new direction
proposed by Verborgh et al. \cite{ldf1} who wanted a system that would
be able to answer individual triple patterns to enable clients to
answer queries, and in doing so, they employed hypermedia, and formulated
this as Triple Pattern Fragments (TPF). The key to this hypermedia is that
an answer does not only include the data, but also metadata,
importantly a cardinality estimate for the triple pattern, and also
control information, that tells a client explicitly in every response
how further interaction should be done, like in obtaining further data.

Overall, the work on query answering with Triple Pattern Fragments
share many of the observations and goals that
motivate this study. Their stated goal is to transfer as much as
possible of the burden of evaluating a query from the server to the
client. This can happen because a simple pattern match for a single
triple pattern need not have a SPARQL parser, nor a query
planner. Moreover, the server may materialise responses to frequent
triple patterns and store them in a file system, which can be managed
by a simple Web server. The results may also be paginated, so that each
individual response may be kept small. 

However, it is not necessarily the case that evaluating the entire
query on the client side is the most efficient, depending in
particular on the perspective of the different
actors. Chapter~\ref{sec:tpfcacheplanning} adopts Triple Pattern
Fragments, but makes different assumptions, for example by trying to avoid a
large number of HTTP requests. 

Also, TPF mandates that every response must contain a cardinality
estimate for every triple pattern, an operation that may be quite
expensive for the server.
Indeed, this information may be so expensive to provide that SPARQL
planners may not do so, in order to keep the cost of planning itself
down.

In \cite{verborgh2014querying}, they demonstrated SPARQL evaluation
over TPF, and TPF has become an active area of research. The impact of
caching is also measured in this work, as the result of a single
triple pattern is far easier to cache using HTTP mechanisms than the
result of an entire SPARQL query.

Under the auspices of the  World Wide Web Consortium a standard for
read-write applications using RDF has been developed: The Linked Data
Platform \cite{ldp1}. The abstract in the specification describes it
as:
\begin{quote}
Linked Data Platform (LDP) defines a set of rules for HTTP operations
on web resources, some based on RDF, to provide an architecture for
read-write Linked Data on the web.
\end{quote}
While developers who are familiar with HTTP-based applications and
Linked Data may feel familiar with the platform, it still requires
that they read and understand the specification to be able to use it,
and therefore, it is not hypermedia, and I think it misses out on many
of the benefits of hypermedia. A discussion at the Developers Workshop
at the Extended Semantic Web Conference 2015\footnote{See
  \url{https://www.youtube.com/watch?v=F4WN4XEpViA} for a recording of
  the discussion session.} found rough consensus around a comment
formulated by me:
\begin{quote}
LDP has to be superseded by hypermedia in some way.
\end{quote}


\subsection{Query Answering With Cache}\label{sec:relcache}

First, we note a claim that SPARQL query caching is not possible:
In \cite{hogan2014paths}, the authors examine cacheability as one of the
desiderata for sustainable data access. They claim, without further
justification, that SPARQL isn't cacheable. With this work and much of
the below related work, I suggest they are mistaken. Not only can
whole SPARQL queries be cached, but it is possible to break down the
query to parts that can be cached.

As such, this mirrors an extensive development known as ``Semantic
Caching'' in the database community. Ahmad et al. \cite{4777801}
surveyed the state of the art in that area. They divided caching
approaches into \emph{page}, \emph{tuple} and \emph{semantic} caches,
and noted that in the two former cases, if only partial results were
available in the cache, the entire result would have to be fetched
from the remote database. Indeed, a page or tuple cache may be used to
cache the results of a whole SPARQL query in HTTP context, but SPARQL
queries are also trivial to partition by e.g. triple patterns or Basic
Graph Patterns, which provides further opportunities for caching where
only partial results are available.

Two papers that span nearly two decades of research are of particular
interest: The HERMES system \cite{adali1996query} considered query
processing and cost-model based optimisation in a mediator system
where the mediator does not have access to source statistics
information. Recently, Papailiou et al. \cite{papailiou2015graph}
made an extensive study of SPARQL caching, addressing some of the same
problems as the present work.

Even though these do not share our perspective, which is deployment on
proxies in the Internet and integration with HTTP-based caching, these
contributions provide important context. 

HERMES \cite{adali1996query} provided motivation for the paper in
Section~\ref{sec:conpush}, but is also important for the work in
Chapter~\ref{sec:tpfcacheplanning}. An important concept in the HERMES
system is ``invariants''. They are, in our terminology, patterns that
do not need to be looked up remotely, but can be integrated in the
query answer by the mediator. The expectation in HERMES is that such
invariants will be encoded by a domain expert. The HERMES planner
bases decision on a cost vector with estimates for time required to
find the first answer, time to find all answers and the cardinality of
the answer set. Caching summary statistics is another important
feature of HERMES, and it also contributes lossy and lossless
summaries, for the purpose of saving space and reduce the time it
takes to compute a cost estimate. HERMES further has a query rewriter
and optimiser, that rewrites the query taking into account the cache
and the invariants. While statistics summarisation is relevant and should be
studied in future work, we note that with a Linked Data Fragments
server, a cardinality estimate is required to be available for every
triple pattern, and is also cacheable with the same constraints as the
data themselves.

Interestingly, \cite{papailiou2015graph} opens by noting that many
optimisations used by SQL\linebreak[4] databases are ineffective in RDF databases,
due to that RDF schema, even if they exist, are not as helpful as the
obligatory schema in SQL databases. This provides an opportunity for
novel research. The solution proposed in \cite{papailiou2015graph}  is not a cache on
a proxy or a mediator like that which is my primary focus. The execution engine
in this case, has access to both the primary indices of the RDF store
and the cache. A key contribution of the paper is a canonical
labelling algorithm that can create a string that will be unique for a
Basic Graph Pattern including optional blocks. This is done by
transforming a SPARQL query to a directed vertex-coloured graph, where
each triple pattern is represented by a vertex, and triple patterns
that share a common variable is linked with an edge. For each vertex,
a label that consists of three IDs are created. Bound terms are
translated to IDs using a dictionary, while variables are translated
to zero. Then, the graph is directed and edges labelled according to
the type of join they represent. The vertex and edge labels are then
translated to non-negative integers (colours) using a sort key, to
form a directed vertex-and-edge-coloured graph that according to the
authors is isomorphic with the original SPARQL. Finally, the graph is
transformed to a simpler directed vertex-coloured graph. With this
graph, the author's method can produce a canonical label for the query
graph.

The query planner first examines all connected subgraphs of a full
SPARQL query, and generates canonical labels for each one. To
further enhance the query planner, the cache can optionally include
indexing. The authors also introduce the concept of
\textit{profitable} query patterns. It is not quite clear from the
text, but it seems this implies that the cache can also prefetch
results into the cache, if it finds that the result may benefit
subsequent queries.

Olaf Hartig has a large number of papers that are adjacent to the
present work, including his Ph.D. dissertation \cite{hartig2014querying} where he
explores querying Linked Data, i.e. query data that is not contained
in an \textit{a priori} defined collection of RDF data, but where resources
are traversed to compute a result. Hartig provides solid formal
foundations for such query processing, as well as computational
feasibility, soundness and completeness. The most relevant paper to
this work is \cite{hartig2011caching}. In that work, he considers the
impact of caching on performance and completeness. Completeness is
important in the context of query answering when the collection is not
defined, and the cache may therefore provide results that were not
accessible for some reason, at query time. Hartig finds many cases
where it is profitable to cache, but also cases where it is not.

Lampo et al. \cite{lampo2011cache} found similar results. They found
that queries with star-shaped groups, that is, triple patterns that
share a common variable in the subject position, take best advantage
of a cache, and suggest rewriting queries to take better advantage of
a cache.

Acosta et al. has published a poster on ``SHEPHERD''
\cite{acosta2014shepherd}, presumed to be ongoing work, to
create a SPARQL processor that takes into account observations done by
the SPARQLES survey \cite{buil2013sparql}, to decompose a SPARQL query into
subqueries that are indicated to be easier for the server to evaluate,
and so is well aligned with our goal to ease the load of the
server. It is not quite clear from the brief description in 
\cite{acosta2014shepherd} whether the SHEPHERD system can cache results locally.

The following studies do not consider how the results of one query can
benefit the execution of a subsequent query. Nevertheless, they are
important to consider as prior work:
Umbrich et al. \cite{umbrich2012hybrid} considered a situation where
parts of the graph change at different paces. For the slow-changing
parts, they prefetch an entire dataset to a local store to relieve
remote endpoints from some processing, and thereby speed up query
responses. Of central importance to this approach is the notion of
coherence of query patterns, defined through the ratio between results
that were found in the live remote endpoint but not in the local cache
and the results from the live remote endpoint. Based on this, the need
for correct, up to date answers can be balanced against the need for
good response times.  This approach differs radically from our
approach in Chapter~\ref{sec:tpfcacheplanning} in that it replicates
an entire dataset rather than just fragments.

Naturally, the problem of caching in a proxy has much in common with
the federation problem as the cache, as seen from the query execution
engine's perspective, could be viewed as yet another possible data
source, and the number of HTTP requests to the remote endpoint can be
wisely assumed to be kept small. Schwarte et
al. \cite{springerlink:10.1007/978-3-642-25073-6-38} addressed these
problems with FedX, in which they sought to group connected subgraphs
with their concept of exclusive groups.  Montoya et
al. \cite{montoya2015federated} propose a query federation system that
minimise the data needed to be transferred while preserving
completeness in the common case where the data relevant to a query is
available on different endpoints. GÃ¶rlitz and Staab \cite{splendid}
exploited cardinality estimates sometimes available in VoID
\cite{voidnote} descriptions to improve join order execution plans. It
could be advantageous for the system in
Chapter~\ref{sec:tpfcacheplanning} to exploit as well, and
straightforward to implement, but since it has not seen wide uptake
and the statistics are mandated by Triple Pattern Fragments, and
therefore available in some cases to the planner, we chose not to.

Martin et al. \cite{sparqlproxy} implemented a reverse caching proxy
that controlled the changes to the dataset rather than rely on HTTP
headers to track changes, and so could relieve the endpoint of the
burden of evaluating all queries. Such a proxy would work poorly in
the scenario considered in Chapter~\ref{sec:tpfcacheplanning}, as
there is no way to mandate use of the proxy in this scenario.

Williams and Weaver \cite{kaseicache} consider the implications of the
HTTP protocol on query caching in a quad store on the indexing methods
that were employed. They propose to introduce a modification timestamp
in index structures, and find that the query engine would be able to
identify pieces that have been changed during execution, and could
therefore expose a \httph{Last-Modified} header and validate an
earlier response in accordance with RFC7232 \cite{rfc7232} in a cheaper way
than by evaluating the query. This also represent an alternative to HTTP
\httph{ETag} as in the discussion of \cite{papailiou2015graph}.

Dividino et al. \cite{Dividino2015} proposed strategies to keep caches
of Linked Open Data up to date, in face of changing data. The authors
set out to learn the change frequency of resources representations,
and based on that schedule updates of them as efficiently as possible
under bandwidth constraints. Instead of relying on resource metadata,
the authors propose an algorithm that iteratively ensures the quality
of the cache by examining the resource representations that have been
downloaded and compute certain metrics, such a dynamicity and
changerate.


While the approach is interesting and useful, I nevertheless have some
points of critisism: They motivate their approach from an earlier
finding that only 8\% of \httph{Last-Modified} headers they surveyed
reflected the true change frequency. While I have no reason to doubt
this finding, the authors boldly go on to claim that `` The only
alternative [to \httph{Last-Modified}] is to actually retrieve the
data from the sources and check it for changes.'', but this
contradicts the results I found in \cite{kjernsmo_survey_2015}. The
authors have not considered all alternatives, including not the
alternative presented by the HTTP caching standard RFC7234 discussed
in Section~\ref{sec:httpcache}, which invalidates that
assertion. Moreover, the author's approach would violate RFC7234 in
the case where their algorithm caches results that should not be
cached or has an explicit freshness lifetime, as discussed in
Section~\ref{sec:httpcache}. As a general observation, it is my belief
that the Semantic Web community should be more cautious about
violating core Web standards.



%\subsection{Tools for developer efficiency}
%\todo{more here}


While the approaches are diverse, these studies pull in the same
direction: Motivated by the problems of maintaining SPARQL endpoints,
they all seek to share the burden, and so make query answering
economically feasible.



%%  LocalWords:  summarisation Schwarte
