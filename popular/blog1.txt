Jeg startet med en dyp fascinasjon for ideen om Semantic Web, og mye
praktisk erfaring. Som forsker er det kanskje ikke det beste
utgangspunktet, siden det kan kan være en utfordring for
objektiviteten forskeren bør ha. Men akkurat det viste seg å være mitt
minste vitenskapsfilosofiske problem, vitenskapsfilosofiske grublerier
skulle bli langt mer framtredende en jeg hadde forestilt meg på
forhånd.

Semantic Web er ideen om at man skal kunne bygge ut dagens World Wide
Web med data som skal i større og mindre grad analyseres av
datamaskiner. Egentlig var det ideen helt fra starten: Hvis en webside
har en tabell, og en av kolonnene har overskriften "Pris" kan man
gjette seg til at i cellene står prisene for produktene som angis på
radene. Innen 1997 begynte det å bli ganske klart at websider ble i
praksis kodet på en slik måte at dette var noe man ikke ville klare i
praksis og dermed startet man arbeidet med en datamodell kalt
"Resource Description Framework" (RDF).

Året etter startet jeg på mitt hovedfag i astrofysikk, men i bakhodet
hadde jeg en problemstilling som jeg hadde opplevd da jeg på frivillig
basis startet skepsis.no: Problemet med å opplyse de overtroiske var
ikke snakk om å nå ut med informasjon, det er banalt enkelt. Det
vanskelige var å nå inn i lukkede sinn, trenge inn i ekkokamre, få
noen interessert i det de aller minst vil høre. Naturligvis er de
tekniske løsningene bare en liten del av bildet, men det begynte å
tegne seg et bilde av noen teknologiske muligheter i mitt hode. Jeg
skrev noen ideer og sendte dem litt rundt på nett, og en av dem som
plukket dem opp og fortalte at de tenkte i samme retning var han som
uka etter skulle bli leder og redaktør for standardiseringsarbeidet av
RDF. Han overbeviste meg raskt om at mine tekniske ideer var
realistiske innenfor rammeverket av Semantic Web. Skjønt, selv om jeg
fikk sjansen til å jobbe med det, var det for mye for en mann å løse
med svært umoden teknologi. Kommentarfeltenes tragiske endelikt
skylder jeg fortsatt på at ingen gjorde den jobben, demokratiet kunne
ha vunnet så mye.

Anvendelsene av Semantic Web-teknologi er brede, så brede at det bør
kunne gjennomsyre samfunnet i langt større grad en dagens
Web. Rørleggeren som skal levere tilbud burde bruke det, skal man ut
på reise bare vet hva man vil oppleve og ikke helt hvor man skal, bør
man bruke det, i medisin og i industri. Kort sagt over alt der noe som
er offentlig tilgjengelig skal integreres med noe som er privat, for å
hjelpe noen med å ta en avgjørelse. 

Dataintegrasjon er stikkordet. I dag er situasjonen at hvis man skal
integrere flere datakilder trenger man gjerne en programmerer til å
gjøre jobben. Jo flere datakilder som skal integreres, jo mer arbeid,
og sannsynligvis er det ikke bare dobbelt så mye arbeid for dobbelt så
mange datakilder, det er enda verre! Samtidig går antallet som er
interessert i akkurat den kombinasjonen ned. At kostnaden går opp
etterhvert som færre kan betale for jobben gjør at man ikke ser så
veldig store integrasjonsprosjekter til daglig, de er for dyre.

På Semantic Web skal hver enkelt gjøre en litt større innsats for å
gjøre sine egne data lettere å integrere, men belønningen er at
datamaskinene skal gjøre resten av jobben. Det bør legge teknologien
i hendene til folk flest.

Og det er nettopp det som er idealet bak mitt prosjekt. Industrien har
tatt teknologien i bruk, og Semantic Web ble brukt da IBM brukte sin
Watson til å sette menneskeheten i forlegenhet da den vant Jeopardy
over de mestvinnende menneskene i øvelsen. Mektig imponerende, helt
klart, men teknologi for de få i overskuelig framtid.

Prosjektet skulle først og fremst være praktisk. Måten man lager
applikasjoner på må endres, fra systemer der utviklere må ha
eksplisitt informasjon til systemer som drives av dataene de får fra
nettet. Dette er essensen i hypermedia. Når jeg sier hypermedia,
tenker de fleste på en film med tekst rundt, men det er bare det
enkleste tilfellet. I det tilfellet vet applikasjonen at når den har
siden, og finner en film, så kan den filmen spilles av. Men hypermedia
handler egentlig om at det er en dialog der brukerens system
resonnerer seg fram til hva den skal gjøre. En applikasjon skal ikke
trenge å være kodet for å bestille pizza, den blir fortalt hvordan den
skal gå fram av restaurantens websider.

Likevel er det utenfor hypermedia-området jeg har jobbet mest, med et
spørrespråk ved navn SPARQL. Enkle spørrespråk har nok de fleste sett:
På Google kan man skrive + foran et ord hvis man vil at det skal være
med, eller gåseøyne rundt en frase hvis ordene skal stå akkurat
slik. Med et ordentlig spørrespråk kan man f.eks. si at "gi meg alle
steder der det er meldt over 25 grader, i nærheten av en UNESCO world
heritage site, men også badestrand, minst 4 stjernes hotell, hvor det
er mindre enn 6 timer å fly fra min nærmeste flyplass med ledig plass
neste uke, og min nåværende posisjon er...", pluss at man kan ordne,
begrense antall resultater, osv., osv.

Dataene kan være under forskjellige aktørers kontroll, i forskjellige
databaser med forskjellig struktur. Det Semantic Web gir oss er en
måte å koble dem sammen hvis man enes om et navn for hver ting og
felles navn for egenskapene, og et språk til å bygge broer der man
ikke enes. Navnene som brukes på Semantic Web er velkjent fra
Weben. Navnet på denne artikkelen er dens adresse, og den kalles en
URL: http://TODO/. Semantic Web generaliserer dette til noe vi kaller
URI, som kan brukes for ting som ikke er på nett, som en person, eller
abstrakte ting, som en værtype. Men, siden dette er Web-teknologi, så
begynner gjerne også disse navnene med 'http' slik at man kan få data
om tingen bare ved å laste ned navnet. Data som brukes på denne måten
kalles "Linked Data".

Med SPARQL kan man stille spørsmål og få svar på svært innviklede
spørsmål, og etter ganske kort tid spratt det opp flere hundre
databaser åpent på nett der enhver som kunne språket eller hadde et
verktøy som kunne hjelpe med å lage spørringene, noe som også finnes
flere av, kunne gjøre det.

Det som dermed ble en interessant forskningsproblemstilling var å
bruke språket ikke bare på en database, men på kryss av mange, alt
etter hvilke data som finnes hvor. Dette ble mitt tema i likhet med
mange andre grupper.

Problemet var den nagende mistanken om at infrastrukturen ikke klarer
trafikken, noe som ble bekreftet av et studie av flere hundre slike
databaser. De var svært ustabile og svært få var stabile nok til
praktisk bruk. 

Et nytt felt, som Semantic Web, kan beskyldes for å glemme gammel
lærdom, og snart kom kritikken fra database-miljøet: Selvfølgelig kan
man ikke la tilfeldige brukere stille tilfeldige spørringer med et så
kraftig spørrespråk som SPARQL, det er ikke vanskelig å lage en
spørring som vil ta årevis å svare på. Det kan skje enten fordi man ikke
forstår hva man gjør, eller som et angrep. Man ser da heller ikke
vanlige databaser på nett, det er alltid et forenklet grensesnitt man
får som tilfeldig bruker. Nei, sa de, sentralisering av databaser til
datavarehus og strenge begrensninger på hvilke spørsmål man kan stille
er løsningen. 

Rent teknisk kan det være korrekt, men det er ikke bare tekniske
forhold som kan motivere forskningspørsmål. Sentralisering kan føre
meg seg at enkeltaktører blir for mektige, og at hvis du ikke er
innenfor en større kundegruppe blir dine problemer kommersielt
uinteressante. Det er nettopp kommersielt uinteressante problemer vi
skulle løse, fordi virkelige mennesker trenger en løsning også der det
ikke finnes en klar forretningsmodell.

Så dermed må vi utnytte fordelen med å være et nytt felt til å
utfordre gamle selvfølgeligheter, ved å formulere nye
problemstillinger. Det er to retninger min forskning har gått i:

For det første: Hvorfor har vi ikke gjort de store framskrittene mot
stabile løsninger? Spørsmålet plaget meg så mye at jeg tok en lang tur
bort fra det praktiske og over til det vitenskapsfilosofiske og den
statistiske literaturen.

Mye av forskningen foregår med formelle metoder, som står støtt på
sine formelle bevis. Problemet er når veldig mange formelle resultater
legges sammen til et ferdig utviklet databasesystem har systemet blitt
så komplekst at evalueringen av helheten må skje med empiriske
metoder. De er lite utviklet i literaturen.  Hypotesetester finnes
nesten ikke, man setter opp systemet man har utviklet, sender noen
spørringer til det og måler hvor lang tid det tok før man fikk
svar. "Benchmarking" kaller man det. Det er ingen samlende statistikk
rundt de forskjellige testspørringene, ingen strukturert måte å finne
ut om evalueringen i seg selv gir meningsfylte resultater. Man
forsøker å møte problemet med at man i prinsippet kan velge spørringer
utifra hvilken konklusjon man ønsker ved å standardisere spørringene,
men dermed gjør man det umulig å teste utsagn som ligger utenfor
standarden. 

Mitt forslag var derfor å introdusere metode fra statistikken kjent
som "Design of Experiments". Med et eksperiment som kjørte i flere
dager viste vi hvordan man kan generere testspørringer, gjøre
hypotesetester, finne de viktigste problemene og lage en strukturert
måte å vise om selve eksperimentet har viktige feil.

Likevel fortsatte flere vitenskapsteoretiske problemstillinger å plage
meg. Når man står på Semantic Web-konferanser, som forøvrig
kjennetegnes av svært godt sosialt og inkluderende miljø, så påstår så
godt som alle at deres løsning fremmer visjonen om Semantic Web. Men
hvordan underbygger man en slik påstand, all den tid Semantic Web ikke
eksisterer ennå i den formen som visjonen forutsetter? Kan den trege
utviklingen tilskrives at vi rett og slett ikke driver vitenskap, og i
så fall, hva er vitenskap? Klare, popperske kriterier viser raskt sine
problematiske sider, ettersom man innenfor dagens evalueringsmetodikk
ikke har rimelig grad av sikkerhet på om det er studiets resultat som
falsifiseres eller bare evalueringsmetodikken som er for dårlig. Vi
har ingen formulert teori der hypotesene finner sin kontekst, slik min
hovedfagsoppgave i sin tid fant sin kontekst i generell
relativitetsteori og kvasarteori. 

De gamle vitenskapsfilosofer gir en god ide om hva som kan gå galt
hvis man ikke holder tunga rett i munnen, men jeg finner lite
rettledning i de konkrete problemene jeg står oppi, og spørsmålet om
sviktende metode er en av årsakene til treg framdrift har blitt
stående.

Et annet svar til databasemiljøet er at "vi har nå et fungerende,
globalt informasjonsystem som dere ikke hadde da dere prøvde å legge
databaser åpent for mange år siden". Dette informasjonsystemet,
Webben, har visse egenskaper i infrastrukturen som muligens kan
utnyttes. Et av dem er mellomlagring. I nettet finnes det maskiner som
mellomlagrer data fra forskjellige kilder for å hjelpe til med å ta
last fra publiseringssystemet, og for å sørge for at brukeren får
bedre opplevd hastighet.

Det første spørsmålet var om Web-standardene for mellomlagre er i
tilstrekkelig utbredt bruk av eksisterende datakilder på Semantic
Web, noe som allerede innebærer ganske mye data. Jeg gjorde et
omfattende søk over alle data jeg fant, og konkluderte med at
situasjonen må forbedres mye, men at for enkelte deler kan man dra
nytte av denne infrastrukturen.

For mitt siste arbeid gikk jeg tilbake til det praktiske, nemlig å
lage et system som utnytter mellomlagringen i nettet, prøver å gjette
hvilke data som kan være nyttige for brukernes framtidige spørringer
og integrerer hypermedia i tillegg til databasene. Dette skal bli et
modulært og ganske komplett system jeg publiserer som fri
programvare. To av modulene har allerede blitt tatt opp av
Debian-prosjektet, som danner grunnlaget for de mest utbredte
Linux-baserte operativsystemene.

Dette arbeidet vil også ha en Design of Experiments-basert
evaluering. Uansett hva den sier, vil de store spørsmålene bli stående
ubesvart, jeg kan fortsatt ikke si om mitt arbeid er et verdifullt
bidrag på veien mot Semantic Web.
