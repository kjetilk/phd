\documentclass{llncs}
%\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{wrapfig}

\newcommand{\rdfterm}[1]{\texttt{#1}}
\newcommand{\httph}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\ensuremath{^{\textrm{\tiny{TODO}\normalsize}}}\footnote{\textbf{TODO:}~#1}}

\title{A survey of HTTP caching options on the open Web}
\author{Kjetil Kjernsmo\inst{1}}
\institute{Department of Informatics,
Postboks 1080 Blindern,
N-0316 Oslo, Norway \email{kjetil@kjernsmo.net}

\subtitle{---Unpublished Working Draft, do not circulate---}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Caching has been given a prominent place in the foundational documents
of the World Wide Web. Out of the 6 documents that make up the HTTP
1.1 standard, RFC7234 \todo{ref} is entirely devoted to the topic, and
RFC7232 defines conditional requests, which is also important when
constructing caches. As the former notes:
\begin{quote} 
  The goal of caching in HTTP/1.1 is to significantly improve
  performance by reusing a prior response message to satisfy a current
  request.
\end{quote}
Furthermore, the Architecture of the World Wide Web \todo{ref}
discusses caching throughout, and the definition of the
Representational State Transfer (REST) architectural style \todo{ref}
is partly motivated from the requirement to implement efficient
caching. We also note that caching in the Internet infrastructure,
through so-called Content Delivery Networks, is both a large business
area and something that could provide great value to the Semantic Web
community.

In spite of this, we have not seen it in widespread use, and therefore
we decided to conduct a survey to investigate the actual compliance to
RFC7234 and RFC7232. Our reasons to do this include
\begin{enumerate}
\item Understand the actual usage rather than rely on anecdotal
  conceptions.
\item Encourage the implementation of these mechanisms in Semantic Web
  infrastructure.
\item Point out future research directions.
\end{enumerate}

We note that caching is not only useful for long-living information
resources, even though that may be the most important. If a resource
is frequently requested, it may make sense to cache it even though it
may be fresh for only a very short period.

Caching may be deployed at several different levels: A HTTP cache may
be in a reverse proxy close to the server, in which case it may have
much in common with a conventional database cache. It may also be
anywhere between a server and a client, in which case it may be
shared, i.e. it may cache responses from several servers served to
several clients. Another example is an institutional forward proxy,
which are close to several users. Finally, the User Agent may
implement a private cache for its user at the client side.

Caching, as defined in RFC7234, requires that the server declares how
long into the future it expects the information resource in the
response to remain fresh, i.e. how long it is permissible to use
it. Based on this, the client need not contact the server at all to
reuse a cached response, but this requires a commitment from the
content provider. The standard also gives the client an opportunity to
heuristically determine a time to live. RFC7232, on the other hand,
defines a protocol for asking the server if the cached response is
still fresh using conditional requests. This doesn't burden the
content provider with the task of estimating the time to live
beforehand, but then, it must be able to answer if the resource has
changed less expensively than serving the entire response. These two
approaches can be combined, as a client may ask if a response is fresh
after it has expired.

LDF\todo{ref} asserted that SPARQL query caching is only possible on a
per-query basis, but future research should challenge this
assumption. With this in mind, if reasoning is not involved, we note
that ontologies are not relevant to query answering since they usually
occur in bound terms. It is interesting to understand caching when
this is not the case. Moreover, many ontologies are maintained by a
small group of humans and change seldomly.

\section{Related work}

We are not aware of any surveys of this type. Although the database
literature is rich with query cache literature, it is mostly relevant
to what would happen within the server or between the server and a
reverse proxy, which is opaque to the Internet, and therefore not of
our concern.

LDF\todo{ref paper1} claims to take advantage of caching and contrasts
this with the inavailabilty of SPARQL query caches, and claims this is
an architectural problem.

The implications of HTTP for query caches has been discussed by
\todo{ref kasei}. In \todo{marin, Auer} the authors implemented a
reverse proxy that controlled the changes to the dataset, and
therefore could 

A general caching approach is given in \todo{Caching intermediate
  result of sparql queries}. 

\todo{fast vs. fresh} didn't consider caching in the sense we do, they
rather prefetch an entire dataset to a local store and based on
heuristics tried to determine which parts of the query should be
evaluated remotely and locally.

\todo{cache or not} explored when caching had a positive effect on
complex SPARQL queries.

In the broader Web literature, \todo{Revisiting Cacheability
in Times of User Generated Content} analysed the value of caching
based anonymized traces of actual Web usage at a major Internet
Service Provider.

\section{Methodology}

We want to find information resources on the Web, and examine HTTP
headers that may allow caching. We record headers recommended by
current standards, as well as obsoleted and non-standard
headers. Additionally, we examine the triples in the returned
information resources to see if there is information there that may be
used to calculate heuristic freshness.

However, as the community has both spidered the Web as well as
recorded lists of relevant resources, we need not do that ourselves. 

The SPARQLES survey\todo{ref} has recorded the availability of SPARQL
endpoints registered in the datahub.io portal, and found that many of
them have ceased to operate. We used their list of SPARQL endpoints as
of 2014-11-17, and filtered out those deemed unresponsive by
them. This resulted in a list of 312 endpoints.

To compile a list of Web ontologies, we first queried the Linked Open
Vocabularies \todo{ref} SPARQL endpoint to retrieve all registered
ontologies. Then, we took the full list of URIs registered with
prefix.cc. The assumption that they all identify an ontology is
clearly false, as we found that for example Tim Berners-Lee's FOAF
profile has a registered URI, but it is unlikely to introduce
significant bias.

Then, we retrieved the dataset of the Billion Triple Challenge
2014\todo{ref}, a 4~GTriple corpus of spidered Web data. To compile a
list of interesting candidates to further examine, we performed a
series of data reduction steps, manually inspecting the result between
each step. Due to the presence of invalid RDF, we
iterated through the NQuad files on a line-by-line basis. First, we
matched each line against a regular expression were lines matching
\texttt{ontology|endpoint|sparql|vocabular} passed the filter. Then,
the Perl framework \text{RDF::Trine}\todo{footnote} was used to parse
the line. Lines that failed to parse was discarded. We have not
investigated whether this could introduce biases. Statements were then
accepted into a new NQuad file if they had a predicate that matched
the \rdfterm{sd:endpoint}\todo{footnote} or matched a case-insensitive
regular expression \texttt{sparql} if the subject and objects were
both resources, or the predicates \rdfterm{void:vocabulary},
\rdfterm{rdfa:vocabulary} or \rdfterm{api:vocabulary} \todo{footnotes
  for all}, as well as having an resource as object. Finally,
statements with the the classes \rdfterm{cogs:Endpoint},
\rdfterm{owl:Ontology} and \rdfterm{voaf:Vocabulary} \todo{footnotes for
  all} in the object position were also accepted. More classes and
properties were considered, but not used in the data reduction if they
did not occur in the original data.

In the next step, we filtered out statements with URIs that were
invalid or irrelevant, e.g. URIs that didn't have a scheme or where
the scheme wasn't HTTP(S), or they were referring to private IP addresses.

We then sought to classify resources into the categories ``endpoint''
for SPARQL endpoints, ``vocabulary'' for ontologies, ``dataset'' for
datasets that may contain further descriptions of several resources,
or simply ``inforesources'' for those that did not fit in the above
classes. To do so, we classified based on certain predicates and
classes, see table\todo{create table}. Additionally, URIs derived from
prefix.cc were classified as ``vocabulary'' and those from SPARQLES as
``endpoint''.

Since we blatantly violated URI opacity with our regular expression
matching in the first step, we needed to further filter candidates for
SPARQL endpoints. This step therefore included filtering as well as
classification.

We found in the data a large number of ontologies that consist of many
information resources with just a few triples in each. Since they
appear to be produced by the same software, usually Semantic
Mediawiki, we assume that they are configured with a single setup, and
thus we will merely sample these resources.

We continued to also sample the HTTP headers gathered in the BTC2014
dataset. 
%as they had recorded some of the relevant headers,
%\httph{Expires}, \httph{Last-Modified} and \httph{ETag}. 
First, we traversed the files with a simple UNIX \texttt{grep} to find
the resources that had reported one of the RDF serializations as
content type. We then traversed this list, first discarded the
resources that did not have a valid IRI (this amounted to just 3273
resources). For the resources we found, as well as for the resources
that was of \rdfterm{rdf:type owl:Ontology} above, we kept one
resource per hostname, with the exception of the popular blogging
platforms Livejournal and SAPO, where each blog has their own host and
they expose FOAF data. For those, we only kept one hostname. 



\end{document}
