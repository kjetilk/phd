%\documentclass{llncs}
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{verbatim}
\usepackage{graphicx}
%\usepackage{wrapfig}
\usepackage{appendix}

\newcommand{\rdfterm}[1]{\texttt{#1}}
\newcommand{\pvalue}{\textit{p}-value}
\newcommand{\httph}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\ensuremath{^{\textrm{\tiny{TODO}\normalsize}}}\footnote{\textbf{TODO:}~#1}}

\title{A survey of HTTP caching options on the open Web}
\author{Kjetil Kjernsmo}%\inst{1}}
%\institute{Department of Informatics,
%Postboks 1080 Blindern,
%N-0316 Oslo, Norway \email{kjetil@kjernsmo.net}

%\subtitle{---Unpublished Working Draft, do not circulate---}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Caching has been given a prominent place in the foundational documents
of the World Wide Web. Out of the 6 documents that make up the HTTP
1.1 standard, RFC7234 \todo{ref} is entirely devoted to the topic, and
RFC7232 defines conditional requests, which is also important when
constructing caches. As the former notes:
\begin{quote} 
  The goal of caching in HTTP/1.1 is to significantly improve
  performance by reusing a prior response message to satisfy a current
  request.
\end{quote}
Furthermore, the Architecture of the World Wide Web \todo{ref}
discusses caching throughout, and the definition of the
Representational State Transfer (REST) architectural style \todo{ref}
is partly motivated from the requirement to implement efficient
caching. We also note that caching in the Internet infrastructure,
through so-called Content Delivery Networks, is both a large business
area and something that could provide great value to the Semantic Web
community.

In spite of this, we have not seen it in widespread use in the
Semantic Web, and therefore we decided to conduct a survey to
investigate the actual compliance to RFC7234 and RFC7232. Our reasons
to do this include
\begin{enumerate}
\item Understand the actual usage rather than rely on anecdotal
  conceptions.
\item Encourage the implementation of these mechanisms in Semantic Web
  infrastructure.
\item Point out future research directions.
\end{enumerate}

We note that caching is not only useful for long-living information
resources, even though that may be the most important. If a resource
is frequently requested, it may make sense to cache it even though it
may be fresh for only a very short period.

Caching may be deployed at several different levels: A HTTP cache may
be in a reverse proxy close to the server, in which case it may have
much in common with a conventional database cache. It may also be
anywhere between a server and a client, in which case it may be
shared, i.e. it may cache responses from several servers served to
several clients. Another example is an institutional forward proxy,
which are close to several users. Finally, the User Agent may
implement a private cache for its user at the client side.

Caching, as defined in RFC7234, requires that the server declares how
long into the future it expects the information resource in the
response to remain fresh, i.e. how long it is permissible to use
it. Based on this, the client need not contact the server at all to
reuse a cached response, but this requires a commitment from the
content provider. The standard also gives the client an opportunity to
heuristically determine a time to live. RFC7232, on the other hand,
defines a protocol for asking the server if the cached response is
still fresh using conditional requests. This doesn't burden the
content provider with the task of estimating the time to live
beforehand, but then, it must be able to answer if the resource has
changed less expensively than serving the entire response. These two
approaches can be combined, as a client may ask if a response is fresh
after it has expired.

LDF\todo{ref} asserted that SPARQL query caching is only possible on a
per-query basis, but future research should challenge this
assumption. With this in mind, if reasoning is not involved, we note
that ontologies are not relevant to query answering since they usually
occur in bound terms. It is interesting to understand caching when
this is not the case. Moreover, many ontologies are maintained by a
small group of humans and change seldomly.

Key contributions of this paper are:\todo{write}

\section{Related work}

We are not aware of any surveys of this type. Although the database
literature is rich with query cache literature, it is mostly relevant
to what would happen within the server or between the server and a
reverse proxy, which is opaque to the Internet, and therefore not of
our concern.

LDF\todo{ref paper1} claims to take advantage of caching and contrasts
this with the unavailability of SPARQL query caches, and claims this is
an architectural problem.

The implications of HTTP for query caches has been discussed by
\todo{ref kasei}. In \todo{marin, Auer} the authors implemented a
reverse proxy that controlled the changes to the dataset, and
therefore could 

A general caching approach is given in \todo{Caching intermediate
  result of sparql queries}. 

\todo{fast vs. fresh} didn't consider caching in the sense we do, they
rather prefetch an entire dataset to a local store and based on
heuristics tried to determine which parts of the query should be
evaluated remotely and locally.

\todo{cache or not} explored when caching had a positive effect on
complex SPARQL queries.

In the broader Web literature, \todo{Revisiting Cacheability
in Times of User Generated Content} analysed the value of caching
based anonymized traces of actual Web usage at a major Internet
Service Provider.

\section{Methodology}

We want to find information resources on the Web, and examine HTTP
headers that may allow caching. We record headers recommended by
current standards, as well as obsoleted and non-standard
headers. Additionally, we examine the triples in the returned
information resources to see if there is information there that may be
used to calculate heuristic freshness.

However, as the community has both spidered the Web as well as
recorded lists of relevant resources, we need not do that ourselves. 

The SPARQLES survey\todo{ref} has recorded the availability of SPARQL
endpoints registered in the datahub.io portal, and found that many of
them have ceased to operate. We used their list of SPARQL endpoints as
of 2014-11-17, and filtered out those deemed unresponsive by
them. This resulted in a list of 312 endpoints.

To compile a list of Web ontologies, we first queried the Linked Open
Vocabularies \todo{ref} SPARQL endpoint to retrieve all registered
ontologies. Then, we took the full list of URIs registered with
prefix.cc. The assumption that they all identify an ontology is
clearly false, as we found that for example Tim Berners-Lee's FOAF
profile has a registered URI, but it is unlikely to introduce
significant bias.

To examine as many different implementations and hosts as possible, we
noted that the Billion Triple Challenge 2014\todo{ref} dataset
consisted of a 4~GTriple corpus of spidered Web data. To compile a
list of interesting candidates to further examine, we performed a
series of data reduction steps, manually inspecting the result between
each step. The details of this process is given in
Appendix~\ref{app:reduction}.

The end result of this process is a list of 3117 unique hosts, for
each several resources would be visited, some several times, as they
may host SPARQL Endpoints, ontologies, or other information resources,
by a spider detailed in Appendix~\ref{app:fetcher}

This results in an NQuads file per host, which is then loaded into a
SPARQL Endpoint\todo{which} for analysis by using the statistics
system R\todo{ref}.

\section{Analytics}

\subsection{Different server implementations}

First, we investigated whether certain servers provided better support
for caching than others. To do this, we formulated SPARQL queries to
examine the \httph{Server} headers of successful responses, with
optional clauses matching the standard-compliant computed freshness
time (which is the ideal) as well as whether the response had other
promising features that could assist caching, such as \httph{ETag}s,
modification time, certain predicates, etc.

For each unique \httph{Server} header, we found the ones where
\emph{all} responses had a freshness time or a promising feature. For
the former, this amounted to 22 servers, tabulated in~\todo{include
  table}. 70 servers always responded with promising features. As we
can see, apart from well-known Virtuoso and Callimachus servers, as
well as the Perl modules RDF::LinkedData and RDF::Endpoint, which is
partly developed by and runs on a server operated by this author, most
headers tells very little about the RDF-specific parts of the
underlying server implementation. A quick inspection of all
\httph{Server} headers confirm that few reveal any further detail.

For a more systematic approach, we wish to test the hypothesis that
some servers are better configured to support caching than others. To
test this hypothesis, we create two so-called \emph{contigency
  tables}\todo{ref}. This formalism is suited to see if the
distribution of  \httph{Server} headers are different for those implementations that
offer caching headers from those that don't. Intuitively, we expect
these distributions to be similar, ``long-tail'' distributions, i.e. a
handful of servers are used by a large number of projects, and then it
falls off rapidly, and so, some servers are used only by very
few. Likewise, it is only to be expected that only a few projects have
given caching enough attention, but that they account for the majority
of the support. The question is if the presence of caching headers is
merely a matter of proportion, or if there are some that have given it
more attention, but still is in relatively little use.

Using the statistics system R\todo{ref}, we use a statistical test,
namely Pearson's $\chi^2$ test with simulated \pvalue (based on 10000
replicates). The simulation is done using a Monte Carlo method to
compensate for the fact that many servers will not expose caching
headers at all.

In both the cases of standard-compliant freshness time and for the
other promising features, the test reports \pvalue = 0.0001. We can
conclude that it is highly likely that some servers are better at
exposing cache headers than others. Unfortunately, since most
\httph{Server} headers only report generic values, little can be
learnt about these implementations. We note, however, that DBPedia
exposes standards-compliant freshness time of 604800 seconds (i.e. 1
week) for both LOD and SPARQL endpoints. \todo{do this specifically
  for SPARQLES endpoints?}

\section{Future work}

Identify implementations that expose suitable caching headers.

\section{Recommendations}\todo{write}

\begin{appendices}

\section{Implementation of data reduction}\label{app:reduction}

The Billion Triple Challenge 2014 dataset provided data in the form of
NQuad files. Due to the presence of invalid RDF, we iterated through
the NQuad files on a line-by-line basis. First, we matched each line
against a regular expression were lines matching
\texttt{ontology|endpoint|sparql|vocabular} passed the filter. Then,
the Perl framework RDF::Trine\todo{footnote} was used to parse the
line. Lines that failed to parse was discarded. We have not
investigated whether this could introduce biases. Statements were then
accepted into a new NQuad file if they had a predicate that matched
the \rdfterm{sd:endpoint}\todo{footnote} or matched a case-insensitive
regular expression \texttt{sparql} if the subject and objects were
both resources, or the predicates \rdfterm{void:vocabulary},
\rdfterm{rdfa:vocabulary} or \rdfterm{api:vocabulary} \todo{footnotes
  for all}, as well as having an resource as object. Finally,
statements with the the classes \rdfterm{cogs:Endpoint},
\rdfterm{owl:Ontology} and \rdfterm{voaf:Vocabulary} \todo{footnotes
  for all} in the object position were also accepted. More classes and
properties were considered, but not used in the data reduction if they
did not occur in the original data.

In the next step, we filtered out statements with URIs that were
invalid or irrelevant, e.g. URIs that didn't have a scheme or where
the scheme wasn't HTTP(S), or they were referring to private IP addresses.

We then sought to classify resources into the categories ``endpoint''
for SPARQL endpoints, ``vocabulary'' for ontologies, ``dataset'' for
datasets that may contain further descriptions of several resources,
or simply ``inforesources'' for those that did not fit in the above
classes. To do so, we classified based on certain predicates and
classes, see table\todo{create table}. Additionally, URIs derived from
prefix.cc were classified as ``vocabulary'' and those from SPARQLES as
``endpoint''.

Since we blatantly violated URI opacity with our regular expression
matching in the first step, we needed to further filter candidates for
SPARQL endpoints. This step therefore included filtering as well as
classification.

We found in the data a large number of ontologies that consist of many
information resources with just a few triples in each. Since they
appear to be produced by the same software, usually Semantic
Mediawiki, we assume that they are configured with a single setup, and
thus we will merely sample these resources.

We continued to also sample the HTTP headers gathered in the BTC2014
dataset. 
%as they had recorded some of the relevant headers,
%\httph{Expires}, \httph{Last-Modified} and \httph{ETag}. 
First, we traversed the files with a simple UNIX \texttt{grep} to find
the resources that had reported one of the RDF serializations as
content type. We then traversed this list, first discarded the
resources that did not have a valid IRI (this amounted to just 3273
resources). For the resources we found, as well as for the resources
that was of \rdfterm{rdf:type owl:Ontology} above, we kept one
resource per hostname, with the exception of the popular blogging
platforms Livejournal and SAPO, where each blog has their own host and
they expose FOAF data. For those, we only kept one hostname. 

\section{Implementation of spider}\label{app:fetcher}

We then developed a fairly elaborate parallel spider to examine the
resources found on hosts that the previous steps deemed interesting
using the Perl frameworks RDF::Trine and libwww. The spider operated
with a timeout of 20 seconds and a maximum message size it would
accept of 1 MB.

The parallel spider would then launch a process per host, but each
request to one host would be delayed by 10 seconds. For each host, the
spider would go through the list of URLs found by previous steps for
that host. Since the BTC2014 recorded
the \httph{Expires} (an RFC7234 caching header), \httph{Last-Modified}
and \httph{ETag} (RFC7232 conditional request) headers where they
existed, we first examined if any of the resources were still fresh,
but none were. Wherever the last two headers existed, we added the
corresponding \httph{If-Modified-Since} and \httph{If-None-Match}
headers for a conditional initial request.

\begin{table}
\caption{Recorded HTTP headers}
 \begin{tabular}{ | l |  p{3cm} | p{5cm} |}
    \hline
    Header & Reference & Description \\ \hline
\httph{Age} & RFC7234 & When obtaining response from a cache, the number of
seconds since validation \\ \hline
\httph{Cache-Control} & RFC7234 & Header used for a variety of directives \\ \hline
\httph{Expires} & RFC7234  & Gives the date/time after which the
   response is considered stale. \\ \hline
\httph{Pragma} & RFC7234 & Archaic HTTP 1.0 header  \\ \hline
\httph{Warning} & RFC7234  & For additional information about possible incorrectness \\ \hline
\httph{Content-Type} & RFC7231 & To select the correct parser \\ \hline
\httph{If-None-Match} & RFC7232  & Request header to check if
                                   \httph{ETag} has changed   \\ \hline
\httph{If-Modified-Since} & RFC7232  & Request header to check if
                                   \httph{Last-Modified} has changed    \\ \hline
\httph{Last-Modified} & RFC7232 & When the resource was last modified \\ \hline
\httph{ETag} & RFC7232 & An opaque validator to check if the resource
has changed  \\ \hline
\httph{X-Cache} &  & Inserted by some caches to indicate cache status \\ \hline
\httph{Date} & RFC7231 & The time of the message. Used in conditional requests
and heuristics \\ \hline
\httph{Surrogates} & Edge Architecture Specification\todo{ref} & Draft
to allow more fine-grained control for proxies. \\ \hline
\httph{Client-Aborted} & libwww\todo{ref}  & Header inserted by User
Agent to indicate that it aborted the download \\ \hline
\httph{Client-Warning} & libwww\todo{ref}  & Header inserted by User
Agent to give details about problems with the download \\ \hline

    \hline
    \end{tabular}
\end{table}


For endpoints, we made the following SPARQL query:
\begin{verbatim}
SELECT REDUCED ?Concept WHERE {[] a ?Concept} LIMIT 2
\end{verbatim}
which should be quite light, yet likely yield results.

Then, the first request would be made, and a selection of the
resulting HTTP headers recorded in an per-host NQuads file. For this
purpose, we developed and released a module
RDF::Generator::HTTP\todo{footnote} to the Comprehensive Perl Archive
Network. We recorded whether the conditional request showed that the
BTC data were still fresh, and if it was, we retrieved the current
data, as we had not couple the headers to the body in our original
retrieval.

Based on the resulting headers, we first let libwww calculate the
freshness lifetime in seconds if it was possible, and recorded
that. Then, we also let it calculate a \emph{heuristic} freshness
time based on a suggestion in RFC7234, section~4.2.2, and recorded that.

If the initial response had RFC7232 headers, we make another
request to see if the server just included the headers but does not
actually support conditional requests. The heuristic we employed is
that if the headers remain the same, but the result was returned,
rather than just a response code \httph{304} (which indicates that the
previous result can be reused), then the server does most probably not
support it.

For endpoints, we examined the response message, to see if there are
any results to our query, and recorded that if there are.

For all others, we parsed the response, and recorded any errors if the
parser concluded the content were invalid. We noted that the Dublin Core
Metadata terms vocabulary has a number of predicates that may become
useful in determining heuristic freshness in the future, so we
recorded any statements containing the predicates \rdfterm{dct:date},
\rdfterm{dct:accrualPeriodicity}, \rdfterm{dct:created},
\rdfterm{dct:issued}, \rdfterm{dct:modified} or \rdfterm{dct:valid}.

For resource types other than ``vocabulary'', we look for SPARQL
endpoints in the response, using the predicates \rdfterm{sd:endpoint}
and \rdfterm{void:sparqlEndpoint}. We then do the same query as above
and record the relevant headers. Unfortunately, we found early that
this only turns up misconfigured endpoints that point to localhost,
and was removed from the spider for the final analysis.

Finally, if the LOV SPARQL endpoint used a URI for the vocabulary that
was different from the namespace URI (after a normalization step),
another request would be made to record the selected HTTP headers from
that as well.
\end{appendices}

\end{document}
